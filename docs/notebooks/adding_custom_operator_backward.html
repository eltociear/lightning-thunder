


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-PQBQ3CV');
  </script>
  <!-- End Google Tag Manager -->

  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="google-site-verification" content="okUst94cAlWSsUsGZTB4xSS4UKTtRV8Nu5XZ9pdd3Aw" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Defining custom forward and backward for existing operators &mdash; lightning-thunder 0.1.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/icon.svg"/>
  
  
  
    <link rel="canonical" href="https://github.com/Lightning-AI/lightning-thunder/notebooks/adding_custom_operator_backward.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx_paramlinks.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Extending Thunder" href="dev_tutorials/extend.html" />
    <link rel="prev" title="Defining new Thunder operators" href="adding_custom_operator.html" />
  <!-- Google Analytics -->
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Light.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Semibold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/Inconsolata/Inconsolata.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <script defer src="https://use.fontawesome.com/releases/v6.1.1/js/all.js" integrity="sha384-xBXmu0dk1bEoiwd71wOonQLyH+VpgR1XcDH3rtxrLww5ajNTuMvBdL5SOiFZnNdp" crossorigin="anonymous"></script>

  <script src="https://unpkg.com/react@18/umd/react.development.js" crossorigin></script>
  <script src="https://unpkg.com/react-dom@18/umd/react-dom.development.js" crossorigin></script>
  <script src="https://unpkg.com/babel-standalone@6/babel.min.js"></script>
  <script src="../_static/js/react/react.jsx" type="text/babel"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://lightning-thunder.rtfd.io/en/latest/" aria-label="PyTorch Lightning">
      <!--  <img class="call-to-action-img" src="../_static/images/logo-lightning-icon.png"/> -->
      </a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://lightning-thunder.readthedocs.io/en/latest/introduction_guide.html">Get Started</a>
          </li>

          <li>
            <a href="https://www.Lightning-AI.ai/blog">Blog</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning.ai/docs/pytorch/stable/">
                  <span class="dropdown-title">PyTorch Lightning</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning.ai/docs/fabric/stable/">
                  <span class="dropdown-title">Lightning Fabric</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://torchmetrics.readthedocs.io/en/stable/">
                  <span class="dropdown-title">TorchMetrics</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-flash.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Flash</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-bolts.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Bolts</span>
                </a>
            </div>
          </li>

          <!--<li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://lightning-thunder.readthedocs.io/en/latest/#community-examples">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>-->

          

          <li>
            <a href="https://github.com/Lightning-AI/lightning-thunder">GitHub</a>
          </li>

          <li>
            <a href="https://www.lightning.ai/">Lightning AI</a>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  0.1.0
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Home</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Welcome to ⚡ Lightning Thunder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fundamentals/installation.html">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fundamentals/hello_world.html">Hello World</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fundamentals/examine.html">Using examine</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../basic/overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="zero_to_thunder.html">Zero to Thunder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic/inspecting_traces.html">Thunder step by step</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic/sharp_edges.html">The sharp edges</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic/mlp_mnist.html">Train a MLP on MNIST</a></li>
<li class="toctree-l1"><a class="reference internal" href="functional_jit.html">Functional jit</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Intermediate</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/additional_executors.html">Additional executors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/whats_next.html">What's next</a></li>
<li class="toctree-l1"><a class="reference internal" href="dev_tutorials/fsdp_tutorial.html">FSDP Under the Hood Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../advanced/inside_thunder.html">Inside thunder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/extending.html">Extending thunder</a></li>
<li class="toctree-l1"><a class="reference internal" href="adding_custom_operator.html">Defining new Thunder operators</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Defining custom forward and backward for existing operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Experimental dev tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dev_tutorials/extend.html">Extending Thunder</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/thunder.html">thunder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/common/index.html">thunder.common</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/core/index.html">thunder.core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/clang/index.html">thunder.clang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/examine/index.html">thunder.examine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/distributed/index.html">thunder.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/executors/index.html">thunder.executors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/torch/index.html">thunder.torch</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Defining custom forward and backward for existing operators</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/adding_custom_operator_backward.ipynb.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="Defining-custom-forward-and-backward-for-existing-operators">
<h1>Defining custom forward and backward for existing operators<a class="headerlink" href="#Defining-custom-forward-and-backward-for-existing-operators" title="Permalink to this heading">¶</a></h1>
<p>We are going to add custom executor for forward and backward of <code class="docutils literal notranslate"><span class="pre">torch.nn.functional.cross_entropy</span></code> operator.</p>
<p>Here’s <code class="docutils literal notranslate"><span class="pre">SoftmaxCrossEntropyLoss</span></code> definition from <a class="reference external" href="https://github.com/NVIDIA/apex/blob/master/apex/contrib/xentropy/softmax_xentropy.py">https://github.com/NVIDIA/apex/blob/master/apex/contrib/xentropy/softmax_xentropy.py</a>:</p>
<div class="highlight-py notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">import</span> <span class="nn">xentropy_cuda</span>


<span class="k">class</span> <span class="nc">SoftmaxCrossEntropyLoss</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>
    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">padding_idx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">half_to_float</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="n">losses</span><span class="p">,</span> <span class="n">max_log_sum_exp</span> <span class="o">=</span> <span class="n">xentropy_cuda</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span>
            <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">smoothing</span><span class="p">,</span> <span class="n">half_to_float</span><span class="p">)</span>
        <span class="n">losses</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">labels</span><span class="o">==</span><span class="n">padding_idx</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">max_log_sum_exp</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">([</span><span class="n">smoothing</span><span class="p">]),</span>
            <span class="n">torch</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">padding_idx</span><span class="p">]))</span>

        <span class="k">return</span> <span class="n">losses</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">grad_loss</span><span class="p">):</span>
        <span class="n">logits</span><span class="p">,</span> <span class="n">max_log_sum_exp</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">smoothing</span><span class="p">,</span> <span class="n">padding_idx</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">grad_loss</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">():</span>
            <span class="n">grad_loss</span> <span class="o">=</span> <span class="n">grad_loss</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
        <span class="n">grad_loss</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">labels</span><span class="o">==</span><span class="n">padding_idx</span><span class="o">.</span><span class="n">item</span><span class="p">(),</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">grad_logits</span> <span class="o">=</span> <span class="n">xentropy_cuda</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span>
            <span class="n">grad_loss</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span> <span class="n">logits</span><span class="p">,</span> <span class="n">max_log_sum_exp</span><span class="p">,</span>
            <span class="n">labels</span><span class="p">,</span> <span class="n">smoothing</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

        <span class="k">return</span> <span class="n">grad_logits</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</pre></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;..&#39;</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">thunder</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">thunder.core.proxies</span> <span class="kn">import</span> <span class="n">TensorProxy</span>
<br/></pre></div>
</div>
</div>
<p>In Thunder, we define <em>Executors</em> to run given ops. Our executor will handle specific ops (rather than fusion regions), so our first thing is to create our own <code class="docutils literal notranslate"><span class="pre">OperatorExecutor</span></code>and register it with Thunder</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">thunder.extend</span> <span class="kn">import</span> <span class="n">OperatorExecutor</span><span class="p">,</span> <span class="n">register_executor</span>
<span class="n">apex_xentropy_ex</span> <span class="o">=</span> <span class="n">OperatorExecutor</span><span class="p">(</span><span class="s2">&quot;apex_xentropy_ex&quot;</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="s2">&quot;0.1&quot;</span><span class="p">)</span>
<span class="n">register_executor</span><span class="p">(</span><span class="n">apex_xentropy_ex</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
apex_xentropy_ex
</pre></div></div>
</div>
<p>To get a feel of what’s going on, let’s have a wrapper that prints function calls and their arguments.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">functools</span>

<span class="n">_indentation</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">def</span> <span class="nf">_log</span><span class="p">(</span><span class="n">msg</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Print a message at current indentation.&quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">msg</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;  &quot;</span> <span class="o">*</span> <span class="n">_indentation</span> <span class="o">+</span> <span class="n">msg</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">_log_indent</span><span class="p">(</span><span class="n">msg</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Print a message and then indent the rest.&quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_indentation</span>
    <span class="n">_log</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>
    <span class="n">_indentation</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">_indentation</span>

<span class="k">def</span> <span class="nf">_log_unindent</span><span class="p">(</span><span class="n">msg</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Unindent then print a message.&quot;&quot;&quot;</span>
    <span class="k">global</span> <span class="n">_indentation</span>
    <span class="n">_indentation</span> <span class="o">=</span> <span class="n">_indentation</span> <span class="o">-</span> <span class="mi">2</span>
    <span class="n">_log</span><span class="p">(</span><span class="n">msg</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">log</span><span class="p">(</span><span class="n">func</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;A decorator for functions to log arguments and results.&quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="vm">__name__</span>
    <span class="k">def</span> <span class="nf">pp</span><span class="p">(</span><span class="n">v</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Print certain values more succinctly&quot;&quot;&quot;</span>
        <span class="n">vtype</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
            <span class="k">return</span> <span class="s2">&quot;(</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">pp_values</span><span class="p">(</span><span class="n">v</span><span class="p">))</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">thunder</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">proxies</span><span class="o">.</span><span class="n">TensorProxy</span><span class="p">):</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;TensorProxy(name=</span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">, shape=</span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, dtype=</span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">, device=</span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">)&quot;</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Tensor(shape=</span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">, stride=</span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">stride</span><span class="p">()</span><span class="si">}</span><span class="s2">, dtype=</span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">dtype</span><span class="si">}</span><span class="s2">, device=</span><span class="si">{</span><span class="n">v</span><span class="o">.</span><span class="n">device</span><span class="si">}</span><span class="s2">) with values </span><span class="si">{</span><span class="n">v</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">pp_values</span><span class="p">(</span><span class="n">args</span><span class="p">):</span>
        <span class="k">return</span> <span class="s2">&quot;, &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">pp</span><span class="p">(</span><span class="n">arg</span><span class="p">)</span> <span class="k">for</span> <span class="n">arg</span> <span class="ow">in</span> <span class="n">args</span><span class="p">])</span>

    <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">func_wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="n">_log_indent</span><span class="p">(</span><span class="s2">&quot;call </span><span class="si">{}</span><span class="s2">(</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">pp_values</span><span class="p">(</span><span class="n">args</span><span class="p">)))</span>
        <span class="n">res</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="n">_log_unindent</span><span class="p">(</span><span class="s2">&quot;|&lt;- </span><span class="si">{}</span><span class="s2"> = </span><span class="si">{}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">pp</span><span class="p">(</span><span class="n">res</span><span class="p">)))</span>
        <span class="k">return</span> <span class="n">res</span>

    <span class="k">return</span> <span class="n">func_wrapper</span>
</pre></div>
</div>
</div>
<p>We want to define operators <code class="docutils literal notranslate"><span class="pre">apex_xentropy_forward</span></code> and <code class="docutils literal notranslate"><span class="pre">apex_xentropy_backward</span></code>. In thunder, we define a <em>meta</em> function that only defines the metadata (like shapes) of outputs and the actual implementation for each operator and then register the pair with our executor. So we do this for the forward…</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@log</span>
<span class="k">def</span> <span class="nf">apex_xentropy_forward_meta</span><span class="p">(</span>
    <span class="n">a</span><span class="p">,</span>
    <span class="n">target</span><span class="p">,</span>
    <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">max_log_sum_exp</span> <span class="o">=</span> <span class="n">TensorProxy</span><span class="p">(</span><span class="n">like</span><span class="o">=</span><span class="n">target</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">TensorProxy</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="p">,</span>
                               <span class="n">requires_grad</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">),</span> <span class="n">max_log_sum_exp</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid reduction: </span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">xentropy_cuda</span>

<span class="nd">@log</span>
<span class="k">def</span> <span class="nf">apex_xentropy_forward_impl</span><span class="p">(</span>
    <span class="n">a</span><span class="p">,</span>
    <span class="n">target</span><span class="p">,</span>
    <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">losses</span><span class="p">,</span> <span class="n">max_log_sum_exp</span> <span class="o">=</span> <span class="n">xentropy_cuda</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="o">==</span> <span class="s2">&quot;none&quot;</span><span class="p">:</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="n">losses</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid reduction: </span><span class="si">{</span><span class="n">reduction</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">losses</span><span class="p">,</span> <span class="n">max_log_sum_exp</span>


<span class="n">apex_xentropy_forward</span> <span class="o">=</span> <span class="n">apex_xentropy_ex</span><span class="o">.</span><span class="n">register_operator</span><span class="p">(</span>
    <span class="s2">&quot;apex_xentropy_forward&quot;</span><span class="p">,</span> <span class="n">meta</span><span class="o">=</span><span class="n">apex_xentropy_forward_meta</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">apex_xentropy_forward_impl</span>
<span class="p">)</span>
<br/><br/></pre></div>
</div>
</div>
<p>…and the backward…</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@log</span>
<span class="k">def</span> <span class="nf">apex_xentropy_backward_meta</span><span class="p">(</span>
    <span class="n">grad</span><span class="p">,</span>
    <span class="n">logits</span><span class="p">,</span>
    <span class="n">labels</span><span class="p">,</span>
    <span class="n">max_log_sum_exp</span><span class="p">,</span>
    <span class="n">smoothing</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">return</span> <span class="n">TensorProxy</span><span class="p">(</span><span class="n">like</span><span class="o">=</span><span class="n">logits</span><span class="p">)</span>


<span class="nd">@log</span>
<span class="k">def</span> <span class="nf">apex_xentropy_backward_impl</span><span class="p">(</span>
    <span class="n">grad</span><span class="p">,</span>
    <span class="n">logits</span><span class="p">,</span>
    <span class="n">labels</span><span class="p">,</span>
    <span class="n">max_log_sum_exp</span><span class="p">,</span>
    <span class="n">smoothing</span><span class="p">,</span>
<span class="p">):</span>
    <span class="k">return</span> <span class="n">xentropy_cuda</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(),</span> <span class="n">logits</span><span class="p">,</span> <span class="n">max_log_sum_exp</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">smoothing</span><span class="p">)</span>

<span class="n">apex_xentropy_backward</span> <span class="o">=</span> <span class="n">apex_xentropy_ex</span><span class="o">.</span><span class="n">register_operator</span><span class="p">(</span>
    <span class="s2">&quot;apex_xentropy_backward&quot;</span><span class="p">,</span> <span class="n">meta</span><span class="o">=</span><span class="n">apex_xentropy_backward_meta</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">apex_xentropy_backward_impl</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>Because Thunder currently does not allow keyword arguments passed to the operators, we define a convenience wrapper:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">apex_xentropy</span><span class="p">(</span>
    <span class="n">a</span><span class="p">,</span>
    <span class="n">target</span><span class="p">,</span>
    <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">res</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">apex_xentropy_forward</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="p">,</span> <span class="n">size_average</span><span class="p">,</span> <span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduce</span><span class="p">,</span> <span class="n">reduction</span><span class="p">,</span> <span class="n">label_smoothing</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>
<br/></pre></div>
</div>
</div>
<p>We can now <code class="docutils literal notranslate"><span class="pre">thunder.jit</span></code> functions using our operator:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">apex_xentropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>

<span class="n">jfn</span> <span class="o">=</span> <span class="n">thunder</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">50257</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50257</span><span class="p">,</span> <span class="p">[</span><span class="mi">2048</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="n">actual_result</span> <span class="o">=</span> <span class="n">jfn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">expected_result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;deviation from pytorch implementation:&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">actual_result</span> <span class="o">-</span> <span class="n">expected_result</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
call apex_xentropy_forward_meta(TensorProxy(name=t_0, shape=(2048, 50257), dtype=float32, device=cuda:0), TensorProxy(name=t_1, shape=(2048,), dtype=int64, device=cuda:0), None, None, -100, None, none, 0.0)
|&lt;- apex_xentropy_forward_meta = (TensorProxy(name=t1, shape=(2048,), dtype=float32, device=cuda:0), TensorProxy(name=t0, shape=(2048,), dtype=int64, device=cuda:0))

call apex_xentropy_forward_impl(Tensor(shape=torch.Size([2048, 50257]), stride=(50257, 1), dtype=torch.float32, device=cuda:0) with values tensor([[ 0.1940,  2.1614, -0.1721,  ..., -0.4797,  1.4608, -0.5221],
        [ 1.8288,  0.2116,  0.1760,  ..., -0.1599,  0.1195,  0.0073],
        [-2.1704,  1.0396,  2.2924,  ...,  0.6021,  0.6498, -0.6316],
        ...,
        [ 0.4908, -0.3445,  2.6618,  ..., -2.0946, -0.2890,  0.1500],
        [-1.0561, -1.3547, -1.0354,  ...,  0.4304, -0.7882, -0.5496],
        [-0.6883, -1.3283,  0.3513,  ..., -0.6951,  0.2013, -1.0238]],
       device=&#39;cuda:0&#39;), Tensor(shape=torch.Size([2048]), stride=(1,), dtype=torch.int64, device=cuda:0) with values tensor([ 9132, 12067,  5347,  ...,  9268, 12534, 33582], device=&#39;cuda:0&#39;), None, None, -100, None, none, 0.0)
|&lt;- apex_xentropy_forward_impl = (Tensor(shape=torch.Size([2048]), stride=(1,), dtype=torch.float32, device=cuda:0) with values tensor([10.7236, 11.9374, 11.0063,  ..., 11.7434,  9.5018, 10.8008],
       device=&#39;cuda:0&#39;), Tensor(shape=torch.Size([2048]), stride=(1,), dtype=torch.float32, device=cuda:0) with values tensor([11.3132, 11.3291, 11.3287,  ..., 11.3279, 11.3251, 11.3301],
       device=&#39;cuda:0&#39;))

deviation from pytorch implementation: 9.5367431640625e-07
</pre></div></div>
</div>
<p>We can also inspect what program thunder recorded to admire the beauty of our operator being called:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">thunder</span><span class="o">.</span><span class="n">last_traces</span><span class="p">(</span><span class="n">jfn</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
# Constructed by Delete Last Used (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast()
def computation(logits, labels):
  # logits: &#34;cuda:0 f32[2048, 50257]&#34;
  # labels: &#34;cuda:0 i64[2048]&#34;
  (res, _) = apex_xentropy_forward(logits, labels, None, None, -100, None, &#39;none&#39;, 0.0)
  del logits, labels
  return res
</pre></div></div>
</div>
<p>But it might be more awesome to have Thunder automatically use our new operators if applicable. We can define a transformation to do this for us. This consists of two parts: - a <code class="docutils literal notranslate"><span class="pre">checker</span></code>function that takes the arguments of the function we want to replace (but with <code class="docutils literal notranslate"><span class="pre">Tensor</span></code> arguments replaced by <code class="docutils literal notranslate"><span class="pre">TensorProxy</span></code> ones) and outputs <code class="docutils literal notranslate"><span class="pre">True</span></code> if we handle this case and <code class="docutils literal notranslate"><span class="pre">False</span></code> if not. - an <code class="docutils literal notranslate"><span class="pre">execution_transform</span></code> that is just a function with the same parameters and same return value as the
function we want to replace and does the compute (as you would expect by calling our operator).</p>
<p>Note that we attach this implementation to the <code class="docutils literal notranslate"><span class="pre">thunder.torch.cross_entropy</span></code> <em>Symbol</em> (an operator as appearing in Thunder traces, just like our <code class="docutils literal notranslate"><span class="pre">apex_xentropy_forward</span></code> is a Symbol).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">apex_xentropy_checker</span><span class="p">(</span>
    <span class="n">a</span><span class="p">:</span> <span class="n">TensorProxy</span><span class="p">,</span>
    <span class="o">/</span><span class="p">,</span>
    <span class="n">target</span><span class="p">:</span> <span class="n">TensorProxy</span><span class="p">,</span>
    <span class="n">weight</span><span class="p">:</span> <span class="kc">None</span> <span class="o">|</span> <span class="n">TensorProxy</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">size_average</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">ignore_index</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">reduce</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="n">label_smoothing</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="n">DeviceType</span> <span class="o">=</span> <span class="n">thunder</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">DeviceType</span>
    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">devicetype</span> <span class="o">!=</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">CUDA</span> <span class="ow">or</span> <span class="n">target</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">devicetype</span> <span class="o">!=</span> <span class="n">DeviceType</span><span class="o">.</span><span class="n">CUDA</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="n">probability_target</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="n">thunder</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">same_shape</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">probability_target</span> <span class="ow">or</span> <span class="n">label_smoothing</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="n">torch_dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">thunder</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">to_torch_dtype</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">torch_dtype</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">ignore_index</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="c1"># NOTE These parameters are deprecated and not supported</span>
    <span class="k">if</span> <span class="n">size_average</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">reduce</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">reduction</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="s2">&quot;none&quot;</span><span class="p">]:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="c1"># Checks from</span>
    <span class="c1"># https://github.com/NVIDIA/apex/blob/7b2e71b0d4013f8e2f9f1c8dd21980ff1d76f1b6/apex/contrib/csrc/xentropy/xentropy_kernel.cu#L587-L590</span>
    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">numel</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="c1"># Xentropy kernel produces incorrect results if a.shape[1] is less</span>
    <span class="c1"># than 30 and not a multiple of 4</span>
    <span class="k">if</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mi">30</span> <span class="ow">and</span> <span class="n">a</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="mi">4</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>

    <span class="k">return</span> <span class="kc">True</span>

<span class="kn">from</span> <span class="nn">thunder.core.transforms</span> <span class="kn">import</span> <span class="n">get_grad</span><span class="p">,</span> <span class="n">put_grads</span>


<span class="k">def</span> <span class="nf">cross_entropy_to_apex</span><span class="p">(</span>
    <span class="n">a</span><span class="p">,</span>
    <span class="n">target</span><span class="p">,</span>
    <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">max_log_sum_exp</span> <span class="o">=</span> <span class="n">apex_xentropy_forward</span><span class="p">(</span>
        <span class="n">a</span><span class="p">,</span>
        <span class="n">target</span><span class="p">,</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">size_average</span><span class="p">,</span>
        <span class="n">ignore_index</span><span class="p">,</span>
        <span class="n">reduce</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">,</span>
        <span class="n">label_smoothing</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="n">apex_xentropy_ex</span><span class="o">.</span><span class="n">register_implementation</span><span class="p">(</span><span class="n">thunder</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">,</span> <span class="n">checker</span><span class="o">=</span><span class="n">apex_xentropy_checker</span><span class="p">,</span>
                                <span class="n">execution_transform</span><span class="o">=</span><span class="n">cross_entropy_to_apex</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We now can run the “unmodified” PyTorch function with <code class="docutils literal notranslate"><span class="pre">F.cross_entroy</span></code> and still get our implementation (but don’t forget the executor in the call to the jit!):</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>

<span class="n">jfn</span> <span class="o">=</span> <span class="n">thunder</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">executors</span><span class="o">=</span><span class="p">[</span><span class="n">apex_xentropy_ex</span><span class="p">])</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">50257</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50257</span><span class="p">,</span> <span class="p">[</span><span class="mi">2048</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="n">actual_result</span> <span class="o">=</span> <span class="n">jfn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">expected_result</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;deviation from pytorch implementation:&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">actual_result</span> <span class="o">-</span> <span class="n">expected_result</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="nb">print</span><span class="p">(</span><span class="n">thunder</span><span class="o">.</span><span class="n">last_traces</span><span class="p">(</span><span class="n">jfn</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
call apex_xentropy_forward_meta(TensorProxy(name=logits, shape=(2048, 50257), dtype=float32, device=cuda:0), TensorProxy(name=labels, shape=(2048,), dtype=int64, device=cuda:0), None, None, -100, None, none, 0.0)
|&lt;- apex_xentropy_forward_meta = (TensorProxy(name=t19, shape=(2048,), dtype=float32, device=cuda:0), TensorProxy(name=t18, shape=(2048,), dtype=int64, device=cuda:0))

call apex_xentropy_forward_impl(Tensor(shape=torch.Size([2048, 50257]), stride=(50257, 1), dtype=torch.float32, device=cuda:0) with values tensor([[ 1.2891, -0.2912,  0.6866,  ..., -1.5067,  1.3132, -0.7352],
        [-1.9077, -0.8366, -0.0747,  ...,  1.6109, -0.7460,  0.7346],
        [-1.0830, -0.2586,  0.0402,  ..., -0.2030, -1.0907, -1.7308],
        ...,
        [ 0.5805, -0.0830, -0.4658,  ..., -0.1023, -1.3720,  0.1850],
        [-0.8181,  1.3273,  0.8034,  ...,  1.2658, -1.4824,  0.0482],
        [ 0.9964, -1.8733,  0.3547,  ...,  0.0190, -0.3228,  0.4827]],
       device=&#39;cuda:0&#39;), Tensor(shape=torch.Size([2048]), stride=(1,), dtype=torch.int64, device=cuda:0) with values tensor([ 8137, 23633, 42622,  ..., 39128, 39817, 18664], device=&#39;cuda:0&#39;), None, None, -100, None, none, 0.0)
|&lt;- apex_xentropy_forward_impl = (Tensor(shape=torch.Size([2048]), stride=(1,), dtype=torch.float32, device=cuda:0) with values tensor([12.9479, 11.7810,  9.1981,  ..., 10.1080, 10.4095, 10.5884],
       device=&#39;cuda:0&#39;), Tensor(shape=torch.Size([2048]), stride=(1,), dtype=torch.float32, device=cuda:0) with values tensor([11.3268, 11.3203, 11.3294,  ..., 11.3251, 11.3333, 11.3225],
       device=&#39;cuda:0&#39;))

deviation from pytorch implementation: 9.5367431640625e-07
# Constructed by Delete Last Used (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast()
def computation(logits, labels):
  # logits: &#34;cuda:0 f32[2048, 50257]&#34;
  # labels: &#34;cuda:0 i64[2048]&#34;
  (t17, _) = apex_xentropy_forward(logits, labels, None, None, -100, None, &#39;none&#39;, 0.0)
  del logits, labels
  return t17
</pre></div></div>
</div>
<section id="So-what-is-with-the-backward?">
<h2>So what is with the backward?<a class="headerlink" href="#So-what-is-with-the-backward?" title="Permalink to this heading">¶</a></h2>
<p>Well, we can define a gradient function and register it along with our implementation.</p>
<p>We thought a lot about how our extension point for gradients looked like - PyTorch’s <code class="docutils literal notranslate"><span class="pre">autograd.Functions</span></code> is probably the most well-known way - and we felt that it would be nice to make the connection between tensors in the computation and their gradients explicit.</p>
<p>So the grad transform we implement below is a function that does the following:</p>
<ul class="simple">
<li><p>it takes the same arguments as the forward,</p></li>
<li><p>it computes the forward from its arguments,</p></li>
<li><p>it then uses <code class="docutils literal notranslate"><span class="pre">get_grad</span></code> to obtain the required gradients for the forward outputs,</p></li>
<li><p>computes the gradients for the inputs (this is the backward),</p></li>
<li><p>finally attaches the computed gradients to the respective tensors with <code class="docutils literal notranslate"><span class="pre">put_grad</span></code></p></li>
</ul>
<p>We supply the grad function as an additional argument of <code class="docutils literal notranslate"><span class="pre">register_implementation</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nd">@log</span>
<span class="k">def</span> <span class="nf">apex_cross_entropy_grad</span><span class="p">(</span>
    <span class="n">a</span><span class="p">,</span>
    <span class="n">target</span><span class="p">,</span>
    <span class="n">weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">size_average</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">reduce</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span>
    <span class="n">label_smoothing</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
<span class="p">):</span>
    <span class="n">loss</span><span class="p">,</span> <span class="n">max_log_sum_exp</span> <span class="o">=</span> <span class="n">apex_xentropy_forward</span><span class="p">(</span>
        <span class="n">a</span><span class="p">,</span>
        <span class="n">target</span><span class="p">,</span>
        <span class="n">weight</span><span class="p">,</span>
        <span class="n">size_average</span><span class="p">,</span>
        <span class="n">ignore_index</span><span class="p">,</span>
        <span class="n">reduce</span><span class="p">,</span>
        <span class="n">reduction</span><span class="p">,</span>
        <span class="n">label_smoothing</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">grad</span> <span class="o">=</span> <span class="n">get_grad</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">grad_logits</span> <span class="o">=</span> <span class="n">apex_xentropy_backward</span><span class="p">(</span>
        <span class="n">grad</span><span class="p">,</span>
        <span class="n">a</span><span class="p">,</span>
        <span class="n">target</span><span class="p">,</span>
        <span class="n">max_log_sum_exp</span><span class="p">,</span>
        <span class="n">label_smoothing</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">put_grads</span><span class="p">((</span><span class="n">a</span><span class="p">,),</span> <span class="p">(</span><span class="n">grad_logits</span><span class="p">,))</span>
    <span class="k">return</span> <span class="n">loss</span>

<span class="n">apex_xentropy_ex</span><span class="o">.</span><span class="n">register_implementation</span><span class="p">(</span><span class="n">thunder</span><span class="o">.</span><span class="n">torch</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">,</span> <span class="n">checker</span><span class="o">=</span><span class="n">apex_xentropy_checker</span><span class="p">,</span>
                                <span class="n">execution_transform</span><span class="o">=</span><span class="n">cross_entropy_to_apex</span><span class="p">,</span> <span class="n">grad_transform</span><span class="o">=</span><span class="n">apex_cross_entropy_grad</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>With these registrations, we can compile a function and it will be automatically transformed into forward and backward and wrapped in a PyTorch autograd.Function calling the backward trace computed by Thunder.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">thunder</span> <span class="kn">import</span> <span class="n">torch</span> <span class="k">as</span> <span class="n">ltorch</span>

<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">50257</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50257</span><span class="p">,</span> <span class="p">[</span><span class="mi">2048</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">cross_entropy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;none&quot;</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="n">cfn</span> <span class="o">=</span> <span class="n">thunder</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">,</span> <span class="n">executors</span><span class="o">=</span><span class="p">[</span><span class="n">apex_xentropy_ex</span><span class="p">])</span>

<span class="n">actual_loss</span> <span class="o">=</span> <span class="n">cfn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">go</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn_like</span><span class="p">(</span><span class="n">actual_loss</span><span class="p">)</span>

<span class="n">actual_grad</span><span class="p">,</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">actual_loss</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">go</span><span class="p">)</span>

<span class="n">expected_loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
<span class="n">expected_grad</span><span class="p">,</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">expected_loss</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">go</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Max error in loss:&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">actual_loss</span> <span class="o">-</span> <span class="n">expected_loss</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Max error in logits grad:&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">actual_grad</span> <span class="o">-</span> <span class="n">expected_grad</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="n">thunder</span><span class="o">.</span><span class="n">last_traces</span><span class="p">(</span><span class="n">cfn</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
call apex_cross_entropy_grad(TensorProxy(name=a, shape=(2048, 50257), dtype=float32, device=cuda:0), TensorProxy(name=target, shape=(2048,), dtype=int64, device=cuda:0), None, None, [IntegerProxy name=ignore_index, value=-1], None, none, [FloatProxy name=label_smoothing, value=0.0])
    call apex_xentropy_forward_meta(TensorProxy(name=a, shape=(2048, 50257), dtype=float32, device=cuda:0), TensorProxy(name=target, shape=(2048,), dtype=int64, device=cuda:0), None, None, [IntegerProxy name=ignore_index, value=-1], None, none, [FloatProxy name=label_smoothing, value=0.0])
    |&lt;- apex_xentropy_forward_meta = (TensorProxy(name=t1, shape=(2048,), dtype=float32, device=cuda:0), TensorProxy(name=t0, shape=(2048,), dtype=int64, device=cuda:0))

    call apex_xentropy_backward_meta(TensorProxy(name=t2, shape=(2048,), dtype=float32, device=cuda:0), TensorProxy(name=a, shape=(2048, 50257), dtype=float32, device=cuda:0), TensorProxy(name=target, shape=(2048,), dtype=int64, device=cuda:0), TensorProxy(name=t0, shape=(2048,), dtype=int64, device=cuda:0), [FloatProxy name=label_smoothing, value=0.0])
    |&lt;- apex_xentropy_backward_meta = TensorProxy(name=t3, shape=(2048, 50257), dtype=float32, device=cuda:0)

|&lt;- apex_cross_entropy_grad = TensorProxy(name=t1, shape=(2048,), dtype=float32, device=cuda:0)

call apex_xentropy_forward_meta(TensorProxy(name=logits, shape=(2048, 50257), dtype=float32, device=cuda:0), TensorProxy(name=labels, shape=(2048,), dtype=int64, device=cuda:0), None, None, -1, None, none, 0.0)
|&lt;- apex_xentropy_forward_meta = (TensorProxy(name=t1, shape=(2048,), dtype=float32, device=cuda:0), TensorProxy(name=t0, shape=(2048,), dtype=int64, device=cuda:0))

call apex_xentropy_backward_meta(TensorProxy(name=t2, shape=(2048,), dtype=float32, device=cuda:0), TensorProxy(name=logits, shape=(2048, 50257), dtype=float32, device=cuda:0), TensorProxy(name=labels, shape=(2048,), dtype=int64, device=cuda:0), TensorProxy(name=t0, shape=(2048,), dtype=int64, device=cuda:0), [FloatProxy name=f0, value=0.0])
|&lt;- apex_xentropy_backward_meta = TensorProxy(name=t3, shape=(2048, 50257), dtype=float32, device=cuda:0)

call apex_xentropy_forward_impl(Tensor(shape=torch.Size([2048, 50257]), stride=(50257, 1), dtype=torch.float32, device=cuda:0) with values tensor([[-9.2466e-01, -4.2534e-01, -2.6438e+00,  ...,  4.5115e-01,
          2.4087e-01,  1.9543e+00],
        [ 7.5610e-03, -4.9079e-01,  3.6572e-01,  ...,  2.5072e+00,
          9.0470e-01, -1.4305e+00],
        [-4.4104e-01, -7.6137e-01, -1.1172e+00,  ...,  5.9006e-02,
         -1.0212e+00,  3.0210e-02],
        ...,
        [-4.2869e+00,  1.4900e+00, -9.1910e-01,  ...,  3.6535e-03,
         -6.8372e-01,  7.1824e-01],
        [-4.2704e-02,  1.3505e+00,  2.1361e+00,  ..., -1.1139e+00,
          6.1626e-01,  4.8158e-01],
        [-7.3334e-01,  2.0820e+00,  3.7722e-02,  ..., -7.2141e-01,
          4.6871e-01,  7.0758e-01]], device=&#39;cuda:0&#39;, requires_grad=True), Tensor(shape=torch.Size([2048]), stride=(1,), dtype=torch.int64, device=cuda:0) with values tensor([ 3957, 45831, 13902,  ..., 45225, 32145, 12167], device=&#39;cuda:0&#39;), None, None, -1, None, none, 0.0)
|&lt;- apex_xentropy_forward_impl = (Tensor(shape=torch.Size([2048]), stride=(1,), dtype=torch.float32, device=cuda:0) with values tensor([12.4000, 10.9672, 12.6648,  ..., 11.7144, 11.8293, 10.9396],
       device=&#39;cuda:0&#39;), Tensor(shape=torch.Size([2048]), stride=(1,), dtype=torch.float32, device=cuda:0) with values tensor([11.3186, 11.3176, 11.3300,  ..., 11.3257, 11.3189, 11.3202],
       device=&#39;cuda:0&#39;))

call apex_xentropy_backward_impl(Tensor(shape=torch.Size([2048]), stride=(1,), dtype=torch.float32, device=cuda:0) with values tensor([ 0.8882, -0.0650, -1.2035,  ..., -0.4344, -0.0588, -2.5740],
       device=&#39;cuda:0&#39;), Tensor(shape=torch.Size([2048, 50257]), stride=(50257, 1), dtype=torch.float32, device=cuda:0) with values tensor([[-9.2466e-01, -4.2534e-01, -2.6438e+00,  ...,  4.5115e-01,
          2.4087e-01,  1.9543e+00],
        [ 7.5610e-03, -4.9079e-01,  3.6572e-01,  ...,  2.5072e+00,
          9.0470e-01, -1.4305e+00],
        [-4.4104e-01, -7.6137e-01, -1.1172e+00,  ...,  5.9006e-02,
         -1.0212e+00,  3.0210e-02],
        ...,
        [-4.2869e+00,  1.4900e+00, -9.1910e-01,  ...,  3.6535e-03,
         -6.8372e-01,  7.1824e-01],
        [-4.2704e-02,  1.3505e+00,  2.1361e+00,  ..., -1.1139e+00,
          6.1626e-01,  4.8158e-01],
        [-7.3334e-01,  2.0820e+00,  3.7722e-02,  ..., -7.2141e-01,
          4.6871e-01,  7.0758e-01]], device=&#39;cuda:0&#39;, requires_grad=True), Tensor(shape=torch.Size([2048]), stride=(1,), dtype=torch.int64, device=cuda:0) with values tensor([ 3957, 45831, 13902,  ..., 45225, 32145, 12167], device=&#39;cuda:0&#39;), Tensor(shape=torch.Size([2048]), stride=(1,), dtype=torch.float32, device=cuda:0) with values tensor([11.3186, 11.3176, 11.3300,  ..., 11.3257, 11.3189, 11.3202],
       device=&#39;cuda:0&#39;), 0.0)
|&lt;- apex_xentropy_backward_impl = Tensor(shape=torch.Size([2048, 50257]), stride=(50257, 1), dtype=torch.float32, device=cuda:0) with values tensor([[ 4.2787e-06,  7.0495e-06,  7.6679e-07,  ...,  1.6936e-05,
          1.3724e-05,  7.6143e-05],
        [-7.9652e-07, -4.8391e-07, -1.1396e-06,  ..., -9.7005e-06,
         -1.9535e-06, -1.8908e-07],
        [-9.2972e-06, -6.7489e-06, -4.7280e-06,  ..., -1.5329e-05,
         -5.2049e-06, -1.4894e-05],
        ...,
        [-7.2011e-08, -2.3243e-05, -2.0894e-06,  ..., -5.2573e-06,
         -2.6439e-06, -1.0743e-05],
        [-6.8437e-07, -2.7565e-06, -6.0470e-06,  ..., -2.3447e-07,
         -1.3227e-06, -1.1561e-06],
        [-1.4990e-05, -2.5033e-04, -3.2410e-05,  ..., -1.5170e-05,
         -4.9872e-05, -6.3328e-05]], device=&#39;cuda:0&#39;)

Max error in loss: 9.5367431640625e-07
Max error in logits grad: 2.384185791015625e-07
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[# Constructed by Backward pass
 import torch
 from thunder.executors.torchex import no_autocast

 @torch.no_grad()
 @no_autocast()
 def backward_fn(saved_for_backward, cotangents):
   # saved_for_backward: &#34;Collection&#34;
   # cotangents: &#34;Collection&#34;
   C0, \
   C1, \
   = saved_for_backward
   t2, \
   = cotangents
   logits, \
   labels, \
   t0, \
   = C0
   f0, \
   = C1
   t3 = apex_xentropy_backward(t2, logits, labels, t0, f0)  # t3: &#34;cuda:0 f32[2048, 50257]&#34;
   return (t3, None),
 # Constructed by Transform for execution (took 0 milliseconds)
 import torch
 from thunder.executors.torchex import no_autocast

 @torch.no_grad()
 @no_autocast()
 def backward_fn(saved_for_backward, cotangents):
   # saved_for_backward: &#34;Collection&#34;
   # cotangents: &#34;Collection&#34;
   C0, \
   C1, \
   = saved_for_backward
   t2, \
   = cotangents
   logits, \
   labels, \
   t0, \
   = C0
   f0, \
   = C1
   t3 = apex_xentropy_backward(t2, logits, labels, t0, f0)  # t3: &#34;cuda:0 f32[2048, 50257]&#34;
   return (t3, None),
 # Constructed by Update Call Context (took 0 milliseconds)
 import torch
 from thunder.executors.torchex import no_autocast

 @torch.no_grad()
 @no_autocast()
 def backward_fn(saved_for_backward, cotangents):
   # saved_for_backward: &#34;Collection&#34;
   # cotangents: &#34;Collection&#34;
   C0, \
   C1, \
   = saved_for_backward
   t2, \
   = cotangents
   labels, \
   logits, \
   t0, \
   = C0
   f0, \
   = C1
   t3 = apex_xentropy_backward(t2, logits, labels, t0, f0)  # t3: &#34;cuda:0 f32[2048, 50257]&#34;
   return (t3, None),
 # Constructed by Delete Last Used (took 0 milliseconds)
 import torch
 from thunder.executors.torchex import no_autocast

 @torch.no_grad()
 @no_autocast()
 def backward_fn(saved_for_backward, cotangents):
   # saved_for_backward: &#34;Collection&#34;
   # cotangents: &#34;Collection&#34;
   C0, \
   C1, \
   = saved_for_backward
   clear_collection(saved_for_backward)
   del saved_for_backward
   t2, \
   = cotangents
   clear_collection(cotangents)
   del cotangents
   labels, \
   logits, \
   t0, \
   = C0
   clear_collection(C0)
   del C0
   f0, \
   = C1
   clear_collection(C1)
   del C1
   t3 = apex_xentropy_backward(t2, logits, labels, t0, f0)  # t3: &#34;cuda:0 f32[2048, 50257]&#34;
   del t2, logits, labels, t0, f0
   return (t3, None)]
</pre></div></div>
</div>
<p>Alternatively, we can also use the <code class="docutils literal notranslate"><span class="pre">grad</span></code> transform to get the gradient:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">([</span><span class="mi">2048</span><span class="p">,</span> <span class="mi">50257</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">50257</span><span class="p">,</span> <span class="p">[</span><span class="mi">2048</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="n">grad_jfn</span> <span class="o">=</span> <span class="n">thunder</span><span class="o">.</span><span class="n">core</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">jfn</span><span class="p">)</span>
<span class="n">actual_grad</span><span class="p">,</span> <span class="o">=</span> <span class="n">grad_jfn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>

<span class="n">expected_grad</span><span class="p">,</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="n">logits</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Difference:&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">actual_grad</span> <span class="o">-</span> <span class="n">expected_grad</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="n">thunder</span><span class="o">.</span><span class="n">last_traces</span><span class="p">(</span><span class="n">grad_jfn</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
call apex_cross_entropy_grad(TensorProxy(name=logits, shape=(2048, 50257), dtype=float32, device=cuda:0), TensorProxy(name=labels, shape=(2048,), dtype=int64, device=cuda:0), None, None, -100, None, none, 0.0)
    call apex_xentropy_forward_meta(TensorProxy(name=logits, shape=(2048, 50257), dtype=float32, device=cuda:0), TensorProxy(name=labels, shape=(2048,), dtype=int64, device=cuda:0), None, None, -100, None, none, 0.0)
    |&lt;- apex_xentropy_forward_meta = (TensorProxy(name=t1, shape=(2048,), dtype=float32, device=cuda:0), TensorProxy(name=t0, shape=(2048,), dtype=int64, device=cuda:0))

    call apex_xentropy_backward_meta(TensorProxy(name=t2, shape=(2048,), dtype=float32, device=cuda:0), TensorProxy(name=logits, shape=(2048, 50257), dtype=float32, device=cuda:0), TensorProxy(name=labels, shape=(2048,), dtype=int64, device=cuda:0), TensorProxy(name=t0, shape=(2048,), dtype=int64, device=cuda:0), 0.0)
    |&lt;- apex_xentropy_backward_meta = TensorProxy(name=t3, shape=(2048, 50257), dtype=float32, device=cuda:0)

|&lt;- apex_cross_entropy_grad = TensorProxy(name=t1, shape=(2048,), dtype=float32, device=cuda:0)

call apex_xentropy_forward_impl(Tensor(shape=torch.Size([2048, 50257]), stride=(50257, 1), dtype=torch.float32, device=cuda:0) with values tensor([[ 0.5390,  0.1760, -1.0790,  ...,  0.1695, -0.8082, -0.6984],
        [ 2.1555,  1.3938,  0.3928,  ...,  0.8937, -0.4949,  1.1610],
        [ 0.6784,  1.1188,  0.7508,  ..., -0.0941,  0.8380,  0.1878],
        ...,
        [-1.5834, -0.1573, -1.3511,  ...,  0.6167, -0.1083,  0.4116],
        [-0.5476,  0.5831,  0.0791,  ..., -0.4986, -0.5270,  0.0954],
        [ 0.2825, -1.0378, -0.5506,  ...,  0.0149,  1.3521, -1.0823]],
       device=&#39;cuda:0&#39;, requires_grad=True), Tensor(shape=torch.Size([2048]), stride=(1,), dtype=torch.int64, device=cuda:0) with values tensor([44917, 35770, 41569,  ...,  9798, 33992, 36123], device=&#39;cuda:0&#39;), None, None, -100, None, none, 0.0)
|&lt;- apex_xentropy_forward_impl = (Tensor(shape=torch.Size([2048]), stride=(1,), dtype=torch.float32, device=cuda:0) with values tensor([10.0233, 11.9095, 11.2898,  ..., 10.9289, 10.7487, 10.7455],
       device=&#39;cuda:0&#39;), Tensor(shape=torch.Size([2048]), stride=(1,), dtype=torch.float32, device=cuda:0) with values tensor([11.3241, 11.3207, 11.3283,  ..., 11.3224, 11.3186, 11.3205],
       device=&#39;cuda:0&#39;))

call apex_xentropy_backward_impl(Tensor(shape=torch.Size([2048]), stride=(1,), dtype=torch.float32, device=cuda:0) with values tensor([1., 1., 1.,  ..., 1., 1., 1.], device=&#39;cuda:0&#39;), Tensor(shape=torch.Size([2048, 50257]), stride=(50257, 1), dtype=torch.float32, device=cuda:0) with values tensor([[ 0.5390,  0.1760, -1.0790,  ...,  0.1695, -0.8082, -0.6984],
        [ 2.1555,  1.3938,  0.3928,  ...,  0.8937, -0.4949,  1.1610],
        [ 0.6784,  1.1188,  0.7508,  ..., -0.0941,  0.8380,  0.1878],
        ...,
        [-1.5834, -0.1573, -1.3511,  ...,  0.6167, -0.1083,  0.4116],
        [-0.5476,  0.5831,  0.0791,  ..., -0.4986, -0.5270,  0.0954],
        [ 0.2825, -1.0378, -0.5506,  ...,  0.0149,  1.3521, -1.0823]],
       device=&#39;cuda:0&#39;, requires_grad=True), Tensor(shape=torch.Size([2048]), stride=(1,), dtype=torch.int64, device=cuda:0) with values tensor([44917, 35770, 41569,  ...,  9798, 33992, 36123], device=&#39;cuda:0&#39;), Tensor(shape=torch.Size([2048]), stride=(1,), dtype=torch.float32, device=cuda:0) with values tensor([11.3241, 11.3207, 11.3283,  ..., 11.3224, 11.3186, 11.3205],
       device=&#39;cuda:0&#39;), 0.0)
|&lt;- apex_xentropy_backward_impl = Tensor(shape=torch.Size([2048, 50257]), stride=(50257, 1), dtype=torch.float32, device=cuda:0) with values tensor([[2.0706e-05, 1.4403e-05, 4.1058e-06,  ..., 1.4309e-05, 5.3827e-06,
         6.0079e-06],
        [1.0461e-04, 4.8840e-05, 1.7949e-05,  ..., 2.9621e-05, 7.3879e-06,
         3.8697e-05],
        [2.3705e-05, 3.6822e-05, 2.5485e-05,  ..., 1.0948e-05, 2.7806e-05,
         1.4513e-05],
        ...,
        [2.4836e-06, 1.0338e-05, 3.1331e-06,  ..., 2.2417e-05, 1.0857e-05,
         1.8259e-05],
        [7.0235e-06, 2.1758e-05, 1.3145e-05,  ..., 7.3762e-06, 7.1699e-06,
         1.3360e-05],
        [1.6078e-05, 4.2941e-06, 6.9897e-06,  ..., 1.2304e-05, 4.6857e-05,
         4.1070e-06]], device=&#39;cuda:0&#39;)

Difference: 1.3969838619232178e-09
# Constructed by Delete Last Used (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast()
def computation(logits, labels):
  # logits: &#34;cuda:0 f32[2048, 50257]&#34;
  # labels: &#34;cuda:0 i64[2048]&#34;
  (_, t0) = apex_xentropy_forward(logits, labels, None, None, -100, None, &#39;none&#39;, 0.0)
  t4 = torch.full((2048,), 1.0, device=torch.device(&#34;cuda:0&#34;), dtype=torch.float32)  # t4: &#34;cuda:0 f32[2048]&#34;
    # t4 = ltorch.full((2048,), 1.0, device=torch.device(&#34;cuda:0&#34;), dtype=torch.float32)  # t4: &#34;cuda:0 f32[2048]&#34;
      # t4 = prims.full((2048,), 1.0, device=devices.Device(&#34;cuda:0&#34;), dtype=dtypes.float32)  # t4: &#34;cuda:0 f32[2048]&#34;
  t3 = apex_xentropy_backward(t4, logits, labels, t0, 0.0)  # t3: &#34;cuda:0 f32[2048, 50257]&#34;
  del t4, logits, labels, t0
  return [t3]
</pre></div></div>
</div>
<p>So let’s wrap up what we did here:</p>
<ul class="simple">
<li><p>We defined a custom executor with custom operations (Symbols in Thunder language), each with a <em>Meta-</em> (data propagation) <em>function</em> and an implementation.</p></li>
<li><p>We defined and registered rules to map existing operations to our new operations. This allows us to use optimizations on our model without changing the model’s code!</p></li>
<li><p>We defined a gradient rule and saw how our automatic PyTorch Autograd integration or the explicit <code class="docutils literal notranslate"><span class="pre">grad</span></code> transform uses it.</p></li>
</ul>
<p>Now go and implement your favourite optimized operators. We would love to hear about your use-cases!</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="dev_tutorials/extend.html" class="btn btn-neutral float-right" title="Extending Thunder" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="adding_custom_operator.html" class="btn btn-neutral" title="Defining new Thunder operators" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2024 Lightning-AI et al.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Defining custom forward and backward for existing operators</a><ul>
<li><a class="reference internal" href="#So-what-is-with-the-backward?">So what is with the backward?</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  

  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/sphinx_highlight.js"></script>
         <script src="../_static/clipboard.min.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script>let toggleHintShow = 'Click to show';</script>
         <script>let toggleHintHide = 'Click to hide';</script>
         <script>let toggleOpenOnPrint = 'true';</script>
         <script src="../_static/togglebutton.js"></script>
         <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <!-- <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources"> -->
    <!-- <div class="container"> -->
      <!-- <div class="row"> -->
        <!--
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://lightning-thunder.rtfd.io/en/latest">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://lightning-thunder.readthedocs.io/en/latest/#tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://lightning-thunder.readthedocs.io/en/latest/#community-examples">View Resources</a>
        </div>
        -->
      <!-- </div> -->
    <!-- </div> -->
  <!-- </div> -->

  <!--
  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://lightning-thunder.rtfd.io/en/latest/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://lightning-thunder.rtfd.io/en/latest/">PyTorch</a></li>
            <li><a href="https://lightning-thunder.readthedocs.io/en/latest/introduction_guide.html">Get Started</a></li>
            <li><a href="https://lightning-thunder.rtfd.io/en/latest/">Features</a></li>
            <li><a href="">Ecosystem</a></li>
            <li><a href="https://www.Lightning-AI.ai/blog">Blog</a></li>
            <li><a href="https://github.com/Lightning-AI/lightning-thunder/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://lightning-thunder.readthedocs.io/en/latest/#community-examples">Resources</a></li>
            <li><a href="https://lightning-thunder.readthedocs.io/en/latest/#tutorials">Tutorials</a></li>
            <li><a href="https://lightning-thunder.rtfd.io/en/latest">Docs</a></li>
            <li><a href="https://pytorch-lightning.slack.com" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/Lightning-AI/lightning-thunder/issues" target="_blank">Github Issues</a></li>
            <li><a href="" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/LightningAI" target="_blank" class="twitter"></a>
            <a href="" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>
  -->

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. Read PyTorch Lightning's <a href="https://pytorchlightning.ai/privacy-policy">Privacy Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://lightning-thunder.rtfd.io/en/latest/" aria-label="PyTorch Lightning"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://lightning-thunder.readthedocs.io/en/latest/introduction_guide.html">Get Started</a>
          </li>

          <li>
            <a href="https://www.Lightning-AI.ai/blog">Blog</a>
          </li>

          <li class="resources-mobile-menu-title">
            Ecosystem
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://lightning.ai/docs/pytorch/stable/">PyTorch Lightning</a>
            </li>

            <li>
              <a href="https://lightning.ai/docs/fabric/stable/">Lightning Fabric</a>
            </li>

            <li>
              <a href="https://torchmetrics.readthedocs.io/en/stable/">TorchMetrics</a>
            </li>

            <li>
              <a href="https://lightning.ai/docs/fabric/stable/">Fabric</a>
            </li>
          </ul>

          <!--<li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://lightning-thunder.readthedocs.io/en/latest/#community-examples">Developer Resources</a>
            </li>

            <li>
              <a href="https://lightning-thunder.rtfd.io/en/latest/">About</a>
            </li>

            <li>
              <a href="">Models (Beta)</a>
            </li>

            <li>
              <a href="">Community</a>
            </li>

            <li>
              <a href="">Forums</a>
            </li>
          </ul>-->

          <li>
            <a href="https://github.com/Lightning-AI/lightning-thunder">Github</a>
          </li>

          <li>
            <a href="https://www.lightning.ai/">Lightning.ai</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PQBQ3CV"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
 </body>
</html>