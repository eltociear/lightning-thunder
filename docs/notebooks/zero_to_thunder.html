


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <!-- Google Tag Manager -->
  <script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
  new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
  j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
  'https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
  })(window,document,'script','dataLayer','GTM-PQBQ3CV');
  </script>
  <!-- End Google Tag Manager -->

  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />

  <meta name="google-site-verification" content="okUst94cAlWSsUsGZTB4xSS4UKTtRV8Nu5XZ9pdd3Aw" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Zero to Thunder &mdash; lightning-thunder 0.1.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/icon.svg"/>
  
  
  
    <link rel="canonical" href="https://github.com/Lightning-AI/lightning-thunder/notebooks/zero_to_thunder.html"/>
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/copybutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/togglebutton.css" type="text/css" />
  <link rel="stylesheet" href="../_static/sphinx_paramlinks.css" type="text/css" />
  <link rel="stylesheet" href="../_static/nbsphinx-code-cells.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Thunder step by step" href="../basic/inspecting_traces.html" />
    <link rel="prev" title="Thunder Overview" href="../basic/overview.html" />
  <!-- Google Analytics -->
  
  <!-- End Google Analytics -->
  

  
  <script src="../_static/js/modernizr.min.js"></script>

  <!-- Preload the theme fonts -->

<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Light.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/UCity/UCity-Semibold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/Inconsolata/Inconsolata.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
  <script defer src="https://use.fontawesome.com/releases/v6.1.1/js/all.js" integrity="sha384-xBXmu0dk1bEoiwd71wOonQLyH+VpgR1XcDH3rtxrLww5ajNTuMvBdL5SOiFZnNdp" crossorigin="anonymous"></script>

  <script src="https://unpkg.com/react@18/umd/react.development.js" crossorigin></script>
  <script src="https://unpkg.com/react-dom@18/umd/react-dom.development.js" crossorigin></script>
  <script src="https://unpkg.com/babel-standalone@6/babel.min.js"></script>
  <script src="../_static/js/react/react.jsx" type="text/babel"></script>
</head>

<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://lightning-thunder.rtfd.io/en/latest/" aria-label="PyTorch Lightning">
      <!--  <img class="call-to-action-img" src="../_static/images/logo-lightning-icon.png"/> -->
      </a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://lightning-thunder.readthedocs.io/en/latest/introduction_guide.html">Get Started</a>
          </li>

          <li>
            <a href="https://www.Lightning-AI.ai/blog">Blog</a>
          </li>

          <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-orange-arrow">
                Docs
              </a>
              <div class="resources-dropdown-menu">
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning.ai/docs/pytorch/stable/">
                  <span class="dropdown-title">PyTorch Lightning</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning.ai/docs/fabric/stable/">
                  <span class="dropdown-title">Lightning Fabric</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://torchmetrics.readthedocs.io/en/stable/">
                  <span class="dropdown-title">TorchMetrics</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-flash.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Flash</span>
                </a>
                <a class="doc-dropdown-option nav-dropdown-item" href="https://lightning-bolts.readthedocs.io/en/stable/">
                  <span class="dropdown-title">Lightning Bolts</span>
                </a>
            </div>
          </li>

          <!--<li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
              <a class="resource-option with-down-arrow">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Community</span>
                  <p>Join the PyTorch developer community to contribute, learn, and get your questions answered.</p>
                </a>
                <a class="nav-dropdown-item" href="https://lightning-thunder.readthedocs.io/en/latest/#community-examples">
                  <span class="dropdown-title">Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="" target="_blank">
                  <span class="dropdown-title">Forums</span>
                  <p>A place to discuss PyTorch code, issues, install, research</p>
                </a>
                <a class="nav-dropdown-item" href="">
                  <span class="dropdown-title">Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
              </div>
            </div>
          </li>-->

          

          <li>
            <a href="https://github.com/Lightning-AI/lightning-thunder">GitHub</a>
          </li>

          <li>
            <a href="https://www.lightning.ai/">Lightning AI</a>
          </li>

        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>
  </div>
</div>

<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
                <div class="version">
                  0.1.0
                </div>
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Home</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../index.html">Welcome to ⚡ Lightning Thunder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fundamentals/installation.html">Install</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fundamentals/hello_world.html">Hello World</a></li>
<li class="toctree-l1"><a class="reference internal" href="../fundamentals/examine.html">Using examine</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Basic</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../basic/overview.html">Overview</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Zero to Thunder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic/inspecting_traces.html">Thunder step by step</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic/sharp_edges.html">The sharp edges</a></li>
<li class="toctree-l1"><a class="reference internal" href="../basic/mlp_mnist.html">Train a MLP on MNIST</a></li>
<li class="toctree-l1"><a class="reference internal" href="functional_jit.html">Functional jit</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Intermediate</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/additional_executors.html">Additional executors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/ddp.html">Distributed Data Parallel</a></li>
<li class="toctree-l1"><a class="reference internal" href="../intermediate/whats_next.html">What's next</a></li>
<li class="toctree-l1"><a class="reference internal" href="dev_tutorials/fsdp_tutorial.html">FSDP Under the Hood Tutorial</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Advanced</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../advanced/inside_thunder.html">Inside thunder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../advanced/extending.html">Extending thunder</a></li>
<li class="toctree-l1"><a class="reference internal" href="adding_custom_operator.html">Defining new Thunder operators</a></li>
<li class="toctree-l1"><a class="reference internal" href="adding_custom_operator_backward.html">Defining custom forward and backward for existing operators</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Experimental dev tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="dev_tutorials/extend.html">Extending Thunder</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">API reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../reference/thunder.html">thunder</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/common/index.html">thunder.common</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/core/index.html">thunder.core</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/clang/index.html">thunder.clang</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/examine/index.html">thunder.examine</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/distributed/index.html">thunder.distributed</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/executors/index.html">thunder.executors</a></li>
<li class="toctree-l1"><a class="reference internal" href="../reference/torch/index.html">thunder.torch</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
      <li>Zero to Thunder</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
            <a href="../_sources/notebooks/zero_to_thunder.ipynb.txt" rel="nofollow"><img src="../_static/images/view-page-source-icon.svg"></a>
          
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Shortcuts
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <section id="Zero-to-Thunder">
<h1>Zero to Thunder<a class="headerlink" href="#Zero-to-Thunder" title="Permalink to this heading">¶</a></h1>
<p>Here we take a very short tour of what is possible with Thunder.</p>
<p>To get started we import it (and a bunch of things for this notebook).</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">sys</span>
<span class="n">sys</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;..&#39;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">torch</span><span class="o">,</span> <span class="nn">thunder</span>
</pre></div>
</div>
</div>
<section id="Compiling-a-first-module-with-Thunder">
<h2>Compiling a first module with Thunder<a class="headerlink" href="#Compiling-a-first-module-with-Thunder" title="Permalink to this heading">¶</a></h2>
<p>So let’s get started! As a “Hello World”, let us apply it to it to a small model, say, the MLP part found in Llama 2. We take it from LitGPT.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">LLaMAMLP</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">intermediate_size</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">intermediate_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc_2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_embd</span><span class="p">,</span> <span class="n">intermediate_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">intermediate_size</span><span class="p">,</span> <span class="n">n_embd</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="n">x_fc_1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x_fc_2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc_2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">silu</span><span class="p">(</span><span class="n">x_fc_1</span><span class="p">)</span> <span class="o">*</span> <span class="n">x_fc_2</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">LLaMAMLP</span><span class="p">(</span><span class="mi">4096</span><span class="p">,</span> <span class="mi">11008</span><span class="p">)</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">p</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
LLaMAMLP(
  (fc_1): Linear(in_features=4096, out_features=11008, bias=False)
  (fc_2): Linear(in_features=4096, out_features=11008, bias=False)
  (proj): Linear(in_features=11008, out_features=4096, bias=False)
)
</pre></div></div>
</div>
<p>Now we can apply Thunder. This uses the most important function of Thunder, <code class="docutils literal notranslate"><span class="pre">thunder.jit</span></code>, which can be used to compile a <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> or a function. It will wrap our MLP in a <code class="docutils literal notranslate"><span class="pre">ThunderModule</span></code></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">thunder_model</span> <span class="o">=</span> <span class="n">thunder</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">thunder_model</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
ThunderModule(
  (_model): LLaMAMLP(
    (fc_1): Linear(in_features=4096, out_features=11008, bias=False)
    (fc_2): Linear(in_features=4096, out_features=11008, bias=False)
    (proj): Linear(in_features=11008, out_features=4096, bias=False)
  )
)
</pre></div></div>
</div>
<p>Our Thunder module computes (up to numerical accuracy) the same thing as our original model and for a small model like this, it also has approximately the same performance.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2048</span><span class="p">,</span> <span class="mi">4096</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;deviation:&#39;</span><span class="p">,</span> <span class="p">(</span><span class="n">thunder_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">-</span> <span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">))</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="o">%</span><span class="k">timeit</span> thunder_model(x); torch.cuda.synchronize()
<span class="o">%</span><span class="k">timeit</span> m(x); torch.cuda.synchronize()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
deviation: 1.4901161193847656e-07
61.3 ms ± 106 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
62.1 ms ± 89.8 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)
</pre></div></div>
</div>
<p>So what has changed? Quite a bit!</p>
<p>When we call the Thunder module, it do the computation in a single function without control flow. And what’s more, it applies optimizations, such as creating fusions for NVFuser to execute. We can see all this by showing the last computation trace:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">thunder</span><span class="o">.</span><span class="n">last_traces</span><span class="p">(</span><span class="n">thunder_model</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
# Constructed by Delete Last Used (took 0 milliseconds)
import torch
import torch.nn.functional
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast()
def computation(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):
  # x: &#34;cuda:0 f32[2, 2048, 4096]&#34;
  # t_fc_1_weight: &#34;cuda:0 f32[11008, 4096]&#34;
  # t_fc_2_weight: &#34;cuda:0 f32[11008, 4096]&#34;
  # t_proj_weight: &#34;cuda:0 f32[4096, 11008]&#34;
  x_fc_1 = torch.nn.functional.linear(x, t_fc_1_weight, None)  # x_fc_1: &#34;cuda:0 f32[2, 2048, 11008]&#34;
    # x_fc_1 = ltorch.linear(x, t_fc_1_weight, None)  # x_fc_1: &#34;cuda:0 f32[2, 2048, 11008]&#34;
      # x_fc_1 = prims.linear(x, t_fc_1_weight, None)  # x_fc_1: &#34;cuda:0 f32[2, 2048, 11008]&#34;
  del t_fc_1_weight
  x_fc_2 = torch.nn.functional.linear(x, t_fc_2_weight, None)  # x_fc_2: &#34;cuda:0 f32[2, 2048, 11008]&#34;
    # x_fc_2 = ltorch.linear(x, t_fc_2_weight, None)  # x_fc_2: &#34;cuda:0 f32[2, 2048, 11008]&#34;
      # x_fc_2 = prims.linear(x, t_fc_2_weight, None)  # x_fc_2: &#34;cuda:0 f32[2, 2048, 11008]&#34;
  del x, t_fc_2_weight
  [result] = nvFusion0(x_fc_1, x_fc_2)
    # t9 = prims.neg(x_fc_1)  # t9: &#34;cuda:0 f32[2, 2048, 11008]&#34;
    # t10 = prims.exp(t9)  # t10: &#34;cuda:0 f32[2, 2048, 11008]&#34;
    # t11 = prims.add(1.0, t10)  # t11: &#34;cuda:0 f32[2, 2048, 11008]&#34;
    # t12 = prims.reciprocal(t11)  # t12: &#34;cuda:0 f32[2, 2048, 11008]&#34;
    # a = prims.mul(x_fc_1, t12)  # a: &#34;cuda:0 f32[2, 2048, 11008]&#34;
    # result = prims.mul(a, x_fc_2)  # result: &#34;cuda:0 f32[2, 2048, 11008]&#34;
  del x_fc_1, x_fc_2
  t18 = torch.nn.functional.linear(result, t_proj_weight, None)  # t18: &#34;cuda:0 f32[2, 2048, 4096]&#34;
    # t18 = ltorch.linear(result, t_proj_weight, None)  # t18: &#34;cuda:0 f32[2, 2048, 4096]&#34;
      # t18 = prims.linear(result, t_proj_weight, None)  # t18: &#34;cuda:0 f32[2, 2048, 4096]&#34;
  del result, t_proj_weight
  return t18
</pre></div></div>
</div>
<p>For more detail of what is going on in this trace: - Thunder has transformed the computation (more precisely, <code class="docutils literal notranslate"><span class="pre">m.__call__</span></code>) into a single function which has all the MLP parameters as arguments. - It has recorded the tensor metadata. - Operations have been mapped from the PyTorch functions to <code class="docutils literal notranslate"><span class="pre">thunder.torch</span></code>(aka <code class="docutils literal notranslate"><span class="pre">ltorch</span></code>) equivalents and decomposed into <em>primitive operations</em>. - The multiplication and activation (<code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">torch.nn.functional.silu(x_fc_1)</span> <span class="pre">*</span> <span class="pre">x_fc_2</span></code>have been put into one
NVFuser fusion. (NVFuser here is (a particularly important) one of many optimizations, and we make it easy to add your own.) - You can see how the parameters are obtained and the metadata is checked in the prologue - get it through <code class="docutils literal notranslate"><span class="pre">thunder.last_prologue_traces(thunder_model)[-1]</span></code>.</p>
<p>You can actually see the series of traces, <code class="docutils literal notranslate"><span class="pre">last_traces</span></code> gives you a list of transformed traces in chronological order - for example the initial trace <code class="docutils literal notranslate"><span class="pre">thunder.last_traces(thunder_model)[0]</span></code> does not have the fusion yet.</p>
</section>
<section id="Compiling-a-more-complex-model">
<h2>Compiling a more complex model<a class="headerlink" href="#Compiling-a-more-complex-model" title="Permalink to this heading">¶</a></h2>
<p>Obviously, we aim for larger models, so we can do the same with the entire LLama 2 (well, we have a smaller momdel here to be mild to our CI, but if you have a large GPU, just drop reducing the number of layers):</p>
<p><strong>NOTE</strong>: For running the cells below, we require <code class="docutils literal notranslate"><span class="pre">litgpt</span></code> which can be installed with <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">'litgpt[all]</span> <span class="pre">&#64;</span> <span class="pre">git+https://github.com/Lightning-AI/litgpt'</span></code>. See <a class="reference external" href="https://github.com/Lightning-AI/litgpt">here</a> to learn more about litgpt.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">lit_gpt</span> <span class="kn">import</span> <span class="n">GPT</span>
<span class="kn">from</span> <span class="nn">thunder.tests.lit_gpt_model</span> <span class="kn">import</span> <span class="n">Config</span>
<span class="n">cfg</span> <span class="o">=</span> <span class="n">Config</span><span class="o">.</span><span class="n">from_name</span><span class="p">(</span><span class="s1">&#39;Llama-2-7b-hf&#39;</span><span class="p">)</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">n_layer</span> <span class="o">=</span> <span class="mi">16</span> <span class="c1"># fewer layers</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_default_dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">GPT</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
<span class="n">m</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
GPT(
  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
  (transformer): ModuleDict(
    (wte): Embedding(32000, 4096)
    (h): ModuleList(
      (0-15): 16 x Block(
        (norm_1): RMSNorm()
        (attn): CausalSelfAttention(
          (attn): Linear(in_features=4096, out_features=12288, bias=False)
          (proj): Linear(in_features=4096, out_features=4096, bias=False)
        )
        (norm_2): RMSNorm()
        (mlp): LLaMAMLP(
          (fc_1): Linear(in_features=4096, out_features=11008, bias=False)
          (fc_2): Linear(in_features=4096, out_features=11008, bias=False)
          (proj): Linear(in_features=11008, out_features=4096, bias=False)
        )
      )
    )
    (ln_f): RMSNorm()
  )
)
</pre></div></div>
</div>
<p>Again we jit our model and compare the output…</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">thunder_model</span> <span class="o">=</span> <span class="n">thunder</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

<span class="n">inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">512</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>

<span class="n">actual</span> <span class="o">=</span> <span class="n">thunder_model</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;deviation:&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">actual</span> <span class="o">-</span> <span class="n">expected</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
deviation: 0.03125
</pre></div></div>
</div>
<p>One thing to keep in mind here is that for bf16, the numerical accuracy impact of rearranging operations can be quite pronounced.</p>
<p>Just like before, we can see the program it ran, it is a lot longer, though.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="n">actual</span><span class="o">.</span><span class="n">grad_fn</span><span class="p">)</span>
<span class="n">thunder</span><span class="o">.</span><span class="n">last_traces</span><span class="p">(</span><span class="n">thunder_model</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&lt;torch.autograd.function.ThunderFunctionBackward object at 0x7f923f792ac0&gt;
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
# Constructed by Delete Last Used (took 10 milliseconds)
import torch
from torch import Tensor
import torch.nn.functional
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast()
def augmented_forward_fn(*args):
  # args: &#34;Collection&#34;
  t0, \
  t1, \
  t2, \
  t3, \
  t4, \
  t5, \
  t6, \
  t7, \
  t8, \
  t9, \
  t10, \
  t11, \
  t12, \
  t13, \
  t14, \
  t15, \
  t16, \
  t17, \
  t18, \
  t19, \
  t20, \
  t21, \
  t22, \
  t23, \
  t24, \
  t25, \
  t26, \
  t27, \
  t28, \
  t29, \
  t30, \
  t31, \
  t32, \
  t33, \
  t34, \
  t35, \
  t36, \
  t37, \
  t38, \
  t39, \
  t40, \
  t41, \
  t42, \
  t43, \
  t44, \
  t45, \
  t46, \
  t47, \
  t48, \
  t49, \
  t50, \
  t51, \
  t52, \
  t53, \
  t54, \
  t55, \
  t56, \
  t57, \
  t58, \
  t59, \
  t60, \
  t61, \
  t62, \
  t63, \
  t64, \
  t65, \
  t66, \
  t67, \
  t68, \
  t69, \
  t70, \
  t71, \
  t72, \
  t73, \
  t74, \
  t75, \
  t76, \
  t77, \
  t78, \
  t79, \
  t80, \
  t81, \
  t82, \
  t83, \
  t84, \
  t85, \
  t86, \
  t87, \
  t88, \
  t89, \
  t90, \
  t91, \
  t92, \
  t93, \
  t94, \
  t95, \
  t96, \
  t97, \
  t98, \
  t99, \
  t100, \
  t101, \
  t102, \
  t103, \
  t104, \
  t105, \
  t106, \
  t107, \
  t108, \
  t109, \
  t110, \
  t111, \
  t112, \
  t113, \
  t114, \
  t115, \
  t116, \
  t117, \
  = args
  del args
  t122 = torch.nn.functional.embedding(t0, t117, None, None, 2.0, False, False)  # t122: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t122 = ltorch.embedding(t0, t117, None, None, 2.0, False, False)  # t122: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1867 = ltorch.reshape(t0, [512])  # t1867: &#34;cuda:0 i64[512]&#34;
        # t1867 = prims.reshape(t0, (512,))  # t1867: &#34;cuda:0 i64[512]&#34;
      # t1868 = prims.take(t117, t1867, 0)  # t1868: &#34;cuda:0 bf16[512, 4096]&#34;
      # t122 = ltorch.reshape(t1868, [1, 512, 4096])  # t122: &#34;cuda:0 bf16[1, 512, 4096]&#34;
        # t122 = prims.reshape(t1868, (1, 512, 4096))  # t122: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t118 = torch_slice_prim_impl(t1, [0, 0], [512, 128], [1, 1])  # t118: &#34;cuda:0 f32[512, 128]&#34;
  t119 = torch_slice_prim_impl(t2, [0, 0], [512, 128], [1, 1])  # t119: &#34;cuda:0 f32[512, 128]&#34;
  t2015 = torch.unsqueeze(t53, 0)  # t2015: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2015 = ltorch.unsqueeze(t53, 0)  # t2015: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2015 = prims.broadcast_in_dim(t53, [1, 4096], [1])  # t2015: &#34;cuda:0 bf16[1, 4096]&#34;
  t2016 = torch.unsqueeze(t2015, 1)  # t2016: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2016 = ltorch.unsqueeze(t2015, 1)  # t2016: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2016 = prims.broadcast_in_dim(t2015, [1, 1, 4096], [0, 2])  # t2016: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2015
  t133 = Tensor.expand(t2016, (1, 512, 4096))  # t133: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t133 = ltorch.expand(t2016, (1, 512, 4096))  # t133: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t133 = prims.broadcast_in_dim(t2016, (1, 512, 4096), (0, 1, 2))  # t133: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2016
  t2356 = torch.unsqueeze(t82, 0)  # t2356: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2356 = ltorch.unsqueeze(t82, 0)  # t2356: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2356 = prims.broadcast_in_dim(t82, [1, 4096], [1])  # t2356: &#34;cuda:0 bf16[1, 4096]&#34;
  t2357 = torch.unsqueeze(t2356, 1)  # t2357: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2357 = ltorch.unsqueeze(t2356, 1)  # t2357: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2357 = prims.broadcast_in_dim(t2356, [1, 1, 4096], [0, 2])  # t2357: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2356
  t1609 = Tensor.expand(t2357, (1, 512, 4096))  # t1609: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1609 = ltorch.expand(t2357, (1, 512, 4096))  # t1609: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1609 = prims.broadcast_in_dim(t2357, (1, 512, 4096), (0, 1, 2))  # t1609: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2357
  t2359 = torch.unsqueeze(t58, 0)  # t2359: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2359 = ltorch.unsqueeze(t58, 0)  # t2359: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2359 = prims.broadcast_in_dim(t58, [1, 4096], [1])  # t2359: &#34;cuda:0 bf16[1, 4096]&#34;
  t2360 = torch.unsqueeze(t2359, 1)  # t2360: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2360 = ltorch.unsqueeze(t2359, 1)  # t2360: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2360 = prims.broadcast_in_dim(t2359, [1, 1, 4096], [0, 2])  # t2360: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2359
  t1645 = Tensor.expand(t2360, (1, 512, 4096))  # t1645: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1645 = ltorch.expand(t2360, (1, 512, 4096))  # t1645: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1645 = prims.broadcast_in_dim(t2360, (1, 512, 4096), (0, 1, 2))  # t1645: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2360
  t2044 = torch.unsqueeze(t69, 0)  # t2044: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2044 = ltorch.unsqueeze(t69, 0)  # t2044: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2044 = prims.broadcast_in_dim(t69, [1, 4096], [1])  # t2044: &#34;cuda:0 bf16[1, 4096]&#34;
  t2045 = torch.unsqueeze(t2044, 1)  # t2045: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2045 = ltorch.unsqueeze(t2044, 1)  # t2045: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2045 = prims.broadcast_in_dim(t2044, [1, 1, 4096], [0, 2])  # t2045: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2044
  t205 = Tensor.expand(t2045, (1, 512, 4096))  # t205: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t205 = ltorch.expand(t2045, (1, 512, 4096))  # t205: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t205 = prims.broadcast_in_dim(t2045, (1, 512, 4096), (0, 1, 2))  # t205: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2045
  t2380 = torch.unsqueeze(t83, 0)  # t2380: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2380 = ltorch.unsqueeze(t83, 0)  # t2380: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2380 = prims.broadcast_in_dim(t83, [1, 4096], [1])  # t2380: &#34;cuda:0 bf16[1, 4096]&#34;
  t2381 = torch.unsqueeze(t2380, 1)  # t2381: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2381 = ltorch.unsqueeze(t2380, 1)  # t2381: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2381 = prims.broadcast_in_dim(t2380, [1, 1, 4096], [0, 2])  # t2381: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2380
  t1717 = Tensor.expand(t2381, (1, 512, 4096))  # t1717: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1717 = ltorch.expand(t2381, (1, 512, 4096))  # t1717: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1717 = prims.broadcast_in_dim(t2381, (1, 512, 4096), (0, 1, 2))  # t1717: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2381
  t2047 = torch.unsqueeze(t60, 0)  # t2047: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2047 = ltorch.unsqueeze(t60, 0)  # t2047: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2047 = prims.broadcast_in_dim(t60, [1, 4096], [1])  # t2047: &#34;cuda:0 bf16[1, 4096]&#34;
  t2048 = torch.unsqueeze(t2047, 1)  # t2048: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2048 = ltorch.unsqueeze(t2047, 1)  # t2048: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2048 = prims.broadcast_in_dim(t2047, [1, 1, 4096], [0, 2])  # t2048: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2047
  t241 = Tensor.expand(t2048, (1, 512, 4096))  # t241: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t241 = ltorch.expand(t2048, (1, 512, 4096))  # t241: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t241 = prims.broadcast_in_dim(t2048, (1, 512, 4096), (0, 1, 2))  # t241: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2048
  t2383 = torch.unsqueeze(t59, 0)  # t2383: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2383 = ltorch.unsqueeze(t59, 0)  # t2383: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2383 = prims.broadcast_in_dim(t59, [1, 4096], [1])  # t2383: &#34;cuda:0 bf16[1, 4096]&#34;
  t2384 = torch.unsqueeze(t2383, 1)  # t2384: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2384 = ltorch.unsqueeze(t2383, 1)  # t2384: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2384 = prims.broadcast_in_dim(t2383, [1, 1, 4096], [0, 2])  # t2384: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2383
  t1753 = Tensor.expand(t2384, (1, 512, 4096))  # t1753: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1753 = ltorch.expand(t2384, (1, 512, 4096))  # t1753: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1753 = prims.broadcast_in_dim(t2384, (1, 512, 4096), (0, 1, 2))  # t1753: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2384
  t2068 = torch.unsqueeze(t70, 0)  # t2068: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2068 = ltorch.unsqueeze(t70, 0)  # t2068: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2068 = prims.broadcast_in_dim(t70, [1, 4096], [1])  # t2068: &#34;cuda:0 bf16[1, 4096]&#34;
  t2069 = torch.unsqueeze(t2068, 1)  # t2069: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2069 = ltorch.unsqueeze(t2068, 1)  # t2069: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2069 = prims.broadcast_in_dim(t2068, [1, 1, 4096], [0, 2])  # t2069: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2068
  t313 = Tensor.expand(t2069, (1, 512, 4096))  # t313: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t313 = ltorch.expand(t2069, (1, 512, 4096))  # t313: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t313 = prims.broadcast_in_dim(t2069, (1, 512, 4096), (0, 1, 2))  # t313: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2069
  t2404 = torch.unsqueeze(t84, 0)  # t2404: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2404 = ltorch.unsqueeze(t84, 0)  # t2404: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2404 = prims.broadcast_in_dim(t84, [1, 4096], [1])  # t2404: &#34;cuda:0 bf16[1, 4096]&#34;
  t2405 = torch.unsqueeze(t2404, 1)  # t2405: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2405 = ltorch.unsqueeze(t2404, 1)  # t2405: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2405 = prims.broadcast_in_dim(t2404, [1, 1, 4096], [0, 2])  # t2405: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2404
  t1825 = Tensor.expand(t2405, (1, 512, 4096))  # t1825: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1825 = ltorch.expand(t2405, (1, 512, 4096))  # t1825: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1825 = prims.broadcast_in_dim(t2405, (1, 512, 4096), (0, 1, 2))  # t1825: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2405
  t2071 = torch.unsqueeze(t61, 0)  # t2071: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2071 = ltorch.unsqueeze(t61, 0)  # t2071: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2071 = prims.broadcast_in_dim(t61, [1, 4096], [1])  # t2071: &#34;cuda:0 bf16[1, 4096]&#34;
  t2072 = torch.unsqueeze(t2071, 1)  # t2072: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2072 = ltorch.unsqueeze(t2071, 1)  # t2072: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2072 = prims.broadcast_in_dim(t2071, [1, 1, 4096], [0, 2])  # t2072: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2071
  t349 = Tensor.expand(t2072, (1, 512, 4096))  # t349: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t349 = ltorch.expand(t2072, (1, 512, 4096))  # t349: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t349 = prims.broadcast_in_dim(t2072, (1, 512, 4096), (0, 1, 2))  # t349: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2072
  t2407 = torch.unsqueeze(t52, 0)  # t2407: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2407 = ltorch.unsqueeze(t52, 0)  # t2407: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2407 = prims.broadcast_in_dim(t52, [1, 4096], [1])  # t2407: &#34;cuda:0 bf16[1, 4096]&#34;
  t2408 = torch.unsqueeze(t2407, 1)  # t2408: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2408 = ltorch.unsqueeze(t2407, 1)  # t2408: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2408 = prims.broadcast_in_dim(t2407, [1, 1, 4096], [0, 2])  # t2408: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2407
  t1861 = Tensor.expand(t2408, (1, 512, 4096))  # t1861: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1861 = ltorch.expand(t2408, (1, 512, 4096))  # t1861: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1861 = prims.broadcast_in_dim(t2408, (1, 512, 4096), (0, 1, 2))  # t1861: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2408
  t2095 = torch.unsqueeze(t62, 0)  # t2095: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2095 = ltorch.unsqueeze(t62, 0)  # t2095: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2095 = prims.broadcast_in_dim(t62, [1, 4096], [1])  # t2095: &#34;cuda:0 bf16[1, 4096]&#34;
  t2096 = torch.unsqueeze(t2095, 1)  # t2096: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2096 = ltorch.unsqueeze(t2095, 1)  # t2096: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2096 = prims.broadcast_in_dim(t2095, [1, 1, 4096], [0, 2])  # t2096: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2095
  t457 = Tensor.expand(t2096, (1, 512, 4096))  # t457: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t457 = ltorch.expand(t2096, (1, 512, 4096))  # t457: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t457 = prims.broadcast_in_dim(t2096, (1, 512, 4096), (0, 1, 2))  # t457: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2096
  t2092 = torch.unsqueeze(t71, 0)  # t2092: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2092 = ltorch.unsqueeze(t71, 0)  # t2092: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2092 = prims.broadcast_in_dim(t71, [1, 4096], [1])  # t2092: &#34;cuda:0 bf16[1, 4096]&#34;
  t2093 = torch.unsqueeze(t2092, 1)  # t2093: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2093 = ltorch.unsqueeze(t2092, 1)  # t2093: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2093 = prims.broadcast_in_dim(t2092, [1, 1, 4096], [0, 2])  # t2093: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2092
  t421 = Tensor.expand(t2093, (1, 512, 4096))  # t421: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t421 = ltorch.expand(t2093, (1, 512, 4096))  # t421: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t421 = prims.broadcast_in_dim(t2093, (1, 512, 4096), (0, 1, 2))  # t421: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2093
  t2116 = torch.unsqueeze(t72, 0)  # t2116: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2116 = ltorch.unsqueeze(t72, 0)  # t2116: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2116 = prims.broadcast_in_dim(t72, [1, 4096], [1])  # t2116: &#34;cuda:0 bf16[1, 4096]&#34;
  t2117 = torch.unsqueeze(t2116, 1)  # t2117: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2117 = ltorch.unsqueeze(t2116, 1)  # t2117: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2117 = prims.broadcast_in_dim(t2116, [1, 1, 4096], [0, 2])  # t2117: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2116
  t529 = Tensor.expand(t2117, (1, 512, 4096))  # t529: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t529 = ltorch.expand(t2117, (1, 512, 4096))  # t529: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t529 = prims.broadcast_in_dim(t2117, (1, 512, 4096), (0, 1, 2))  # t529: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2117
  t2119 = torch.unsqueeze(t63, 0)  # t2119: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2119 = ltorch.unsqueeze(t63, 0)  # t2119: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2119 = prims.broadcast_in_dim(t63, [1, 4096], [1])  # t2119: &#34;cuda:0 bf16[1, 4096]&#34;
  t2120 = torch.unsqueeze(t2119, 1)  # t2120: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2120 = ltorch.unsqueeze(t2119, 1)  # t2120: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2120 = prims.broadcast_in_dim(t2119, [1, 1, 4096], [0, 2])  # t2120: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2119
  t565 = Tensor.expand(t2120, (1, 512, 4096))  # t565: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t565 = ltorch.expand(t2120, (1, 512, 4096))  # t565: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t565 = prims.broadcast_in_dim(t2120, (1, 512, 4096), (0, 1, 2))  # t565: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2120
  t2140 = torch.unsqueeze(t73, 0)  # t2140: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2140 = ltorch.unsqueeze(t73, 0)  # t2140: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2140 = prims.broadcast_in_dim(t73, [1, 4096], [1])  # t2140: &#34;cuda:0 bf16[1, 4096]&#34;
  t2141 = torch.unsqueeze(t2140, 1)  # t2141: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2141 = ltorch.unsqueeze(t2140, 1)  # t2141: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2141 = prims.broadcast_in_dim(t2140, [1, 1, 4096], [0, 2])  # t2141: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2140
  t637 = Tensor.expand(t2141, (1, 512, 4096))  # t637: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t637 = ltorch.expand(t2141, (1, 512, 4096))  # t637: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t637 = prims.broadcast_in_dim(t2141, (1, 512, 4096), (0, 1, 2))  # t637: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2141
  t2143 = torch.unsqueeze(t64, 0)  # t2143: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2143 = ltorch.unsqueeze(t64, 0)  # t2143: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2143 = prims.broadcast_in_dim(t64, [1, 4096], [1])  # t2143: &#34;cuda:0 bf16[1, 4096]&#34;
  t2144 = torch.unsqueeze(t2143, 1)  # t2144: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2144 = ltorch.unsqueeze(t2143, 1)  # t2144: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2144 = prims.broadcast_in_dim(t2143, [1, 1, 4096], [0, 2])  # t2144: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2143
  t673 = Tensor.expand(t2144, (1, 512, 4096))  # t673: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t673 = ltorch.expand(t2144, (1, 512, 4096))  # t673: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t673 = prims.broadcast_in_dim(t2144, (1, 512, 4096), (0, 1, 2))  # t673: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2144
  t2164 = torch.unsqueeze(t74, 0)  # t2164: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2164 = ltorch.unsqueeze(t74, 0)  # t2164: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2164 = prims.broadcast_in_dim(t74, [1, 4096], [1])  # t2164: &#34;cuda:0 bf16[1, 4096]&#34;
  t2165 = torch.unsqueeze(t2164, 1)  # t2165: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2165 = ltorch.unsqueeze(t2164, 1)  # t2165: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2165 = prims.broadcast_in_dim(t2164, [1, 1, 4096], [0, 2])  # t2165: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2164
  t745 = Tensor.expand(t2165, (1, 512, 4096))  # t745: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t745 = ltorch.expand(t2165, (1, 512, 4096))  # t745: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t745 = prims.broadcast_in_dim(t2165, (1, 512, 4096), (0, 1, 2))  # t745: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2165
  t2167 = torch.unsqueeze(t65, 0)  # t2167: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2167 = ltorch.unsqueeze(t65, 0)  # t2167: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2167 = prims.broadcast_in_dim(t65, [1, 4096], [1])  # t2167: &#34;cuda:0 bf16[1, 4096]&#34;
  t2168 = torch.unsqueeze(t2167, 1)  # t2168: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2168 = ltorch.unsqueeze(t2167, 1)  # t2168: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2168 = prims.broadcast_in_dim(t2167, [1, 1, 4096], [0, 2])  # t2168: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2167
  t781 = Tensor.expand(t2168, (1, 512, 4096))  # t781: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t781 = ltorch.expand(t2168, (1, 512, 4096))  # t781: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t781 = prims.broadcast_in_dim(t2168, (1, 512, 4096), (0, 1, 2))  # t781: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2168
  t2188 = torch.unsqueeze(t75, 0)  # t2188: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2188 = ltorch.unsqueeze(t75, 0)  # t2188: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2188 = prims.broadcast_in_dim(t75, [1, 4096], [1])  # t2188: &#34;cuda:0 bf16[1, 4096]&#34;
  t2189 = torch.unsqueeze(t2188, 1)  # t2189: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2189 = ltorch.unsqueeze(t2188, 1)  # t2189: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2189 = prims.broadcast_in_dim(t2188, [1, 1, 4096], [0, 2])  # t2189: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2188
  t853 = Tensor.expand(t2189, (1, 512, 4096))  # t853: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t853 = ltorch.expand(t2189, (1, 512, 4096))  # t853: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t853 = prims.broadcast_in_dim(t2189, (1, 512, 4096), (0, 1, 2))  # t853: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2189
  t2191 = torch.unsqueeze(t66, 0)  # t2191: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2191 = ltorch.unsqueeze(t66, 0)  # t2191: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2191 = prims.broadcast_in_dim(t66, [1, 4096], [1])  # t2191: &#34;cuda:0 bf16[1, 4096]&#34;
  t2192 = torch.unsqueeze(t2191, 1)  # t2192: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2192 = ltorch.unsqueeze(t2191, 1)  # t2192: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2192 = prims.broadcast_in_dim(t2191, [1, 1, 4096], [0, 2])  # t2192: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2191
  t889 = Tensor.expand(t2192, (1, 512, 4096))  # t889: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t889 = ltorch.expand(t2192, (1, 512, 4096))  # t889: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t889 = prims.broadcast_in_dim(t2192, (1, 512, 4096), (0, 1, 2))  # t889: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2192
  t2212 = torch.unsqueeze(t76, 0)  # t2212: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2212 = ltorch.unsqueeze(t76, 0)  # t2212: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2212 = prims.broadcast_in_dim(t76, [1, 4096], [1])  # t2212: &#34;cuda:0 bf16[1, 4096]&#34;
  t2213 = torch.unsqueeze(t2212, 1)  # t2213: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2213 = ltorch.unsqueeze(t2212, 1)  # t2213: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2213 = prims.broadcast_in_dim(t2212, [1, 1, 4096], [0, 2])  # t2213: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2212
  t961 = Tensor.expand(t2213, (1, 512, 4096))  # t961: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t961 = ltorch.expand(t2213, (1, 512, 4096))  # t961: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t961 = prims.broadcast_in_dim(t2213, (1, 512, 4096), (0, 1, 2))  # t961: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2213
  t2215 = torch.unsqueeze(t67, 0)  # t2215: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2215 = ltorch.unsqueeze(t67, 0)  # t2215: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2215 = prims.broadcast_in_dim(t67, [1, 4096], [1])  # t2215: &#34;cuda:0 bf16[1, 4096]&#34;
  t2216 = torch.unsqueeze(t2215, 1)  # t2216: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2216 = ltorch.unsqueeze(t2215, 1)  # t2216: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2216 = prims.broadcast_in_dim(t2215, [1, 1, 4096], [0, 2])  # t2216: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2215
  t997 = Tensor.expand(t2216, (1, 512, 4096))  # t997: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t997 = ltorch.expand(t2216, (1, 512, 4096))  # t997: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t997 = prims.broadcast_in_dim(t2216, (1, 512, 4096), (0, 1, 2))  # t997: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2216
  t2236 = torch.unsqueeze(t77, 0)  # t2236: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2236 = ltorch.unsqueeze(t77, 0)  # t2236: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2236 = prims.broadcast_in_dim(t77, [1, 4096], [1])  # t2236: &#34;cuda:0 bf16[1, 4096]&#34;
  t2237 = torch.unsqueeze(t2236, 1)  # t2237: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2237 = ltorch.unsqueeze(t2236, 1)  # t2237: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2237 = prims.broadcast_in_dim(t2236, [1, 1, 4096], [0, 2])  # t2237: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2236
  t1069 = Tensor.expand(t2237, (1, 512, 4096))  # t1069: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1069 = ltorch.expand(t2237, (1, 512, 4096))  # t1069: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1069 = prims.broadcast_in_dim(t2237, (1, 512, 4096), (0, 1, 2))  # t1069: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2237
  t2239 = torch.unsqueeze(t68, 0)  # t2239: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2239 = ltorch.unsqueeze(t68, 0)  # t2239: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2239 = prims.broadcast_in_dim(t68, [1, 4096], [1])  # t2239: &#34;cuda:0 bf16[1, 4096]&#34;
  t2240 = torch.unsqueeze(t2239, 1)  # t2240: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2240 = ltorch.unsqueeze(t2239, 1)  # t2240: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2240 = prims.broadcast_in_dim(t2239, [1, 1, 4096], [0, 2])  # t2240: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2239
  t1105 = Tensor.expand(t2240, (1, 512, 4096))  # t1105: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1105 = ltorch.expand(t2240, (1, 512, 4096))  # t1105: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1105 = prims.broadcast_in_dim(t2240, (1, 512, 4096), (0, 1, 2))  # t1105: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2240
  t2260 = torch.unsqueeze(t78, 0)  # t2260: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2260 = ltorch.unsqueeze(t78, 0)  # t2260: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2260 = prims.broadcast_in_dim(t78, [1, 4096], [1])  # t2260: &#34;cuda:0 bf16[1, 4096]&#34;
  t2261 = torch.unsqueeze(t2260, 1)  # t2261: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2261 = ltorch.unsqueeze(t2260, 1)  # t2261: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2261 = prims.broadcast_in_dim(t2260, [1, 1, 4096], [0, 2])  # t2261: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2260
  t1177 = Tensor.expand(t2261, (1, 512, 4096))  # t1177: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1177 = ltorch.expand(t2261, (1, 512, 4096))  # t1177: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1177 = prims.broadcast_in_dim(t2261, (1, 512, 4096), (0, 1, 2))  # t1177: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2261
  t2263 = torch.unsqueeze(t54, 0)  # t2263: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2263 = ltorch.unsqueeze(t54, 0)  # t2263: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2263 = prims.broadcast_in_dim(t54, [1, 4096], [1])  # t2263: &#34;cuda:0 bf16[1, 4096]&#34;
  t2264 = torch.unsqueeze(t2263, 1)  # t2264: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2264 = ltorch.unsqueeze(t2263, 1)  # t2264: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2264 = prims.broadcast_in_dim(t2263, [1, 1, 4096], [0, 2])  # t2264: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2263
  t1213 = Tensor.expand(t2264, (1, 512, 4096))  # t1213: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1213 = ltorch.expand(t2264, (1, 512, 4096))  # t1213: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1213 = prims.broadcast_in_dim(t2264, (1, 512, 4096), (0, 1, 2))  # t1213: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2264
  t2284 = torch.unsqueeze(t79, 0)  # t2284: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2284 = ltorch.unsqueeze(t79, 0)  # t2284: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2284 = prims.broadcast_in_dim(t79, [1, 4096], [1])  # t2284: &#34;cuda:0 bf16[1, 4096]&#34;
  t2285 = torch.unsqueeze(t2284, 1)  # t2285: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2285 = ltorch.unsqueeze(t2284, 1)  # t2285: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2285 = prims.broadcast_in_dim(t2284, [1, 1, 4096], [0, 2])  # t2285: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2284
  t1285 = Tensor.expand(t2285, (1, 512, 4096))  # t1285: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1285 = ltorch.expand(t2285, (1, 512, 4096))  # t1285: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1285 = prims.broadcast_in_dim(t2285, (1, 512, 4096), (0, 1, 2))  # t1285: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2285
  t2287 = torch.unsqueeze(t55, 0)  # t2287: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2287 = ltorch.unsqueeze(t55, 0)  # t2287: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2287 = prims.broadcast_in_dim(t55, [1, 4096], [1])  # t2287: &#34;cuda:0 bf16[1, 4096]&#34;
  t2288 = torch.unsqueeze(t2287, 1)  # t2288: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2288 = ltorch.unsqueeze(t2287, 1)  # t2288: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2288 = prims.broadcast_in_dim(t2287, [1, 1, 4096], [0, 2])  # t2288: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2287
  t1321 = Tensor.expand(t2288, (1, 512, 4096))  # t1321: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1321 = ltorch.expand(t2288, (1, 512, 4096))  # t1321: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1321 = prims.broadcast_in_dim(t2288, (1, 512, 4096), (0, 1, 2))  # t1321: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2288
  t2308 = torch.unsqueeze(t80, 0)  # t2308: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2308 = ltorch.unsqueeze(t80, 0)  # t2308: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2308 = prims.broadcast_in_dim(t80, [1, 4096], [1])  # t2308: &#34;cuda:0 bf16[1, 4096]&#34;
  t2309 = torch.unsqueeze(t2308, 1)  # t2309: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2309 = ltorch.unsqueeze(t2308, 1)  # t2309: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2309 = prims.broadcast_in_dim(t2308, [1, 1, 4096], [0, 2])  # t2309: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2308
  t1393 = Tensor.expand(t2309, (1, 512, 4096))  # t1393: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1393 = ltorch.expand(t2309, (1, 512, 4096))  # t1393: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1393 = prims.broadcast_in_dim(t2309, (1, 512, 4096), (0, 1, 2))  # t1393: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2309
  t2311 = torch.unsqueeze(t56, 0)  # t2311: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2311 = ltorch.unsqueeze(t56, 0)  # t2311: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2311 = prims.broadcast_in_dim(t56, [1, 4096], [1])  # t2311: &#34;cuda:0 bf16[1, 4096]&#34;
  t2312 = torch.unsqueeze(t2311, 1)  # t2312: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2312 = ltorch.unsqueeze(t2311, 1)  # t2312: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2312 = prims.broadcast_in_dim(t2311, [1, 1, 4096], [0, 2])  # t2312: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2311
  t1429 = Tensor.expand(t2312, (1, 512, 4096))  # t1429: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1429 = ltorch.expand(t2312, (1, 512, 4096))  # t1429: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1429 = prims.broadcast_in_dim(t2312, (1, 512, 4096), (0, 1, 2))  # t1429: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2312
  t2332 = torch.unsqueeze(t81, 0)  # t2332: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2332 = ltorch.unsqueeze(t81, 0)  # t2332: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2332 = prims.broadcast_in_dim(t81, [1, 4096], [1])  # t2332: &#34;cuda:0 bf16[1, 4096]&#34;
  t2333 = torch.unsqueeze(t2332, 1)  # t2333: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2333 = ltorch.unsqueeze(t2332, 1)  # t2333: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2333 = prims.broadcast_in_dim(t2332, [1, 1, 4096], [0, 2])  # t2333: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2332
  t1501 = Tensor.expand(t2333, (1, 512, 4096))  # t1501: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1501 = ltorch.expand(t2333, (1, 512, 4096))  # t1501: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1501 = prims.broadcast_in_dim(t2333, (1, 512, 4096), (0, 1, 2))  # t1501: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2333
  t2335 = torch.unsqueeze(t57, 0)  # t2335: &#34;cuda:0 bf16[1, 4096]&#34;
    # t2335 = ltorch.unsqueeze(t57, 0)  # t2335: &#34;cuda:0 bf16[1, 4096]&#34;
      # t2335 = prims.broadcast_in_dim(t57, [1, 4096], [1])  # t2335: &#34;cuda:0 bf16[1, 4096]&#34;
  t2336 = torch.unsqueeze(t2335, 1)  # t2336: &#34;cuda:0 bf16[1, 1, 4096]&#34;
    # t2336 = ltorch.unsqueeze(t2335, 1)  # t2336: &#34;cuda:0 bf16[1, 1, 4096]&#34;
      # t2336 = prims.broadcast_in_dim(t2335, [1, 1, 4096], [0, 2])  # t2336: &#34;cuda:0 bf16[1, 1, 4096]&#34;
  del t2335
  t1537 = Tensor.expand(t2336, (1, 512, 4096))  # t1537: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1537 = ltorch.expand(t2336, (1, 512, 4096))  # t1537: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1537 = prims.broadcast_in_dim(t2336, (1, 512, 4096), (0, 1, 2))  # t1537: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t2336
  t2036 = torch.unsqueeze(t118, 0)  # t2036: &#34;cuda:0 f32[1, 512, 128]&#34;
    # t2036 = ltorch.unsqueeze(t118, 0)  # t2036: &#34;cuda:0 f32[1, 512, 128]&#34;
      # t2036 = prims.broadcast_in_dim(t118, [1, 512, 128], [1, 2])  # t2036: &#34;cuda:0 f32[1, 512, 128]&#34;
  del t118
  t2037 = torch.unsqueeze(t2036, 1)  # t2037: &#34;cuda:0 f32[1, 1, 512, 128]&#34;
    # t2037 = ltorch.unsqueeze(t2036, 1)  # t2037: &#34;cuda:0 f32[1, 1, 512, 128]&#34;
      # t2037 = prims.broadcast_in_dim(t2036, [1, 1, 512, 128], [0, 2, 3])  # t2037: &#34;cuda:0 f32[1, 1, 512, 128]&#34;
  del t2036
  t154 = Tensor.expand(t2037, (1, 32, 512, 128))  # t154: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t154 = ltorch.expand(t2037, (1, 32, 512, 128))  # t154: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
      # t154 = prims.broadcast_in_dim(t2037, (1, 32, 512, 128), (0, 1, 2, 3))  # t154: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
  del t2037
  t2039 = torch.unsqueeze(t119, 0)  # t2039: &#34;cuda:0 f32[1, 512, 128]&#34;
    # t2039 = ltorch.unsqueeze(t119, 0)  # t2039: &#34;cuda:0 f32[1, 512, 128]&#34;
      # t2039 = prims.broadcast_in_dim(t119, [1, 512, 128], [1, 2])  # t2039: &#34;cuda:0 f32[1, 512, 128]&#34;
  del t119
  t2040 = torch.unsqueeze(t2039, 1)  # t2040: &#34;cuda:0 f32[1, 1, 512, 128]&#34;
    # t2040 = ltorch.unsqueeze(t2039, 1)  # t2040: &#34;cuda:0 f32[1, 1, 512, 128]&#34;
      # t2040 = prims.broadcast_in_dim(t2039, [1, 1, 512, 128], [0, 2, 3])  # t2040: &#34;cuda:0 f32[1, 1, 512, 128]&#34;
  del t2039
  t157 = Tensor.expand(t2040, (1, 32, 512, 128))  # t157: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t157 = ltorch.expand(t2040, (1, 32, 512, 128))  # t157: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
      # t157 = prims.broadcast_in_dim(t2040, (1, 32, 512, 128), (0, 1, 2, 3))  # t157: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
  del t2040
  [t129, t137] = nvFusion0(t122, t133)
    # t123 = prims.convert_element_type(t122, dtypes.float32)  # t123: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t124 = prims.mul(t123, t123)  # t124: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t125 = prims.sum(t124, (2,))  # t125: &#34;cuda:0 f32[1, 512]&#34;
    # t126 = prims.broadcast_in_dim(t125, [1, 512, 1], [0, 1])  # t126: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t127 = prims.div(t126, 4096.0)  # t127: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t128 = prims.add(t127, 1e-05)  # t128: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t129 = prims.rsqrt(t128)  # t129: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t130 = prims.broadcast_in_dim(t129, (1, 512, 4096), (0, 1, 2))  # t130: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t131 = prims.mul(t123, t130)  # t131: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t135 = prims.convert_element_type(t133, dtypes.float32)  # t135: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t136 = prims.mul(t131, t135)  # t136: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t137 = prims.convert_element_type(t136, dtypes.bfloat16)  # t137: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t138 = torch.nn.functional.linear(t137, t3, None)  # t138: &#34;cuda:0 bf16[1, 512, 12288]&#34;
    # t138 = ltorch.linear(t137, t3, None)  # t138: &#34;cuda:0 bf16[1, 512, 12288]&#34;
      # t138 = prims.linear(t137, t3, None)  # t138: &#34;cuda:0 bf16[1, 512, 12288]&#34;
  t139 = torch.reshape(t138, (1, 512, 32, 3, 128))  # t139: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
    # t139 = ltorch.reshape(t138, (1, 512, 32, 3, 128))  # t139: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
      # t139 = prims.reshape(t138, (1, 512, 32, 3, 128))  # t139: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
  del t138
  t140 = torch.permute(t139, (0, 2, 3, 1, 4))  # t140: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
    # t140 = ltorch.permute(t139, (0, 2, 3, 1, 4))  # t140: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
      # t140 = prims.transpose(t139, (0, 2, 3, 1, 4))  # t140: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
  del t139
  (t141, t142, t143) = torch.split(t140, (1, 1, 1), 2)
    # (t141, t142, t143) = ltorch.split(t140, (1, 1, 1), 2)
      # t141 = prims.slice_prim(t140, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t141: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t142 = prims.slice_prim(t140, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t142: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t143 = prims.slice_prim(t140, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t143: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
  del t140
  t144 = torch.reshape(t141, (1, 32, 512, 128))  # t144: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t144 = ltorch.reshape(t141, (1, 32, 512, 128))  # t144: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t144 = prims.reshape(t141, (1, 32, 512, 128))  # t144: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t141
  t145 = torch.reshape(t142, (1, 32, 512, 128))  # t145: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t145 = ltorch.reshape(t142, (1, 32, 512, 128))  # t145: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t145 = prims.reshape(t142, (1, 32, 512, 128))  # t145: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t142
  t146 = torch.reshape(t143, (1, 32, 512, 128))  # t146: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t146 = ltorch.reshape(t143, (1, 32, 512, 128))  # t146: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t146 = prims.reshape(t143, (1, 32, 512, 128))  # t146: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t143
  t147 = torch_slice_prim_impl(t144, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t147: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t162 = torch_slice_prim_impl(t145, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t162: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t177 = torch_slice_prim_impl(t144, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t177: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t144
  t179 = torch_slice_prim_impl(t145, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t179: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t145
  t149 = torch_slice_prim_impl(t147, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t149: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t148 = torch_slice_prim_impl(t147, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t148: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t163 = torch_slice_prim_impl(t162, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t163: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t164 = torch_slice_prim_impl(t162, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t164: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  [t152, t167] = nvFusion1(t147, t149, t162, t164)
    # t150 = prims.convert_element_type(t149, dtypes.float32)  # t150: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t151 = prims.neg(t150)  # t151: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t152 = prims.convert_element_type(t151, dtypes.bfloat16)  # t152: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
    # t165 = prims.convert_element_type(t164, dtypes.float32)  # t165: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t166 = prims.neg(t165)  # t166: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t167 = prims.convert_element_type(t166, dtypes.bfloat16)  # t167: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  del t149, t164
  t168 = torch.cat((t167, t163), -1)  # t168: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t168 = ltorch.cat((t167, t163), -1)  # t168: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t168 = prims.cat((t167, t163), -1)  # t168: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t167, t163
  t153 = torch.cat((t152, t148), -1)  # t153: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t153 = ltorch.cat((t152, t148), -1)  # t153: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t153 = prims.cat((t152, t148), -1)  # t153: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t152, t148
  [t161, t176] = nvFusion2(t147, t153, t154, t157, t162, t168)
    # t155 = prims.convert_element_type(t147, dtypes.float32)  # t155: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t170 = prims.convert_element_type(t162, dtypes.float32)  # t170: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t156 = prims.mul(t155, t154)  # t156: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t158 = prims.convert_element_type(t153, dtypes.float32)  # t158: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t159 = prims.mul(t158, t157)  # t159: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t160 = prims.add(t156, t159)  # t160: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t161 = prims.convert_element_type(t160, dtypes.bfloat16)  # t161: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t171 = prims.mul(t170, t154)  # t171: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t173 = prims.convert_element_type(t168, dtypes.float32)  # t173: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t174 = prims.mul(t173, t157)  # t174: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t175 = prims.add(t171, t174)  # t175: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t176 = prims.convert_element_type(t175, dtypes.bfloat16)  # t176: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t147, t153, t162, t168
  t178 = torch.cat((t161, t177), -1)  # t178: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t178 = ltorch.cat((t161, t177), -1)  # t178: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t178 = prims.cat((t161, t177), -1)  # t178: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t161, t177
  t180 = torch.cat((t176, t179), -1)  # t180: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t180 = ltorch.cat((t176, t179), -1)  # t180: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t180 = prims.cat((t176, t179), -1)  # t180: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t176, t179
  (t181, t182, t183, t184, _, _, t185, t186, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(t178, t180, t146, 0.0, True, scale=0.08838834764831843)
  t188 = torch.permute(t181, (0, 2, 1, 3))  # t188: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
    # t188 = ltorch.permute(t181, (0, 2, 1, 3))  # t188: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
      # t188 = prims.transpose(t181, (0, 2, 1, 3))  # t188: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
  t189 = torch.reshape(t188, (1, 512, 4096))  # t189: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t189 = ltorch.reshape(t188, (1, 512, 4096))  # t189: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t189 = prims.reshape(t188, (1, 512, 4096))  # t189: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t188
  t190 = torch.nn.functional.linear(t189, t85, None)  # t190: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t190 = ltorch.linear(t189, t85, None)  # t190: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t190 = prims.linear(t189, t85, None)  # t190: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t194, t201, t209] = nvFusion3(t122, t190, t205)
    # t191 = prims.convert_element_type(t190, dtypes.float32)  # t191: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t192 = prims.convert_element_type(t122, dtypes.float32)  # t192: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t193 = prims.add(t191, t192)  # t193: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t194 = prims.convert_element_type(t193, dtypes.bfloat16)  # t194: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t196 = prims.mul(t193, t193)  # t196: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t197 = prims.sum(t196, (2,))  # t197: &#34;cuda:0 f32[1, 512]&#34;
    # t198 = prims.broadcast_in_dim(t197, [1, 512, 1], [0, 1])  # t198: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t199 = prims.div(t198, 4096.0)  # t199: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t200 = prims.add(t199, 1e-05)  # t200: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t201 = prims.rsqrt(t200)  # t201: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t202 = prims.broadcast_in_dim(t201, (1, 512, 4096), (0, 1, 2))  # t202: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t203 = prims.mul(t193, t202)  # t203: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t207 = prims.convert_element_type(t205, dtypes.float32)  # t207: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t208 = prims.mul(t203, t207)  # t208: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t209 = prims.convert_element_type(t208, dtypes.bfloat16)  # t209: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t210 = torch.nn.functional.linear(t209, t19, None)  # t210: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t210 = ltorch.linear(t209, t19, None)  # t210: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t210 = prims.linear(t209, t19, None)  # t210: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t211 = torch.nn.functional.linear(t209, t35, None)  # t211: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t211 = ltorch.linear(t209, t35, None)  # t211: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t211 = prims.linear(t209, t35, None)  # t211: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  [t225] = nvFusion4(t210, t211)
    # t212 = prims.convert_element_type(t210, dtypes.float32)  # t212: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t213 = prims.neg(t212)  # t213: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t214 = prims.exp(t213)  # t214: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t215 = prims.add(1.0, t214)  # t215: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t216 = prims.reciprocal(t215)  # t216: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t220 = prims.mul(t212, t216)  # t220: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t223 = prims.convert_element_type(t211, dtypes.float32)  # t223: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t224 = prims.mul(t220, t223)  # t224: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t225 = prims.convert_element_type(t224, dtypes.bfloat16)  # t225: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t226 = torch.nn.functional.linear(t225, t86, None)  # t226: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t226 = ltorch.linear(t225, t86, None)  # t226: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t226 = prims.linear(t225, t86, None)  # t226: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t230, t237, t245] = nvFusion5(t194, t226, t241)
    # t228 = prims.convert_element_type(t194, dtypes.float32)  # t228: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t227 = prims.convert_element_type(t226, dtypes.float32)  # t227: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t229 = prims.add(t227, t228)  # t229: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t230 = prims.convert_element_type(t229, dtypes.bfloat16)  # t230: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t232 = prims.mul(t229, t229)  # t232: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t233 = prims.sum(t232, (2,))  # t233: &#34;cuda:0 f32[1, 512]&#34;
    # t234 = prims.broadcast_in_dim(t233, [1, 512, 1], [0, 1])  # t234: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t235 = prims.div(t234, 4096.0)  # t235: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t236 = prims.add(t235, 1e-05)  # t236: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t237 = prims.rsqrt(t236)  # t237: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t238 = prims.broadcast_in_dim(t237, (1, 512, 4096), (0, 1, 2))  # t238: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t239 = prims.mul(t229, t238)  # t239: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t243 = prims.convert_element_type(t241, dtypes.float32)  # t243: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t244 = prims.mul(t239, t243)  # t244: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t245 = prims.convert_element_type(t244, dtypes.bfloat16)  # t245: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t246 = torch.nn.functional.linear(t245, t4, None)  # t246: &#34;cuda:0 bf16[1, 512, 12288]&#34;
    # t246 = ltorch.linear(t245, t4, None)  # t246: &#34;cuda:0 bf16[1, 512, 12288]&#34;
      # t246 = prims.linear(t245, t4, None)  # t246: &#34;cuda:0 bf16[1, 512, 12288]&#34;
  t247 = torch.reshape(t246, (1, 512, 32, 3, 128))  # t247: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
    # t247 = ltorch.reshape(t246, (1, 512, 32, 3, 128))  # t247: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
      # t247 = prims.reshape(t246, (1, 512, 32, 3, 128))  # t247: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
  del t246
  t248 = torch.permute(t247, (0, 2, 3, 1, 4))  # t248: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
    # t248 = ltorch.permute(t247, (0, 2, 3, 1, 4))  # t248: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
      # t248 = prims.transpose(t247, (0, 2, 3, 1, 4))  # t248: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
  del t247
  (t249, t250, t251) = torch.split(t248, (1, 1, 1), 2)
    # (t249, t250, t251) = ltorch.split(t248, (1, 1, 1), 2)
      # t249 = prims.slice_prim(t248, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t249: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t250 = prims.slice_prim(t248, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t250: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t251 = prims.slice_prim(t248, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t251: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
  del t248
  t252 = torch.reshape(t249, (1, 32, 512, 128))  # t252: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t252 = ltorch.reshape(t249, (1, 32, 512, 128))  # t252: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t252 = prims.reshape(t249, (1, 32, 512, 128))  # t252: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t249
  t253 = torch.reshape(t250, (1, 32, 512, 128))  # t253: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t253 = ltorch.reshape(t250, (1, 32, 512, 128))  # t253: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t253 = prims.reshape(t250, (1, 32, 512, 128))  # t253: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t250
  t254 = torch.reshape(t251, (1, 32, 512, 128))  # t254: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t254 = ltorch.reshape(t251, (1, 32, 512, 128))  # t254: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t254 = prims.reshape(t251, (1, 32, 512, 128))  # t254: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t251
  t285 = torch_slice_prim_impl(t252, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t285: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  t287 = torch_slice_prim_impl(t253, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t287: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  t255 = torch_slice_prim_impl(t252, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t255: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t252
  t270 = torch_slice_prim_impl(t253, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t270: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t253
  t256 = torch_slice_prim_impl(t255, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t256: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t257 = torch_slice_prim_impl(t255, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t257: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t272 = torch_slice_prim_impl(t270, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t272: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t271 = torch_slice_prim_impl(t270, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t271: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  [t260, t275] = nvFusion6(t255, t257, t270, t272)
    # t258 = prims.convert_element_type(t257, dtypes.float32)  # t258: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t259 = prims.neg(t258)  # t259: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t260 = prims.convert_element_type(t259, dtypes.bfloat16)  # t260: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
    # t273 = prims.convert_element_type(t272, dtypes.float32)  # t273: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t274 = prims.neg(t273)  # t274: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t275 = prims.convert_element_type(t274, dtypes.bfloat16)  # t275: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  del t257, t272
  t261 = torch.cat((t260, t256), -1)  # t261: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t261 = ltorch.cat((t260, t256), -1)  # t261: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t261 = prims.cat((t260, t256), -1)  # t261: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t260, t256
  t276 = torch.cat((t275, t271), -1)  # t276: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t276 = ltorch.cat((t275, t271), -1)  # t276: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t276 = prims.cat((t275, t271), -1)  # t276: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t275, t271
  [t269, t284] = nvFusion7(t154, t157, t255, t261, t270, t276)
    # t263 = prims.convert_element_type(t255, dtypes.float32)  # t263: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t278 = prims.convert_element_type(t270, dtypes.float32)  # t278: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t264 = prims.mul(t263, t154)  # t264: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t266 = prims.convert_element_type(t261, dtypes.float32)  # t266: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t267 = prims.mul(t266, t157)  # t267: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t268 = prims.add(t264, t267)  # t268: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t269 = prims.convert_element_type(t268, dtypes.bfloat16)  # t269: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t279 = prims.mul(t278, t154)  # t279: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t281 = prims.convert_element_type(t276, dtypes.float32)  # t281: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t282 = prims.mul(t281, t157)  # t282: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t283 = prims.add(t279, t282)  # t283: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t284 = prims.convert_element_type(t283, dtypes.bfloat16)  # t284: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t255, t261, t270, t276
  t288 = torch.cat((t284, t287), -1)  # t288: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t288 = ltorch.cat((t284, t287), -1)  # t288: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t288 = prims.cat((t284, t287), -1)  # t288: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t284, t287
  t286 = torch.cat((t269, t285), -1)  # t286: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t286 = ltorch.cat((t269, t285), -1)  # t286: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t286 = prims.cat((t269, t285), -1)  # t286: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t269, t285
  (t289, t290, t291, t292, _, _, t293, t294, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(t286, t288, t254, 0.0, True, scale=0.08838834764831843)
  t296 = torch.permute(t289, (0, 2, 1, 3))  # t296: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
    # t296 = ltorch.permute(t289, (0, 2, 1, 3))  # t296: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
      # t296 = prims.transpose(t289, (0, 2, 1, 3))  # t296: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
  t297 = torch.reshape(t296, (1, 512, 4096))  # t297: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t297 = ltorch.reshape(t296, (1, 512, 4096))  # t297: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t297 = prims.reshape(t296, (1, 512, 4096))  # t297: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t296
  t298 = torch.nn.functional.linear(t297, t87, None)  # t298: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t298 = ltorch.linear(t297, t87, None)  # t298: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t298 = prims.linear(t297, t87, None)  # t298: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t302, t309, t317] = nvFusion8(t230, t298, t313)
    # t300 = prims.convert_element_type(t230, dtypes.float32)  # t300: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t299 = prims.convert_element_type(t298, dtypes.float32)  # t299: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t301 = prims.add(t299, t300)  # t301: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t302 = prims.convert_element_type(t301, dtypes.bfloat16)  # t302: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t304 = prims.mul(t301, t301)  # t304: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t305 = prims.sum(t304, (2,))  # t305: &#34;cuda:0 f32[1, 512]&#34;
    # t306 = prims.broadcast_in_dim(t305, [1, 512, 1], [0, 1])  # t306: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t307 = prims.div(t306, 4096.0)  # t307: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t308 = prims.add(t307, 1e-05)  # t308: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t309 = prims.rsqrt(t308)  # t309: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t310 = prims.broadcast_in_dim(t309, (1, 512, 4096), (0, 1, 2))  # t310: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t311 = prims.mul(t301, t310)  # t311: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t315 = prims.convert_element_type(t313, dtypes.float32)  # t315: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t316 = prims.mul(t311, t315)  # t316: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t317 = prims.convert_element_type(t316, dtypes.bfloat16)  # t317: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t318 = torch.nn.functional.linear(t317, t20, None)  # t318: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t318 = ltorch.linear(t317, t20, None)  # t318: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t318 = prims.linear(t317, t20, None)  # t318: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t319 = torch.nn.functional.linear(t317, t36, None)  # t319: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t319 = ltorch.linear(t317, t36, None)  # t319: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t319 = prims.linear(t317, t36, None)  # t319: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  [t333] = nvFusion9(t318, t319)
    # t320 = prims.convert_element_type(t318, dtypes.float32)  # t320: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t321 = prims.neg(t320)  # t321: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t322 = prims.exp(t321)  # t322: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t323 = prims.add(1.0, t322)  # t323: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t324 = prims.reciprocal(t323)  # t324: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t328 = prims.mul(t320, t324)  # t328: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t331 = prims.convert_element_type(t319, dtypes.float32)  # t331: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t332 = prims.mul(t328, t331)  # t332: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t333 = prims.convert_element_type(t332, dtypes.bfloat16)  # t333: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t334 = torch.nn.functional.linear(t333, t88, None)  # t334: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t334 = ltorch.linear(t333, t88, None)  # t334: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t334 = prims.linear(t333, t88, None)  # t334: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t338, t345, t353] = nvFusion10(t302, t334, t349)
    # t336 = prims.convert_element_type(t302, dtypes.float32)  # t336: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t335 = prims.convert_element_type(t334, dtypes.float32)  # t335: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t337 = prims.add(t335, t336)  # t337: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t338 = prims.convert_element_type(t337, dtypes.bfloat16)  # t338: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t340 = prims.mul(t337, t337)  # t340: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t341 = prims.sum(t340, (2,))  # t341: &#34;cuda:0 f32[1, 512]&#34;
    # t342 = prims.broadcast_in_dim(t341, [1, 512, 1], [0, 1])  # t342: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t343 = prims.div(t342, 4096.0)  # t343: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t344 = prims.add(t343, 1e-05)  # t344: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t345 = prims.rsqrt(t344)  # t345: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t346 = prims.broadcast_in_dim(t345, (1, 512, 4096), (0, 1, 2))  # t346: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t347 = prims.mul(t337, t346)  # t347: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t351 = prims.convert_element_type(t349, dtypes.float32)  # t351: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t352 = prims.mul(t347, t351)  # t352: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t353 = prims.convert_element_type(t352, dtypes.bfloat16)  # t353: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t354 = torch.nn.functional.linear(t353, t5, None)  # t354: &#34;cuda:0 bf16[1, 512, 12288]&#34;
    # t354 = ltorch.linear(t353, t5, None)  # t354: &#34;cuda:0 bf16[1, 512, 12288]&#34;
      # t354 = prims.linear(t353, t5, None)  # t354: &#34;cuda:0 bf16[1, 512, 12288]&#34;
  t355 = torch.reshape(t354, (1, 512, 32, 3, 128))  # t355: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
    # t355 = ltorch.reshape(t354, (1, 512, 32, 3, 128))  # t355: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
      # t355 = prims.reshape(t354, (1, 512, 32, 3, 128))  # t355: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
  del t354
  t356 = torch.permute(t355, (0, 2, 3, 1, 4))  # t356: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
    # t356 = ltorch.permute(t355, (0, 2, 3, 1, 4))  # t356: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
      # t356 = prims.transpose(t355, (0, 2, 3, 1, 4))  # t356: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
  del t355
  (t357, t358, t359) = torch.split(t356, (1, 1, 1), 2)
    # (t357, t358, t359) = ltorch.split(t356, (1, 1, 1), 2)
      # t357 = prims.slice_prim(t356, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t357: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t358 = prims.slice_prim(t356, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t358: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t359 = prims.slice_prim(t356, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t359: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
  del t356
  t360 = torch.reshape(t357, (1, 32, 512, 128))  # t360: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t360 = ltorch.reshape(t357, (1, 32, 512, 128))  # t360: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t360 = prims.reshape(t357, (1, 32, 512, 128))  # t360: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t357
  t361 = torch.reshape(t358, (1, 32, 512, 128))  # t361: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t361 = ltorch.reshape(t358, (1, 32, 512, 128))  # t361: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t361 = prims.reshape(t358, (1, 32, 512, 128))  # t361: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t358
  t362 = torch.reshape(t359, (1, 32, 512, 128))  # t362: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t362 = ltorch.reshape(t359, (1, 32, 512, 128))  # t362: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t362 = prims.reshape(t359, (1, 32, 512, 128))  # t362: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t359
  t363 = torch_slice_prim_impl(t360, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t363: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t378 = torch_slice_prim_impl(t361, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t378: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t393 = torch_slice_prim_impl(t360, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t393: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t360
  t395 = torch_slice_prim_impl(t361, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t395: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t361
  t364 = torch_slice_prim_impl(t363, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t364: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t365 = torch_slice_prim_impl(t363, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t365: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t379 = torch_slice_prim_impl(t378, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t379: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t380 = torch_slice_prim_impl(t378, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t380: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  [t368, t383] = nvFusion11(t363, t365, t378, t380)
    # t366 = prims.convert_element_type(t365, dtypes.float32)  # t366: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t367 = prims.neg(t366)  # t367: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t368 = prims.convert_element_type(t367, dtypes.bfloat16)  # t368: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
    # t381 = prims.convert_element_type(t380, dtypes.float32)  # t381: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t382 = prims.neg(t381)  # t382: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t383 = prims.convert_element_type(t382, dtypes.bfloat16)  # t383: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  del t365, t380
  t369 = torch.cat((t368, t364), -1)  # t369: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t369 = ltorch.cat((t368, t364), -1)  # t369: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t369 = prims.cat((t368, t364), -1)  # t369: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t368, t364
  t384 = torch.cat((t383, t379), -1)  # t384: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t384 = ltorch.cat((t383, t379), -1)  # t384: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t384 = prims.cat((t383, t379), -1)  # t384: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t383, t379
  [t377, t392] = nvFusion12(t154, t157, t363, t369, t378, t384)
    # t371 = prims.convert_element_type(t363, dtypes.float32)  # t371: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t386 = prims.convert_element_type(t378, dtypes.float32)  # t386: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t372 = prims.mul(t371, t154)  # t372: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t374 = prims.convert_element_type(t369, dtypes.float32)  # t374: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t375 = prims.mul(t374, t157)  # t375: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t376 = prims.add(t372, t375)  # t376: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t377 = prims.convert_element_type(t376, dtypes.bfloat16)  # t377: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t387 = prims.mul(t386, t154)  # t387: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t389 = prims.convert_element_type(t384, dtypes.float32)  # t389: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t390 = prims.mul(t389, t157)  # t390: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t391 = prims.add(t387, t390)  # t391: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t392 = prims.convert_element_type(t391, dtypes.bfloat16)  # t392: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t363, t369, t378, t384
  t394 = torch.cat((t377, t393), -1)  # t394: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t394 = ltorch.cat((t377, t393), -1)  # t394: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t394 = prims.cat((t377, t393), -1)  # t394: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t377, t393
  t396 = torch.cat((t392, t395), -1)  # t396: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t396 = ltorch.cat((t392, t395), -1)  # t396: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t396 = prims.cat((t392, t395), -1)  # t396: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t392, t395
  (t397, t398, t399, t400, _, _, t401, t402, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(t394, t396, t362, 0.0, True, scale=0.08838834764831843)
  t404 = torch.permute(t397, (0, 2, 1, 3))  # t404: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
    # t404 = ltorch.permute(t397, (0, 2, 1, 3))  # t404: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
      # t404 = prims.transpose(t397, (0, 2, 1, 3))  # t404: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
  t405 = torch.reshape(t404, (1, 512, 4096))  # t405: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t405 = ltorch.reshape(t404, (1, 512, 4096))  # t405: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t405 = prims.reshape(t404, (1, 512, 4096))  # t405: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t404
  t406 = torch.nn.functional.linear(t405, t89, None)  # t406: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t406 = ltorch.linear(t405, t89, None)  # t406: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t406 = prims.linear(t405, t89, None)  # t406: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t410, t417, t425] = nvFusion13(t338, t406, t421)
    # t408 = prims.convert_element_type(t338, dtypes.float32)  # t408: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t407 = prims.convert_element_type(t406, dtypes.float32)  # t407: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t409 = prims.add(t407, t408)  # t409: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t410 = prims.convert_element_type(t409, dtypes.bfloat16)  # t410: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t412 = prims.mul(t409, t409)  # t412: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t413 = prims.sum(t412, (2,))  # t413: &#34;cuda:0 f32[1, 512]&#34;
    # t414 = prims.broadcast_in_dim(t413, [1, 512, 1], [0, 1])  # t414: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t415 = prims.div(t414, 4096.0)  # t415: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t416 = prims.add(t415, 1e-05)  # t416: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t417 = prims.rsqrt(t416)  # t417: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t418 = prims.broadcast_in_dim(t417, (1, 512, 4096), (0, 1, 2))  # t418: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t419 = prims.mul(t409, t418)  # t419: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t423 = prims.convert_element_type(t421, dtypes.float32)  # t423: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t424 = prims.mul(t419, t423)  # t424: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t425 = prims.convert_element_type(t424, dtypes.bfloat16)  # t425: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t426 = torch.nn.functional.linear(t425, t21, None)  # t426: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t426 = ltorch.linear(t425, t21, None)  # t426: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t426 = prims.linear(t425, t21, None)  # t426: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t427 = torch.nn.functional.linear(t425, t37, None)  # t427: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t427 = ltorch.linear(t425, t37, None)  # t427: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t427 = prims.linear(t425, t37, None)  # t427: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  [t441] = nvFusion14(t426, t427)
    # t428 = prims.convert_element_type(t426, dtypes.float32)  # t428: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t429 = prims.neg(t428)  # t429: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t430 = prims.exp(t429)  # t430: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t431 = prims.add(1.0, t430)  # t431: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t432 = prims.reciprocal(t431)  # t432: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t436 = prims.mul(t428, t432)  # t436: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t439 = prims.convert_element_type(t427, dtypes.float32)  # t439: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t440 = prims.mul(t436, t439)  # t440: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t441 = prims.convert_element_type(t440, dtypes.bfloat16)  # t441: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t442 = torch.nn.functional.linear(t441, t90, None)  # t442: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t442 = ltorch.linear(t441, t90, None)  # t442: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t442 = prims.linear(t441, t90, None)  # t442: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t446, t453, t461] = nvFusion15(t410, t442, t457)
    # t444 = prims.convert_element_type(t410, dtypes.float32)  # t444: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t443 = prims.convert_element_type(t442, dtypes.float32)  # t443: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t445 = prims.add(t443, t444)  # t445: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t446 = prims.convert_element_type(t445, dtypes.bfloat16)  # t446: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t448 = prims.mul(t445, t445)  # t448: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t449 = prims.sum(t448, (2,))  # t449: &#34;cuda:0 f32[1, 512]&#34;
    # t450 = prims.broadcast_in_dim(t449, [1, 512, 1], [0, 1])  # t450: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t451 = prims.div(t450, 4096.0)  # t451: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t452 = prims.add(t451, 1e-05)  # t452: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t453 = prims.rsqrt(t452)  # t453: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t454 = prims.broadcast_in_dim(t453, (1, 512, 4096), (0, 1, 2))  # t454: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t455 = prims.mul(t445, t454)  # t455: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t459 = prims.convert_element_type(t457, dtypes.float32)  # t459: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t460 = prims.mul(t455, t459)  # t460: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t461 = prims.convert_element_type(t460, dtypes.bfloat16)  # t461: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t462 = torch.nn.functional.linear(t461, t6, None)  # t462: &#34;cuda:0 bf16[1, 512, 12288]&#34;
    # t462 = ltorch.linear(t461, t6, None)  # t462: &#34;cuda:0 bf16[1, 512, 12288]&#34;
      # t462 = prims.linear(t461, t6, None)  # t462: &#34;cuda:0 bf16[1, 512, 12288]&#34;
  t463 = torch.reshape(t462, (1, 512, 32, 3, 128))  # t463: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
    # t463 = ltorch.reshape(t462, (1, 512, 32, 3, 128))  # t463: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
      # t463 = prims.reshape(t462, (1, 512, 32, 3, 128))  # t463: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
  del t462
  t464 = torch.permute(t463, (0, 2, 3, 1, 4))  # t464: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
    # t464 = ltorch.permute(t463, (0, 2, 3, 1, 4))  # t464: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
      # t464 = prims.transpose(t463, (0, 2, 3, 1, 4))  # t464: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
  del t463
  (t465, t466, t467) = torch.split(t464, (1, 1, 1), 2)
    # (t465, t466, t467) = ltorch.split(t464, (1, 1, 1), 2)
      # t465 = prims.slice_prim(t464, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t465: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t466 = prims.slice_prim(t464, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t466: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t467 = prims.slice_prim(t464, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t467: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
  del t464
  t468 = torch.reshape(t465, (1, 32, 512, 128))  # t468: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t468 = ltorch.reshape(t465, (1, 32, 512, 128))  # t468: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t468 = prims.reshape(t465, (1, 32, 512, 128))  # t468: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t465
  t469 = torch.reshape(t466, (1, 32, 512, 128))  # t469: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t469 = ltorch.reshape(t466, (1, 32, 512, 128))  # t469: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t469 = prims.reshape(t466, (1, 32, 512, 128))  # t469: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t466
  t470 = torch.reshape(t467, (1, 32, 512, 128))  # t470: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t470 = ltorch.reshape(t467, (1, 32, 512, 128))  # t470: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t470 = prims.reshape(t467, (1, 32, 512, 128))  # t470: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t467
  t471 = torch_slice_prim_impl(t468, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t471: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t486 = torch_slice_prim_impl(t469, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t486: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t501 = torch_slice_prim_impl(t468, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t501: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t468
  t503 = torch_slice_prim_impl(t469, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t503: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t469
  t472 = torch_slice_prim_impl(t471, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t472: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t473 = torch_slice_prim_impl(t471, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t473: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t487 = torch_slice_prim_impl(t486, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t487: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t488 = torch_slice_prim_impl(t486, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t488: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  [t476, t491] = nvFusion16(t471, t473, t486, t488)
    # t474 = prims.convert_element_type(t473, dtypes.float32)  # t474: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t475 = prims.neg(t474)  # t475: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t476 = prims.convert_element_type(t475, dtypes.bfloat16)  # t476: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
    # t489 = prims.convert_element_type(t488, dtypes.float32)  # t489: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t490 = prims.neg(t489)  # t490: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t491 = prims.convert_element_type(t490, dtypes.bfloat16)  # t491: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  del t473, t488
  t477 = torch.cat((t476, t472), -1)  # t477: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t477 = ltorch.cat((t476, t472), -1)  # t477: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t477 = prims.cat((t476, t472), -1)  # t477: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t476, t472
  t492 = torch.cat((t491, t487), -1)  # t492: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t492 = ltorch.cat((t491, t487), -1)  # t492: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t492 = prims.cat((t491, t487), -1)  # t492: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t491, t487
  [t485, t500] = nvFusion17(t154, t157, t471, t477, t486, t492)
    # t479 = prims.convert_element_type(t471, dtypes.float32)  # t479: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t494 = prims.convert_element_type(t486, dtypes.float32)  # t494: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t480 = prims.mul(t479, t154)  # t480: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t482 = prims.convert_element_type(t477, dtypes.float32)  # t482: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t483 = prims.mul(t482, t157)  # t483: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t484 = prims.add(t480, t483)  # t484: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t485 = prims.convert_element_type(t484, dtypes.bfloat16)  # t485: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t495 = prims.mul(t494, t154)  # t495: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t497 = prims.convert_element_type(t492, dtypes.float32)  # t497: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t498 = prims.mul(t497, t157)  # t498: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t499 = prims.add(t495, t498)  # t499: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t500 = prims.convert_element_type(t499, dtypes.bfloat16)  # t500: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t471, t477, t486, t492
  t502 = torch.cat((t485, t501), -1)  # t502: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t502 = ltorch.cat((t485, t501), -1)  # t502: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t502 = prims.cat((t485, t501), -1)  # t502: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t485, t501
  t504 = torch.cat((t500, t503), -1)  # t504: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t504 = ltorch.cat((t500, t503), -1)  # t504: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t504 = prims.cat((t500, t503), -1)  # t504: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t500, t503
  (t505, t506, t507, t508, _, _, t509, t510, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(t502, t504, t470, 0.0, True, scale=0.08838834764831843)
  t512 = torch.permute(t505, (0, 2, 1, 3))  # t512: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
    # t512 = ltorch.permute(t505, (0, 2, 1, 3))  # t512: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
      # t512 = prims.transpose(t505, (0, 2, 1, 3))  # t512: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
  t513 = torch.reshape(t512, (1, 512, 4096))  # t513: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t513 = ltorch.reshape(t512, (1, 512, 4096))  # t513: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t513 = prims.reshape(t512, (1, 512, 4096))  # t513: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t512
  t514 = torch.nn.functional.linear(t513, t91, None)  # t514: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t514 = ltorch.linear(t513, t91, None)  # t514: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t514 = prims.linear(t513, t91, None)  # t514: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t518, t525, t533] = nvFusion18(t446, t514, t529)
    # t516 = prims.convert_element_type(t446, dtypes.float32)  # t516: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t515 = prims.convert_element_type(t514, dtypes.float32)  # t515: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t517 = prims.add(t515, t516)  # t517: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t518 = prims.convert_element_type(t517, dtypes.bfloat16)  # t518: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t520 = prims.mul(t517, t517)  # t520: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t521 = prims.sum(t520, (2,))  # t521: &#34;cuda:0 f32[1, 512]&#34;
    # t522 = prims.broadcast_in_dim(t521, [1, 512, 1], [0, 1])  # t522: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t523 = prims.div(t522, 4096.0)  # t523: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t524 = prims.add(t523, 1e-05)  # t524: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t525 = prims.rsqrt(t524)  # t525: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t526 = prims.broadcast_in_dim(t525, (1, 512, 4096), (0, 1, 2))  # t526: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t527 = prims.mul(t517, t526)  # t527: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t531 = prims.convert_element_type(t529, dtypes.float32)  # t531: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t532 = prims.mul(t527, t531)  # t532: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t533 = prims.convert_element_type(t532, dtypes.bfloat16)  # t533: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t534 = torch.nn.functional.linear(t533, t22, None)  # t534: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t534 = ltorch.linear(t533, t22, None)  # t534: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t534 = prims.linear(t533, t22, None)  # t534: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t535 = torch.nn.functional.linear(t533, t38, None)  # t535: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t535 = ltorch.linear(t533, t38, None)  # t535: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t535 = prims.linear(t533, t38, None)  # t535: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  [t549] = nvFusion19(t534, t535)
    # t536 = prims.convert_element_type(t534, dtypes.float32)  # t536: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t537 = prims.neg(t536)  # t537: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t538 = prims.exp(t537)  # t538: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t539 = prims.add(1.0, t538)  # t539: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t540 = prims.reciprocal(t539)  # t540: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t544 = prims.mul(t536, t540)  # t544: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t547 = prims.convert_element_type(t535, dtypes.float32)  # t547: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t548 = prims.mul(t544, t547)  # t548: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t549 = prims.convert_element_type(t548, dtypes.bfloat16)  # t549: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t550 = torch.nn.functional.linear(t549, t92, None)  # t550: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t550 = ltorch.linear(t549, t92, None)  # t550: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t550 = prims.linear(t549, t92, None)  # t550: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t554, t561, t569] = nvFusion20(t518, t550, t565)
    # t552 = prims.convert_element_type(t518, dtypes.float32)  # t552: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t551 = prims.convert_element_type(t550, dtypes.float32)  # t551: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t553 = prims.add(t551, t552)  # t553: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t554 = prims.convert_element_type(t553, dtypes.bfloat16)  # t554: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t556 = prims.mul(t553, t553)  # t556: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t557 = prims.sum(t556, (2,))  # t557: &#34;cuda:0 f32[1, 512]&#34;
    # t558 = prims.broadcast_in_dim(t557, [1, 512, 1], [0, 1])  # t558: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t559 = prims.div(t558, 4096.0)  # t559: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t560 = prims.add(t559, 1e-05)  # t560: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t561 = prims.rsqrt(t560)  # t561: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t562 = prims.broadcast_in_dim(t561, (1, 512, 4096), (0, 1, 2))  # t562: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t563 = prims.mul(t553, t562)  # t563: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t567 = prims.convert_element_type(t565, dtypes.float32)  # t567: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t568 = prims.mul(t563, t567)  # t568: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t569 = prims.convert_element_type(t568, dtypes.bfloat16)  # t569: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t570 = torch.nn.functional.linear(t569, t7, None)  # t570: &#34;cuda:0 bf16[1, 512, 12288]&#34;
    # t570 = ltorch.linear(t569, t7, None)  # t570: &#34;cuda:0 bf16[1, 512, 12288]&#34;
      # t570 = prims.linear(t569, t7, None)  # t570: &#34;cuda:0 bf16[1, 512, 12288]&#34;
  t571 = torch.reshape(t570, (1, 512, 32, 3, 128))  # t571: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
    # t571 = ltorch.reshape(t570, (1, 512, 32, 3, 128))  # t571: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
      # t571 = prims.reshape(t570, (1, 512, 32, 3, 128))  # t571: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
  del t570
  t572 = torch.permute(t571, (0, 2, 3, 1, 4))  # t572: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
    # t572 = ltorch.permute(t571, (0, 2, 3, 1, 4))  # t572: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
      # t572 = prims.transpose(t571, (0, 2, 3, 1, 4))  # t572: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
  del t571
  (t573, t574, t575) = torch.split(t572, (1, 1, 1), 2)
    # (t573, t574, t575) = ltorch.split(t572, (1, 1, 1), 2)
      # t573 = prims.slice_prim(t572, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t573: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t574 = prims.slice_prim(t572, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t574: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t575 = prims.slice_prim(t572, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t575: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
  del t572
  t576 = torch.reshape(t573, (1, 32, 512, 128))  # t576: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t576 = ltorch.reshape(t573, (1, 32, 512, 128))  # t576: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t576 = prims.reshape(t573, (1, 32, 512, 128))  # t576: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t573
  t577 = torch.reshape(t574, (1, 32, 512, 128))  # t577: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t577 = ltorch.reshape(t574, (1, 32, 512, 128))  # t577: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t577 = prims.reshape(t574, (1, 32, 512, 128))  # t577: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t574
  t578 = torch.reshape(t575, (1, 32, 512, 128))  # t578: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t578 = ltorch.reshape(t575, (1, 32, 512, 128))  # t578: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t578 = prims.reshape(t575, (1, 32, 512, 128))  # t578: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t575
  t579 = torch_slice_prim_impl(t576, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t579: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t594 = torch_slice_prim_impl(t577, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t594: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t609 = torch_slice_prim_impl(t576, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t609: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t576
  t611 = torch_slice_prim_impl(t577, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t611: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t577
  t580 = torch_slice_prim_impl(t579, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t580: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t581 = torch_slice_prim_impl(t579, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t581: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t595 = torch_slice_prim_impl(t594, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t595: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t596 = torch_slice_prim_impl(t594, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t596: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  [t584, t599] = nvFusion21(t579, t581, t594, t596)
    # t582 = prims.convert_element_type(t581, dtypes.float32)  # t582: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t583 = prims.neg(t582)  # t583: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t584 = prims.convert_element_type(t583, dtypes.bfloat16)  # t584: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
    # t597 = prims.convert_element_type(t596, dtypes.float32)  # t597: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t598 = prims.neg(t597)  # t598: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t599 = prims.convert_element_type(t598, dtypes.bfloat16)  # t599: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  del t581, t596
  t600 = torch.cat((t599, t595), -1)  # t600: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t600 = ltorch.cat((t599, t595), -1)  # t600: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t600 = prims.cat((t599, t595), -1)  # t600: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t599, t595
  t585 = torch.cat((t584, t580), -1)  # t585: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t585 = ltorch.cat((t584, t580), -1)  # t585: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t585 = prims.cat((t584, t580), -1)  # t585: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t584, t580
  [t593, t608] = nvFusion22(t154, t157, t579, t585, t594, t600)
    # t587 = prims.convert_element_type(t579, dtypes.float32)  # t587: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t602 = prims.convert_element_type(t594, dtypes.float32)  # t602: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t603 = prims.mul(t602, t154)  # t603: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t605 = prims.convert_element_type(t600, dtypes.float32)  # t605: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t606 = prims.mul(t605, t157)  # t606: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t607 = prims.add(t603, t606)  # t607: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t608 = prims.convert_element_type(t607, dtypes.bfloat16)  # t608: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t588 = prims.mul(t587, t154)  # t588: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t590 = prims.convert_element_type(t585, dtypes.float32)  # t590: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t591 = prims.mul(t590, t157)  # t591: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t592 = prims.add(t588, t591)  # t592: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t593 = prims.convert_element_type(t592, dtypes.bfloat16)  # t593: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t579, t585, t594, t600
  t612 = torch.cat((t608, t611), -1)  # t612: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t612 = ltorch.cat((t608, t611), -1)  # t612: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t612 = prims.cat((t608, t611), -1)  # t612: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t608, t611
  t610 = torch.cat((t593, t609), -1)  # t610: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t610 = ltorch.cat((t593, t609), -1)  # t610: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t610 = prims.cat((t593, t609), -1)  # t610: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t593, t609
  (t613, t614, t615, t616, _, _, t617, t618, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(t610, t612, t578, 0.0, True, scale=0.08838834764831843)
  t620 = torch.permute(t613, (0, 2, 1, 3))  # t620: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
    # t620 = ltorch.permute(t613, (0, 2, 1, 3))  # t620: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
      # t620 = prims.transpose(t613, (0, 2, 1, 3))  # t620: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
  t621 = torch.reshape(t620, (1, 512, 4096))  # t621: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t621 = ltorch.reshape(t620, (1, 512, 4096))  # t621: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t621 = prims.reshape(t620, (1, 512, 4096))  # t621: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t620
  t622 = torch.nn.functional.linear(t621, t93, None)  # t622: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t622 = ltorch.linear(t621, t93, None)  # t622: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t622 = prims.linear(t621, t93, None)  # t622: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t626, t633, t641] = nvFusion23(t554, t622, t637)
    # t624 = prims.convert_element_type(t554, dtypes.float32)  # t624: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t623 = prims.convert_element_type(t622, dtypes.float32)  # t623: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t625 = prims.add(t623, t624)  # t625: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t626 = prims.convert_element_type(t625, dtypes.bfloat16)  # t626: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t628 = prims.mul(t625, t625)  # t628: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t629 = prims.sum(t628, (2,))  # t629: &#34;cuda:0 f32[1, 512]&#34;
    # t630 = prims.broadcast_in_dim(t629, [1, 512, 1], [0, 1])  # t630: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t631 = prims.div(t630, 4096.0)  # t631: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t632 = prims.add(t631, 1e-05)  # t632: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t633 = prims.rsqrt(t632)  # t633: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t634 = prims.broadcast_in_dim(t633, (1, 512, 4096), (0, 1, 2))  # t634: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t635 = prims.mul(t625, t634)  # t635: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t639 = prims.convert_element_type(t637, dtypes.float32)  # t639: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t640 = prims.mul(t635, t639)  # t640: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t641 = prims.convert_element_type(t640, dtypes.bfloat16)  # t641: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t643 = torch.nn.functional.linear(t641, t39, None)  # t643: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t643 = ltorch.linear(t641, t39, None)  # t643: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t643 = prims.linear(t641, t39, None)  # t643: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t642 = torch.nn.functional.linear(t641, t23, None)  # t642: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t642 = ltorch.linear(t641, t23, None)  # t642: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t642 = prims.linear(t641, t23, None)  # t642: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  [t657] = nvFusion24(t642, t643)
    # t644 = prims.convert_element_type(t642, dtypes.float32)  # t644: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t645 = prims.neg(t644)  # t645: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t646 = prims.exp(t645)  # t646: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t647 = prims.add(1.0, t646)  # t647: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t648 = prims.reciprocal(t647)  # t648: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t652 = prims.mul(t644, t648)  # t652: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t655 = prims.convert_element_type(t643, dtypes.float32)  # t655: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t656 = prims.mul(t652, t655)  # t656: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t657 = prims.convert_element_type(t656, dtypes.bfloat16)  # t657: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t658 = torch.nn.functional.linear(t657, t94, None)  # t658: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t658 = ltorch.linear(t657, t94, None)  # t658: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t658 = prims.linear(t657, t94, None)  # t658: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t662, t669, t677] = nvFusion25(t626, t658, t673)
    # t660 = prims.convert_element_type(t626, dtypes.float32)  # t660: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t659 = prims.convert_element_type(t658, dtypes.float32)  # t659: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t661 = prims.add(t659, t660)  # t661: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t662 = prims.convert_element_type(t661, dtypes.bfloat16)  # t662: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t664 = prims.mul(t661, t661)  # t664: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t665 = prims.sum(t664, (2,))  # t665: &#34;cuda:0 f32[1, 512]&#34;
    # t666 = prims.broadcast_in_dim(t665, [1, 512, 1], [0, 1])  # t666: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t667 = prims.div(t666, 4096.0)  # t667: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t668 = prims.add(t667, 1e-05)  # t668: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t669 = prims.rsqrt(t668)  # t669: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t670 = prims.broadcast_in_dim(t669, (1, 512, 4096), (0, 1, 2))  # t670: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t671 = prims.mul(t661, t670)  # t671: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t675 = prims.convert_element_type(t673, dtypes.float32)  # t675: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t676 = prims.mul(t671, t675)  # t676: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t677 = prims.convert_element_type(t676, dtypes.bfloat16)  # t677: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t678 = torch.nn.functional.linear(t677, t8, None)  # t678: &#34;cuda:0 bf16[1, 512, 12288]&#34;
    # t678 = ltorch.linear(t677, t8, None)  # t678: &#34;cuda:0 bf16[1, 512, 12288]&#34;
      # t678 = prims.linear(t677, t8, None)  # t678: &#34;cuda:0 bf16[1, 512, 12288]&#34;
  t679 = torch.reshape(t678, (1, 512, 32, 3, 128))  # t679: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
    # t679 = ltorch.reshape(t678, (1, 512, 32, 3, 128))  # t679: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
      # t679 = prims.reshape(t678, (1, 512, 32, 3, 128))  # t679: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
  del t678
  t680 = torch.permute(t679, (0, 2, 3, 1, 4))  # t680: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
    # t680 = ltorch.permute(t679, (0, 2, 3, 1, 4))  # t680: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
      # t680 = prims.transpose(t679, (0, 2, 3, 1, 4))  # t680: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
  del t679
  (t681, t682, t683) = torch.split(t680, (1, 1, 1), 2)
    # (t681, t682, t683) = ltorch.split(t680, (1, 1, 1), 2)
      # t681 = prims.slice_prim(t680, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t681: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t682 = prims.slice_prim(t680, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t682: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t683 = prims.slice_prim(t680, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t683: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
  del t680
  t684 = torch.reshape(t681, (1, 32, 512, 128))  # t684: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t684 = ltorch.reshape(t681, (1, 32, 512, 128))  # t684: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t684 = prims.reshape(t681, (1, 32, 512, 128))  # t684: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t681
  t685 = torch.reshape(t682, (1, 32, 512, 128))  # t685: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t685 = ltorch.reshape(t682, (1, 32, 512, 128))  # t685: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t685 = prims.reshape(t682, (1, 32, 512, 128))  # t685: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t682
  t686 = torch.reshape(t683, (1, 32, 512, 128))  # t686: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t686 = ltorch.reshape(t683, (1, 32, 512, 128))  # t686: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t686 = prims.reshape(t683, (1, 32, 512, 128))  # t686: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t683
  t687 = torch_slice_prim_impl(t684, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t687: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t702 = torch_slice_prim_impl(t685, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t702: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t717 = torch_slice_prim_impl(t684, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t717: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t684
  t719 = torch_slice_prim_impl(t685, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t719: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t685
  t688 = torch_slice_prim_impl(t687, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t688: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t689 = torch_slice_prim_impl(t687, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t689: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t703 = torch_slice_prim_impl(t702, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t703: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t704 = torch_slice_prim_impl(t702, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t704: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  [t692, t707] = nvFusion26(t687, t689, t702, t704)
    # t690 = prims.convert_element_type(t689, dtypes.float32)  # t690: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t691 = prims.neg(t690)  # t691: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t692 = prims.convert_element_type(t691, dtypes.bfloat16)  # t692: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
    # t705 = prims.convert_element_type(t704, dtypes.float32)  # t705: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t706 = prims.neg(t705)  # t706: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t707 = prims.convert_element_type(t706, dtypes.bfloat16)  # t707: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  del t689, t704
  t708 = torch.cat((t707, t703), -1)  # t708: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t708 = ltorch.cat((t707, t703), -1)  # t708: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t708 = prims.cat((t707, t703), -1)  # t708: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t707, t703
  t693 = torch.cat((t692, t688), -1)  # t693: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t693 = ltorch.cat((t692, t688), -1)  # t693: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t693 = prims.cat((t692, t688), -1)  # t693: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t692, t688
  [t701, t716] = nvFusion27(t154, t157, t687, t693, t702, t708)
    # t695 = prims.convert_element_type(t687, dtypes.float32)  # t695: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t710 = prims.convert_element_type(t702, dtypes.float32)  # t710: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t711 = prims.mul(t710, t154)  # t711: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t713 = prims.convert_element_type(t708, dtypes.float32)  # t713: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t714 = prims.mul(t713, t157)  # t714: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t715 = prims.add(t711, t714)  # t715: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t716 = prims.convert_element_type(t715, dtypes.bfloat16)  # t716: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t696 = prims.mul(t695, t154)  # t696: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t698 = prims.convert_element_type(t693, dtypes.float32)  # t698: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t699 = prims.mul(t698, t157)  # t699: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t700 = prims.add(t696, t699)  # t700: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t701 = prims.convert_element_type(t700, dtypes.bfloat16)  # t701: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t687, t693, t702, t708
  t720 = torch.cat((t716, t719), -1)  # t720: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t720 = ltorch.cat((t716, t719), -1)  # t720: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t720 = prims.cat((t716, t719), -1)  # t720: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t716, t719
  t718 = torch.cat((t701, t717), -1)  # t718: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t718 = ltorch.cat((t701, t717), -1)  # t718: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t718 = prims.cat((t701, t717), -1)  # t718: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t701, t717
  (t721, t722, t723, t724, _, _, t725, t726, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(t718, t720, t686, 0.0, True, scale=0.08838834764831843)
  t728 = torch.permute(t721, (0, 2, 1, 3))  # t728: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
    # t728 = ltorch.permute(t721, (0, 2, 1, 3))  # t728: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
      # t728 = prims.transpose(t721, (0, 2, 1, 3))  # t728: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
  t729 = torch.reshape(t728, (1, 512, 4096))  # t729: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t729 = ltorch.reshape(t728, (1, 512, 4096))  # t729: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t729 = prims.reshape(t728, (1, 512, 4096))  # t729: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t728
  t730 = torch.nn.functional.linear(t729, t95, None)  # t730: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t730 = ltorch.linear(t729, t95, None)  # t730: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t730 = prims.linear(t729, t95, None)  # t730: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t734, t741, t749] = nvFusion28(t662, t730, t745)
    # t732 = prims.convert_element_type(t662, dtypes.float32)  # t732: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t731 = prims.convert_element_type(t730, dtypes.float32)  # t731: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t733 = prims.add(t731, t732)  # t733: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t734 = prims.convert_element_type(t733, dtypes.bfloat16)  # t734: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t736 = prims.mul(t733, t733)  # t736: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t737 = prims.sum(t736, (2,))  # t737: &#34;cuda:0 f32[1, 512]&#34;
    # t738 = prims.broadcast_in_dim(t737, [1, 512, 1], [0, 1])  # t738: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t739 = prims.div(t738, 4096.0)  # t739: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t740 = prims.add(t739, 1e-05)  # t740: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t741 = prims.rsqrt(t740)  # t741: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t742 = prims.broadcast_in_dim(t741, (1, 512, 4096), (0, 1, 2))  # t742: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t743 = prims.mul(t733, t742)  # t743: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t747 = prims.convert_element_type(t745, dtypes.float32)  # t747: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t748 = prims.mul(t743, t747)  # t748: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t749 = prims.convert_element_type(t748, dtypes.bfloat16)  # t749: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t750 = torch.nn.functional.linear(t749, t24, None)  # t750: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t750 = ltorch.linear(t749, t24, None)  # t750: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t750 = prims.linear(t749, t24, None)  # t750: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t751 = torch.nn.functional.linear(t749, t40, None)  # t751: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t751 = ltorch.linear(t749, t40, None)  # t751: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t751 = prims.linear(t749, t40, None)  # t751: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  [t765] = nvFusion29(t750, t751)
    # t752 = prims.convert_element_type(t750, dtypes.float32)  # t752: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t753 = prims.neg(t752)  # t753: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t754 = prims.exp(t753)  # t754: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t755 = prims.add(1.0, t754)  # t755: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t756 = prims.reciprocal(t755)  # t756: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t760 = prims.mul(t752, t756)  # t760: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t763 = prims.convert_element_type(t751, dtypes.float32)  # t763: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t764 = prims.mul(t760, t763)  # t764: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t765 = prims.convert_element_type(t764, dtypes.bfloat16)  # t765: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t766 = torch.nn.functional.linear(t765, t96, None)  # t766: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t766 = ltorch.linear(t765, t96, None)  # t766: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t766 = prims.linear(t765, t96, None)  # t766: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t770, t777, t785] = nvFusion30(t734, t766, t781)
    # t768 = prims.convert_element_type(t734, dtypes.float32)  # t768: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t767 = prims.convert_element_type(t766, dtypes.float32)  # t767: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t769 = prims.add(t767, t768)  # t769: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t770 = prims.convert_element_type(t769, dtypes.bfloat16)  # t770: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t772 = prims.mul(t769, t769)  # t772: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t773 = prims.sum(t772, (2,))  # t773: &#34;cuda:0 f32[1, 512]&#34;
    # t774 = prims.broadcast_in_dim(t773, [1, 512, 1], [0, 1])  # t774: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t775 = prims.div(t774, 4096.0)  # t775: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t776 = prims.add(t775, 1e-05)  # t776: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t777 = prims.rsqrt(t776)  # t777: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t778 = prims.broadcast_in_dim(t777, (1, 512, 4096), (0, 1, 2))  # t778: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t779 = prims.mul(t769, t778)  # t779: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t783 = prims.convert_element_type(t781, dtypes.float32)  # t783: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t784 = prims.mul(t779, t783)  # t784: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t785 = prims.convert_element_type(t784, dtypes.bfloat16)  # t785: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t786 = torch.nn.functional.linear(t785, t9, None)  # t786: &#34;cuda:0 bf16[1, 512, 12288]&#34;
    # t786 = ltorch.linear(t785, t9, None)  # t786: &#34;cuda:0 bf16[1, 512, 12288]&#34;
      # t786 = prims.linear(t785, t9, None)  # t786: &#34;cuda:0 bf16[1, 512, 12288]&#34;
  t787 = torch.reshape(t786, (1, 512, 32, 3, 128))  # t787: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
    # t787 = ltorch.reshape(t786, (1, 512, 32, 3, 128))  # t787: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
      # t787 = prims.reshape(t786, (1, 512, 32, 3, 128))  # t787: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
  del t786
  t788 = torch.permute(t787, (0, 2, 3, 1, 4))  # t788: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
    # t788 = ltorch.permute(t787, (0, 2, 3, 1, 4))  # t788: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
      # t788 = prims.transpose(t787, (0, 2, 3, 1, 4))  # t788: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
  del t787
  (t789, t790, t791) = torch.split(t788, (1, 1, 1), 2)
    # (t789, t790, t791) = ltorch.split(t788, (1, 1, 1), 2)
      # t789 = prims.slice_prim(t788, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t789: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t790 = prims.slice_prim(t788, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t790: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t791 = prims.slice_prim(t788, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t791: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
  del t788
  t792 = torch.reshape(t789, (1, 32, 512, 128))  # t792: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t792 = ltorch.reshape(t789, (1, 32, 512, 128))  # t792: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t792 = prims.reshape(t789, (1, 32, 512, 128))  # t792: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t789
  t793 = torch.reshape(t790, (1, 32, 512, 128))  # t793: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t793 = ltorch.reshape(t790, (1, 32, 512, 128))  # t793: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t793 = prims.reshape(t790, (1, 32, 512, 128))  # t793: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t790
  t794 = torch.reshape(t791, (1, 32, 512, 128))  # t794: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t794 = ltorch.reshape(t791, (1, 32, 512, 128))  # t794: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t794 = prims.reshape(t791, (1, 32, 512, 128))  # t794: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t791
  t795 = torch_slice_prim_impl(t792, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t795: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t810 = torch_slice_prim_impl(t793, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t810: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t825 = torch_slice_prim_impl(t792, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t825: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t792
  t827 = torch_slice_prim_impl(t793, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t827: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t793
  t796 = torch_slice_prim_impl(t795, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t796: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t797 = torch_slice_prim_impl(t795, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t797: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t811 = torch_slice_prim_impl(t810, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t811: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t812 = torch_slice_prim_impl(t810, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t812: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  [t800, t815] = nvFusion31(t795, t797, t810, t812)
    # t798 = prims.convert_element_type(t797, dtypes.float32)  # t798: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t799 = prims.neg(t798)  # t799: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t800 = prims.convert_element_type(t799, dtypes.bfloat16)  # t800: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
    # t813 = prims.convert_element_type(t812, dtypes.float32)  # t813: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t814 = prims.neg(t813)  # t814: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t815 = prims.convert_element_type(t814, dtypes.bfloat16)  # t815: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  del t797, t812
  t816 = torch.cat((t815, t811), -1)  # t816: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t816 = ltorch.cat((t815, t811), -1)  # t816: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t816 = prims.cat((t815, t811), -1)  # t816: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t815, t811
  t801 = torch.cat((t800, t796), -1)  # t801: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t801 = ltorch.cat((t800, t796), -1)  # t801: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t801 = prims.cat((t800, t796), -1)  # t801: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t800, t796
  [t809, t824] = nvFusion32(t154, t157, t795, t801, t810, t816)
    # t803 = prims.convert_element_type(t795, dtypes.float32)  # t803: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t818 = prims.convert_element_type(t810, dtypes.float32)  # t818: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t819 = prims.mul(t818, t154)  # t819: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t821 = prims.convert_element_type(t816, dtypes.float32)  # t821: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t822 = prims.mul(t821, t157)  # t822: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t823 = prims.add(t819, t822)  # t823: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t824 = prims.convert_element_type(t823, dtypes.bfloat16)  # t824: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t804 = prims.mul(t803, t154)  # t804: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t806 = prims.convert_element_type(t801, dtypes.float32)  # t806: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t807 = prims.mul(t806, t157)  # t807: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t808 = prims.add(t804, t807)  # t808: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t809 = prims.convert_element_type(t808, dtypes.bfloat16)  # t809: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t795, t801, t810, t816
  t828 = torch.cat((t824, t827), -1)  # t828: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t828 = ltorch.cat((t824, t827), -1)  # t828: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t828 = prims.cat((t824, t827), -1)  # t828: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t824, t827
  t826 = torch.cat((t809, t825), -1)  # t826: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t826 = ltorch.cat((t809, t825), -1)  # t826: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t826 = prims.cat((t809, t825), -1)  # t826: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t809, t825
  (t829, t830, t831, t832, _, _, t833, t834, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(t826, t828, t794, 0.0, True, scale=0.08838834764831843)
  t836 = torch.permute(t829, (0, 2, 1, 3))  # t836: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
    # t836 = ltorch.permute(t829, (0, 2, 1, 3))  # t836: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
      # t836 = prims.transpose(t829, (0, 2, 1, 3))  # t836: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
  t837 = torch.reshape(t836, (1, 512, 4096))  # t837: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t837 = ltorch.reshape(t836, (1, 512, 4096))  # t837: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t837 = prims.reshape(t836, (1, 512, 4096))  # t837: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t836
  t838 = torch.nn.functional.linear(t837, t97, None)  # t838: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t838 = ltorch.linear(t837, t97, None)  # t838: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t838 = prims.linear(t837, t97, None)  # t838: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t842, t849, t857] = nvFusion33(t770, t838, t853)
    # t840 = prims.convert_element_type(t770, dtypes.float32)  # t840: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t839 = prims.convert_element_type(t838, dtypes.float32)  # t839: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t841 = prims.add(t839, t840)  # t841: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t842 = prims.convert_element_type(t841, dtypes.bfloat16)  # t842: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t844 = prims.mul(t841, t841)  # t844: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t845 = prims.sum(t844, (2,))  # t845: &#34;cuda:0 f32[1, 512]&#34;
    # t846 = prims.broadcast_in_dim(t845, [1, 512, 1], [0, 1])  # t846: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t847 = prims.div(t846, 4096.0)  # t847: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t848 = prims.add(t847, 1e-05)  # t848: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t849 = prims.rsqrt(t848)  # t849: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t850 = prims.broadcast_in_dim(t849, (1, 512, 4096), (0, 1, 2))  # t850: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t851 = prims.mul(t841, t850)  # t851: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t855 = prims.convert_element_type(t853, dtypes.float32)  # t855: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t856 = prims.mul(t851, t855)  # t856: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t857 = prims.convert_element_type(t856, dtypes.bfloat16)  # t857: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t858 = torch.nn.functional.linear(t857, t25, None)  # t858: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t858 = ltorch.linear(t857, t25, None)  # t858: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t858 = prims.linear(t857, t25, None)  # t858: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t859 = torch.nn.functional.linear(t857, t41, None)  # t859: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t859 = ltorch.linear(t857, t41, None)  # t859: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t859 = prims.linear(t857, t41, None)  # t859: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  [t873] = nvFusion34(t858, t859)
    # t860 = prims.convert_element_type(t858, dtypes.float32)  # t860: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t861 = prims.neg(t860)  # t861: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t862 = prims.exp(t861)  # t862: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t863 = prims.add(1.0, t862)  # t863: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t864 = prims.reciprocal(t863)  # t864: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t868 = prims.mul(t860, t864)  # t868: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t871 = prims.convert_element_type(t859, dtypes.float32)  # t871: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t872 = prims.mul(t868, t871)  # t872: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t873 = prims.convert_element_type(t872, dtypes.bfloat16)  # t873: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t874 = torch.nn.functional.linear(t873, t98, None)  # t874: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t874 = ltorch.linear(t873, t98, None)  # t874: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t874 = prims.linear(t873, t98, None)  # t874: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t878, t885, t893] = nvFusion35(t842, t874, t889)
    # t876 = prims.convert_element_type(t842, dtypes.float32)  # t876: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t875 = prims.convert_element_type(t874, dtypes.float32)  # t875: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t877 = prims.add(t875, t876)  # t877: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t878 = prims.convert_element_type(t877, dtypes.bfloat16)  # t878: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t880 = prims.mul(t877, t877)  # t880: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t881 = prims.sum(t880, (2,))  # t881: &#34;cuda:0 f32[1, 512]&#34;
    # t882 = prims.broadcast_in_dim(t881, [1, 512, 1], [0, 1])  # t882: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t883 = prims.div(t882, 4096.0)  # t883: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t884 = prims.add(t883, 1e-05)  # t884: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t885 = prims.rsqrt(t884)  # t885: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t886 = prims.broadcast_in_dim(t885, (1, 512, 4096), (0, 1, 2))  # t886: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t887 = prims.mul(t877, t886)  # t887: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t891 = prims.convert_element_type(t889, dtypes.float32)  # t891: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t892 = prims.mul(t887, t891)  # t892: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t893 = prims.convert_element_type(t892, dtypes.bfloat16)  # t893: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t894 = torch.nn.functional.linear(t893, t10, None)  # t894: &#34;cuda:0 bf16[1, 512, 12288]&#34;
    # t894 = ltorch.linear(t893, t10, None)  # t894: &#34;cuda:0 bf16[1, 512, 12288]&#34;
      # t894 = prims.linear(t893, t10, None)  # t894: &#34;cuda:0 bf16[1, 512, 12288]&#34;
  t895 = torch.reshape(t894, (1, 512, 32, 3, 128))  # t895: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
    # t895 = ltorch.reshape(t894, (1, 512, 32, 3, 128))  # t895: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
      # t895 = prims.reshape(t894, (1, 512, 32, 3, 128))  # t895: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
  del t894
  t896 = torch.permute(t895, (0, 2, 3, 1, 4))  # t896: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
    # t896 = ltorch.permute(t895, (0, 2, 3, 1, 4))  # t896: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
      # t896 = prims.transpose(t895, (0, 2, 3, 1, 4))  # t896: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
  del t895
  (t897, t898, t899) = torch.split(t896, (1, 1, 1), 2)
    # (t897, t898, t899) = ltorch.split(t896, (1, 1, 1), 2)
      # t897 = prims.slice_prim(t896, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t897: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t898 = prims.slice_prim(t896, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t898: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t899 = prims.slice_prim(t896, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t899: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
  del t896
  t900 = torch.reshape(t897, (1, 32, 512, 128))  # t900: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t900 = ltorch.reshape(t897, (1, 32, 512, 128))  # t900: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t900 = prims.reshape(t897, (1, 32, 512, 128))  # t900: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t897
  t901 = torch.reshape(t898, (1, 32, 512, 128))  # t901: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t901 = ltorch.reshape(t898, (1, 32, 512, 128))  # t901: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t901 = prims.reshape(t898, (1, 32, 512, 128))  # t901: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t898
  t902 = torch.reshape(t899, (1, 32, 512, 128))  # t902: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t902 = ltorch.reshape(t899, (1, 32, 512, 128))  # t902: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t902 = prims.reshape(t899, (1, 32, 512, 128))  # t902: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t899
  t935 = torch_slice_prim_impl(t901, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t935: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  t903 = torch_slice_prim_impl(t900, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t903: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t918 = torch_slice_prim_impl(t901, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t918: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t901
  t933 = torch_slice_prim_impl(t900, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t933: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t900
  t904 = torch_slice_prim_impl(t903, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t904: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t905 = torch_slice_prim_impl(t903, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t905: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t919 = torch_slice_prim_impl(t918, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t919: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t920 = torch_slice_prim_impl(t918, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t920: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  [t908, t923] = nvFusion36(t903, t905, t918, t920)
    # t906 = prims.convert_element_type(t905, dtypes.float32)  # t906: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t907 = prims.neg(t906)  # t907: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t908 = prims.convert_element_type(t907, dtypes.bfloat16)  # t908: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
    # t921 = prims.convert_element_type(t920, dtypes.float32)  # t921: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t922 = prims.neg(t921)  # t922: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t923 = prims.convert_element_type(t922, dtypes.bfloat16)  # t923: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  del t905, t920
  t924 = torch.cat((t923, t919), -1)  # t924: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t924 = ltorch.cat((t923, t919), -1)  # t924: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t924 = prims.cat((t923, t919), -1)  # t924: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t923, t919
  t909 = torch.cat((t908, t904), -1)  # t909: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t909 = ltorch.cat((t908, t904), -1)  # t909: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t909 = prims.cat((t908, t904), -1)  # t909: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t908, t904
  [t917, t932] = nvFusion37(t154, t157, t903, t909, t918, t924)
    # t911 = prims.convert_element_type(t903, dtypes.float32)  # t911: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t926 = prims.convert_element_type(t918, dtypes.float32)  # t926: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t927 = prims.mul(t926, t154)  # t927: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t929 = prims.convert_element_type(t924, dtypes.float32)  # t929: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t930 = prims.mul(t929, t157)  # t930: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t931 = prims.add(t927, t930)  # t931: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t932 = prims.convert_element_type(t931, dtypes.bfloat16)  # t932: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t912 = prims.mul(t911, t154)  # t912: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t914 = prims.convert_element_type(t909, dtypes.float32)  # t914: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t915 = prims.mul(t914, t157)  # t915: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t916 = prims.add(t912, t915)  # t916: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t917 = prims.convert_element_type(t916, dtypes.bfloat16)  # t917: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t903, t909, t918, t924
  t936 = torch.cat((t932, t935), -1)  # t936: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t936 = ltorch.cat((t932, t935), -1)  # t936: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t936 = prims.cat((t932, t935), -1)  # t936: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t932, t935
  t934 = torch.cat((t917, t933), -1)  # t934: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t934 = ltorch.cat((t917, t933), -1)  # t934: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t934 = prims.cat((t917, t933), -1)  # t934: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t917, t933
  (t937, t938, t939, t940, _, _, t941, t942, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(t934, t936, t902, 0.0, True, scale=0.08838834764831843)
  t944 = torch.permute(t937, (0, 2, 1, 3))  # t944: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
    # t944 = ltorch.permute(t937, (0, 2, 1, 3))  # t944: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
      # t944 = prims.transpose(t937, (0, 2, 1, 3))  # t944: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
  t945 = torch.reshape(t944, (1, 512, 4096))  # t945: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t945 = ltorch.reshape(t944, (1, 512, 4096))  # t945: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t945 = prims.reshape(t944, (1, 512, 4096))  # t945: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t944
  t946 = torch.nn.functional.linear(t945, t99, None)  # t946: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t946 = ltorch.linear(t945, t99, None)  # t946: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t946 = prims.linear(t945, t99, None)  # t946: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t950, t957, t965] = nvFusion38(t878, t946, t961)
    # t948 = prims.convert_element_type(t878, dtypes.float32)  # t948: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t947 = prims.convert_element_type(t946, dtypes.float32)  # t947: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t949 = prims.add(t947, t948)  # t949: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t950 = prims.convert_element_type(t949, dtypes.bfloat16)  # t950: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t952 = prims.mul(t949, t949)  # t952: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t953 = prims.sum(t952, (2,))  # t953: &#34;cuda:0 f32[1, 512]&#34;
    # t954 = prims.broadcast_in_dim(t953, [1, 512, 1], [0, 1])  # t954: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t955 = prims.div(t954, 4096.0)  # t955: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t956 = prims.add(t955, 1e-05)  # t956: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t957 = prims.rsqrt(t956)  # t957: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t958 = prims.broadcast_in_dim(t957, (1, 512, 4096), (0, 1, 2))  # t958: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t959 = prims.mul(t949, t958)  # t959: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t963 = prims.convert_element_type(t961, dtypes.float32)  # t963: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t964 = prims.mul(t959, t963)  # t964: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t965 = prims.convert_element_type(t964, dtypes.bfloat16)  # t965: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t967 = torch.nn.functional.linear(t965, t42, None)  # t967: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t967 = ltorch.linear(t965, t42, None)  # t967: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t967 = prims.linear(t965, t42, None)  # t967: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t966 = torch.nn.functional.linear(t965, t26, None)  # t966: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t966 = ltorch.linear(t965, t26, None)  # t966: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t966 = prims.linear(t965, t26, None)  # t966: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  [t981] = nvFusion39(t966, t967)
    # t968 = prims.convert_element_type(t966, dtypes.float32)  # t968: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t969 = prims.neg(t968)  # t969: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t970 = prims.exp(t969)  # t970: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t971 = prims.add(1.0, t970)  # t971: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t972 = prims.reciprocal(t971)  # t972: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t976 = prims.mul(t968, t972)  # t976: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t979 = prims.convert_element_type(t967, dtypes.float32)  # t979: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t980 = prims.mul(t976, t979)  # t980: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t981 = prims.convert_element_type(t980, dtypes.bfloat16)  # t981: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t982 = torch.nn.functional.linear(t981, t100, None)  # t982: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t982 = ltorch.linear(t981, t100, None)  # t982: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t982 = prims.linear(t981, t100, None)  # t982: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t1001, t986, t993] = nvFusion40(t950, t982, t997)
    # t984 = prims.convert_element_type(t950, dtypes.float32)  # t984: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t983 = prims.convert_element_type(t982, dtypes.float32)  # t983: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t985 = prims.add(t983, t984)  # t985: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t986 = prims.convert_element_type(t985, dtypes.bfloat16)  # t986: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t988 = prims.mul(t985, t985)  # t988: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t989 = prims.sum(t988, (2,))  # t989: &#34;cuda:0 f32[1, 512]&#34;
    # t990 = prims.broadcast_in_dim(t989, [1, 512, 1], [0, 1])  # t990: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t991 = prims.div(t990, 4096.0)  # t991: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t992 = prims.add(t991, 1e-05)  # t992: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t993 = prims.rsqrt(t992)  # t993: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t994 = prims.broadcast_in_dim(t993, (1, 512, 4096), (0, 1, 2))  # t994: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t995 = prims.mul(t985, t994)  # t995: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t999 = prims.convert_element_type(t997, dtypes.float32)  # t999: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1000 = prims.mul(t995, t999)  # t1000: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1001 = prims.convert_element_type(t1000, dtypes.bfloat16)  # t1001: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t1002 = torch.nn.functional.linear(t1001, t11, None)  # t1002: &#34;cuda:0 bf16[1, 512, 12288]&#34;
    # t1002 = ltorch.linear(t1001, t11, None)  # t1002: &#34;cuda:0 bf16[1, 512, 12288]&#34;
      # t1002 = prims.linear(t1001, t11, None)  # t1002: &#34;cuda:0 bf16[1, 512, 12288]&#34;
  t1003 = torch.reshape(t1002, (1, 512, 32, 3, 128))  # t1003: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
    # t1003 = ltorch.reshape(t1002, (1, 512, 32, 3, 128))  # t1003: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
      # t1003 = prims.reshape(t1002, (1, 512, 32, 3, 128))  # t1003: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
  del t1002
  t1004 = torch.permute(t1003, (0, 2, 3, 1, 4))  # t1004: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
    # t1004 = ltorch.permute(t1003, (0, 2, 3, 1, 4))  # t1004: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
      # t1004 = prims.transpose(t1003, (0, 2, 3, 1, 4))  # t1004: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
  del t1003
  (t1005, t1006, t1007) = torch.split(t1004, (1, 1, 1), 2)
    # (t1005, t1006, t1007) = ltorch.split(t1004, (1, 1, 1), 2)
      # t1005 = prims.slice_prim(t1004, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1005: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t1006 = prims.slice_prim(t1004, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1006: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t1007 = prims.slice_prim(t1004, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1007: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
  del t1004
  t1008 = torch.reshape(t1005, (1, 32, 512, 128))  # t1008: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1008 = ltorch.reshape(t1005, (1, 32, 512, 128))  # t1008: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1008 = prims.reshape(t1005, (1, 32, 512, 128))  # t1008: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1005
  t1009 = torch.reshape(t1006, (1, 32, 512, 128))  # t1009: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1009 = ltorch.reshape(t1006, (1, 32, 512, 128))  # t1009: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1009 = prims.reshape(t1006, (1, 32, 512, 128))  # t1009: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1006
  t1010 = torch.reshape(t1007, (1, 32, 512, 128))  # t1010: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1010 = ltorch.reshape(t1007, (1, 32, 512, 128))  # t1010: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1010 = prims.reshape(t1007, (1, 32, 512, 128))  # t1010: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1007
  t1026 = torch_slice_prim_impl(t1009, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1026: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t1041 = torch_slice_prim_impl(t1008, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1041: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  t1043 = torch_slice_prim_impl(t1009, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1043: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t1009
  t1011 = torch_slice_prim_impl(t1008, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1011: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1008
  t1027 = torch_slice_prim_impl(t1026, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1027: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1028 = torch_slice_prim_impl(t1026, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1028: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1013 = torch_slice_prim_impl(t1011, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1013: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1012 = torch_slice_prim_impl(t1011, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1012: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  [t1016, t1031] = nvFusion41(t1011, t1013, t1026, t1028)
    # t1014 = prims.convert_element_type(t1013, dtypes.float32)  # t1014: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1015 = prims.neg(t1014)  # t1015: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1016 = prims.convert_element_type(t1015, dtypes.bfloat16)  # t1016: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
    # t1029 = prims.convert_element_type(t1028, dtypes.float32)  # t1029: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1030 = prims.neg(t1029)  # t1030: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1031 = prims.convert_element_type(t1030, dtypes.bfloat16)  # t1031: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  del t1013, t1028
  t1032 = torch.cat((t1031, t1027), -1)  # t1032: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1032 = ltorch.cat((t1031, t1027), -1)  # t1032: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1032 = prims.cat((t1031, t1027), -1)  # t1032: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1031, t1027
  t1017 = torch.cat((t1016, t1012), -1)  # t1017: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1017 = ltorch.cat((t1016, t1012), -1)  # t1017: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1017 = prims.cat((t1016, t1012), -1)  # t1017: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1016, t1012
  [t1025, t1040] = nvFusion42(t1011, t1017, t1026, t1032, t154, t157)
    # t1019 = prims.convert_element_type(t1011, dtypes.float32)  # t1019: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1034 = prims.convert_element_type(t1026, dtypes.float32)  # t1034: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1020 = prims.mul(t1019, t154)  # t1020: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1022 = prims.convert_element_type(t1017, dtypes.float32)  # t1022: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1023 = prims.mul(t1022, t157)  # t1023: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1024 = prims.add(t1020, t1023)  # t1024: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1025 = prims.convert_element_type(t1024, dtypes.bfloat16)  # t1025: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1035 = prims.mul(t1034, t154)  # t1035: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1037 = prims.convert_element_type(t1032, dtypes.float32)  # t1037: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1038 = prims.mul(t1037, t157)  # t1038: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1039 = prims.add(t1035, t1038)  # t1039: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1040 = prims.convert_element_type(t1039, dtypes.bfloat16)  # t1040: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1011, t1017, t1026, t1032
  t1042 = torch.cat((t1025, t1041), -1)  # t1042: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1042 = ltorch.cat((t1025, t1041), -1)  # t1042: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1042 = prims.cat((t1025, t1041), -1)  # t1042: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1025, t1041
  t1044 = torch.cat((t1040, t1043), -1)  # t1044: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1044 = ltorch.cat((t1040, t1043), -1)  # t1044: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1044 = prims.cat((t1040, t1043), -1)  # t1044: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1040, t1043
  (t1045, t1046, t1047, t1048, _, _, t1049, t1050, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(t1042, t1044, t1010, 0.0, True, scale=0.08838834764831843)
  t1052 = torch.permute(t1045, (0, 2, 1, 3))  # t1052: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
    # t1052 = ltorch.permute(t1045, (0, 2, 1, 3))  # t1052: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
      # t1052 = prims.transpose(t1045, (0, 2, 1, 3))  # t1052: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
  t1053 = torch.reshape(t1052, (1, 512, 4096))  # t1053: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1053 = ltorch.reshape(t1052, (1, 512, 4096))  # t1053: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1053 = prims.reshape(t1052, (1, 512, 4096))  # t1053: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t1052
  t1054 = torch.nn.functional.linear(t1053, t101, None)  # t1054: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1054 = ltorch.linear(t1053, t101, None)  # t1054: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1054 = prims.linear(t1053, t101, None)  # t1054: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t1058, t1065, t1073] = nvFusion43(t1054, t1069, t986)
    # t1056 = prims.convert_element_type(t986, dtypes.float32)  # t1056: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1055 = prims.convert_element_type(t1054, dtypes.float32)  # t1055: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1057 = prims.add(t1055, t1056)  # t1057: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1058 = prims.convert_element_type(t1057, dtypes.bfloat16)  # t1058: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1060 = prims.mul(t1057, t1057)  # t1060: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1061 = prims.sum(t1060, (2,))  # t1061: &#34;cuda:0 f32[1, 512]&#34;
    # t1062 = prims.broadcast_in_dim(t1061, [1, 512, 1], [0, 1])  # t1062: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1063 = prims.div(t1062, 4096.0)  # t1063: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1064 = prims.add(t1063, 1e-05)  # t1064: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1065 = prims.rsqrt(t1064)  # t1065: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1066 = prims.broadcast_in_dim(t1065, (1, 512, 4096), (0, 1, 2))  # t1066: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1067 = prims.mul(t1057, t1066)  # t1067: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1071 = prims.convert_element_type(t1069, dtypes.float32)  # t1071: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1072 = prims.mul(t1067, t1071)  # t1072: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1073 = prims.convert_element_type(t1072, dtypes.bfloat16)  # t1073: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t1074 = torch.nn.functional.linear(t1073, t27, None)  # t1074: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t1074 = ltorch.linear(t1073, t27, None)  # t1074: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t1074 = prims.linear(t1073, t27, None)  # t1074: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t1075 = torch.nn.functional.linear(t1073, t43, None)  # t1075: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t1075 = ltorch.linear(t1073, t43, None)  # t1075: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t1075 = prims.linear(t1073, t43, None)  # t1075: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  [t1089] = nvFusion44(t1074, t1075)
    # t1076 = prims.convert_element_type(t1074, dtypes.float32)  # t1076: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1077 = prims.neg(t1076)  # t1077: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1078 = prims.exp(t1077)  # t1078: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1079 = prims.add(1.0, t1078)  # t1079: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1080 = prims.reciprocal(t1079)  # t1080: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1084 = prims.mul(t1076, t1080)  # t1084: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1087 = prims.convert_element_type(t1075, dtypes.float32)  # t1087: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1088 = prims.mul(t1084, t1087)  # t1088: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1089 = prims.convert_element_type(t1088, dtypes.bfloat16)  # t1089: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t1090 = torch.nn.functional.linear(t1089, t102, None)  # t1090: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1090 = ltorch.linear(t1089, t102, None)  # t1090: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1090 = prims.linear(t1089, t102, None)  # t1090: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t1094, t1101, t1109] = nvFusion45(t1058, t1090, t1105)
    # t1092 = prims.convert_element_type(t1058, dtypes.float32)  # t1092: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1091 = prims.convert_element_type(t1090, dtypes.float32)  # t1091: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1093 = prims.add(t1091, t1092)  # t1093: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1094 = prims.convert_element_type(t1093, dtypes.bfloat16)  # t1094: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1096 = prims.mul(t1093, t1093)  # t1096: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1097 = prims.sum(t1096, (2,))  # t1097: &#34;cuda:0 f32[1, 512]&#34;
    # t1098 = prims.broadcast_in_dim(t1097, [1, 512, 1], [0, 1])  # t1098: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1099 = prims.div(t1098, 4096.0)  # t1099: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1100 = prims.add(t1099, 1e-05)  # t1100: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1101 = prims.rsqrt(t1100)  # t1101: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1102 = prims.broadcast_in_dim(t1101, (1, 512, 4096), (0, 1, 2))  # t1102: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1103 = prims.mul(t1093, t1102)  # t1103: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1107 = prims.convert_element_type(t1105, dtypes.float32)  # t1107: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1108 = prims.mul(t1103, t1107)  # t1108: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1109 = prims.convert_element_type(t1108, dtypes.bfloat16)  # t1109: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t1110 = torch.nn.functional.linear(t1109, t12, None)  # t1110: &#34;cuda:0 bf16[1, 512, 12288]&#34;
    # t1110 = ltorch.linear(t1109, t12, None)  # t1110: &#34;cuda:0 bf16[1, 512, 12288]&#34;
      # t1110 = prims.linear(t1109, t12, None)  # t1110: &#34;cuda:0 bf16[1, 512, 12288]&#34;
  t1111 = torch.reshape(t1110, (1, 512, 32, 3, 128))  # t1111: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
    # t1111 = ltorch.reshape(t1110, (1, 512, 32, 3, 128))  # t1111: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
      # t1111 = prims.reshape(t1110, (1, 512, 32, 3, 128))  # t1111: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
  del t1110
  t1112 = torch.permute(t1111, (0, 2, 3, 1, 4))  # t1112: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
    # t1112 = ltorch.permute(t1111, (0, 2, 3, 1, 4))  # t1112: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
      # t1112 = prims.transpose(t1111, (0, 2, 3, 1, 4))  # t1112: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
  del t1111
  (t1113, t1114, t1115) = torch.split(t1112, (1, 1, 1), 2)
    # (t1113, t1114, t1115) = ltorch.split(t1112, (1, 1, 1), 2)
      # t1113 = prims.slice_prim(t1112, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1113: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t1114 = prims.slice_prim(t1112, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1114: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t1115 = prims.slice_prim(t1112, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1115: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
  del t1112
  t1116 = torch.reshape(t1113, (1, 32, 512, 128))  # t1116: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1116 = ltorch.reshape(t1113, (1, 32, 512, 128))  # t1116: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1116 = prims.reshape(t1113, (1, 32, 512, 128))  # t1116: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1113
  t1117 = torch.reshape(t1114, (1, 32, 512, 128))  # t1117: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1117 = ltorch.reshape(t1114, (1, 32, 512, 128))  # t1117: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1117 = prims.reshape(t1114, (1, 32, 512, 128))  # t1117: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1114
  t1118 = torch.reshape(t1115, (1, 32, 512, 128))  # t1118: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1118 = ltorch.reshape(t1115, (1, 32, 512, 128))  # t1118: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1118 = prims.reshape(t1115, (1, 32, 512, 128))  # t1118: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1115
  t1119 = torch_slice_prim_impl(t1116, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1119: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t1134 = torch_slice_prim_impl(t1117, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1134: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t1149 = torch_slice_prim_impl(t1116, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1149: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t1116
  t1151 = torch_slice_prim_impl(t1117, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1151: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t1117
  t1120 = torch_slice_prim_impl(t1119, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1120: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1121 = torch_slice_prim_impl(t1119, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1121: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1136 = torch_slice_prim_impl(t1134, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1136: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1135 = torch_slice_prim_impl(t1134, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1135: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  [t1124, t1139] = nvFusion46(t1119, t1121, t1134, t1136)
    # t1122 = prims.convert_element_type(t1121, dtypes.float32)  # t1122: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1123 = prims.neg(t1122)  # t1123: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1124 = prims.convert_element_type(t1123, dtypes.bfloat16)  # t1124: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
    # t1137 = prims.convert_element_type(t1136, dtypes.float32)  # t1137: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1138 = prims.neg(t1137)  # t1138: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1139 = prims.convert_element_type(t1138, dtypes.bfloat16)  # t1139: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  del t1121, t1136
  t1125 = torch.cat((t1124, t1120), -1)  # t1125: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1125 = ltorch.cat((t1124, t1120), -1)  # t1125: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1125 = prims.cat((t1124, t1120), -1)  # t1125: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1124, t1120
  t1140 = torch.cat((t1139, t1135), -1)  # t1140: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1140 = ltorch.cat((t1139, t1135), -1)  # t1140: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1140 = prims.cat((t1139, t1135), -1)  # t1140: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1139, t1135
  [t1133, t1148] = nvFusion47(t1119, t1125, t1134, t1140, t154, t157)
    # t1127 = prims.convert_element_type(t1119, dtypes.float32)  # t1127: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1142 = prims.convert_element_type(t1134, dtypes.float32)  # t1142: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1128 = prims.mul(t1127, t154)  # t1128: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1130 = prims.convert_element_type(t1125, dtypes.float32)  # t1130: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1131 = prims.mul(t1130, t157)  # t1131: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1132 = prims.add(t1128, t1131)  # t1132: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1133 = prims.convert_element_type(t1132, dtypes.bfloat16)  # t1133: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1143 = prims.mul(t1142, t154)  # t1143: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1145 = prims.convert_element_type(t1140, dtypes.float32)  # t1145: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1146 = prims.mul(t1145, t157)  # t1146: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1147 = prims.add(t1143, t1146)  # t1147: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1148 = prims.convert_element_type(t1147, dtypes.bfloat16)  # t1148: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1119, t1125, t1134, t1140
  t1152 = torch.cat((t1148, t1151), -1)  # t1152: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1152 = ltorch.cat((t1148, t1151), -1)  # t1152: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1152 = prims.cat((t1148, t1151), -1)  # t1152: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1148, t1151
  t1150 = torch.cat((t1133, t1149), -1)  # t1150: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1150 = ltorch.cat((t1133, t1149), -1)  # t1150: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1150 = prims.cat((t1133, t1149), -1)  # t1150: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1133, t1149
  (t1153, t1154, t1155, t1156, _, _, t1157, t1158, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(t1150, t1152, t1118, 0.0, True, scale=0.08838834764831843)
  t1160 = torch.permute(t1153, (0, 2, 1, 3))  # t1160: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
    # t1160 = ltorch.permute(t1153, (0, 2, 1, 3))  # t1160: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
      # t1160 = prims.transpose(t1153, (0, 2, 1, 3))  # t1160: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
  t1161 = torch.reshape(t1160, (1, 512, 4096))  # t1161: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1161 = ltorch.reshape(t1160, (1, 512, 4096))  # t1161: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1161 = prims.reshape(t1160, (1, 512, 4096))  # t1161: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t1160
  t1162 = torch.nn.functional.linear(t1161, t103, None)  # t1162: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1162 = ltorch.linear(t1161, t103, None)  # t1162: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1162 = prims.linear(t1161, t103, None)  # t1162: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t1166, t1173, t1181] = nvFusion48(t1094, t1162, t1177)
    # t1164 = prims.convert_element_type(t1094, dtypes.float32)  # t1164: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1163 = prims.convert_element_type(t1162, dtypes.float32)  # t1163: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1165 = prims.add(t1163, t1164)  # t1165: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1166 = prims.convert_element_type(t1165, dtypes.bfloat16)  # t1166: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1168 = prims.mul(t1165, t1165)  # t1168: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1169 = prims.sum(t1168, (2,))  # t1169: &#34;cuda:0 f32[1, 512]&#34;
    # t1170 = prims.broadcast_in_dim(t1169, [1, 512, 1], [0, 1])  # t1170: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1171 = prims.div(t1170, 4096.0)  # t1171: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1172 = prims.add(t1171, 1e-05)  # t1172: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1173 = prims.rsqrt(t1172)  # t1173: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1174 = prims.broadcast_in_dim(t1173, (1, 512, 4096), (0, 1, 2))  # t1174: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1175 = prims.mul(t1165, t1174)  # t1175: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1179 = prims.convert_element_type(t1177, dtypes.float32)  # t1179: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1180 = prims.mul(t1175, t1179)  # t1180: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1181 = prims.convert_element_type(t1180, dtypes.bfloat16)  # t1181: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t1182 = torch.nn.functional.linear(t1181, t28, None)  # t1182: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t1182 = ltorch.linear(t1181, t28, None)  # t1182: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t1182 = prims.linear(t1181, t28, None)  # t1182: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t1183 = torch.nn.functional.linear(t1181, t44, None)  # t1183: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t1183 = ltorch.linear(t1181, t44, None)  # t1183: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t1183 = prims.linear(t1181, t44, None)  # t1183: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  [t1197] = nvFusion49(t1182, t1183)
    # t1184 = prims.convert_element_type(t1182, dtypes.float32)  # t1184: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1185 = prims.neg(t1184)  # t1185: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1186 = prims.exp(t1185)  # t1186: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1187 = prims.add(1.0, t1186)  # t1187: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1188 = prims.reciprocal(t1187)  # t1188: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1192 = prims.mul(t1184, t1188)  # t1192: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1195 = prims.convert_element_type(t1183, dtypes.float32)  # t1195: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1196 = prims.mul(t1192, t1195)  # t1196: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1197 = prims.convert_element_type(t1196, dtypes.bfloat16)  # t1197: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t1198 = torch.nn.functional.linear(t1197, t104, None)  # t1198: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1198 = ltorch.linear(t1197, t104, None)  # t1198: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1198 = prims.linear(t1197, t104, None)  # t1198: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t1202, t1209, t1217] = nvFusion50(t1166, t1198, t1213)
    # t1200 = prims.convert_element_type(t1166, dtypes.float32)  # t1200: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1199 = prims.convert_element_type(t1198, dtypes.float32)  # t1199: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1201 = prims.add(t1199, t1200)  # t1201: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1202 = prims.convert_element_type(t1201, dtypes.bfloat16)  # t1202: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1204 = prims.mul(t1201, t1201)  # t1204: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1205 = prims.sum(t1204, (2,))  # t1205: &#34;cuda:0 f32[1, 512]&#34;
    # t1206 = prims.broadcast_in_dim(t1205, [1, 512, 1], [0, 1])  # t1206: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1207 = prims.div(t1206, 4096.0)  # t1207: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1208 = prims.add(t1207, 1e-05)  # t1208: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1209 = prims.rsqrt(t1208)  # t1209: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1210 = prims.broadcast_in_dim(t1209, (1, 512, 4096), (0, 1, 2))  # t1210: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1211 = prims.mul(t1201, t1210)  # t1211: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1215 = prims.convert_element_type(t1213, dtypes.float32)  # t1215: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1216 = prims.mul(t1211, t1215)  # t1216: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1217 = prims.convert_element_type(t1216, dtypes.bfloat16)  # t1217: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t1218 = torch.nn.functional.linear(t1217, t13, None)  # t1218: &#34;cuda:0 bf16[1, 512, 12288]&#34;
    # t1218 = ltorch.linear(t1217, t13, None)  # t1218: &#34;cuda:0 bf16[1, 512, 12288]&#34;
      # t1218 = prims.linear(t1217, t13, None)  # t1218: &#34;cuda:0 bf16[1, 512, 12288]&#34;
  t1219 = torch.reshape(t1218, (1, 512, 32, 3, 128))  # t1219: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
    # t1219 = ltorch.reshape(t1218, (1, 512, 32, 3, 128))  # t1219: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
      # t1219 = prims.reshape(t1218, (1, 512, 32, 3, 128))  # t1219: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
  del t1218
  t1220 = torch.permute(t1219, (0, 2, 3, 1, 4))  # t1220: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
    # t1220 = ltorch.permute(t1219, (0, 2, 3, 1, 4))  # t1220: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
      # t1220 = prims.transpose(t1219, (0, 2, 3, 1, 4))  # t1220: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
  del t1219
  (t1221, t1222, t1223) = torch.split(t1220, (1, 1, 1), 2)
    # (t1221, t1222, t1223) = ltorch.split(t1220, (1, 1, 1), 2)
      # t1221 = prims.slice_prim(t1220, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1221: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t1222 = prims.slice_prim(t1220, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1222: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t1223 = prims.slice_prim(t1220, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1223: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
  del t1220
  t1224 = torch.reshape(t1221, (1, 32, 512, 128))  # t1224: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1224 = ltorch.reshape(t1221, (1, 32, 512, 128))  # t1224: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1224 = prims.reshape(t1221, (1, 32, 512, 128))  # t1224: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1221
  t1225 = torch.reshape(t1222, (1, 32, 512, 128))  # t1225: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1225 = ltorch.reshape(t1222, (1, 32, 512, 128))  # t1225: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1225 = prims.reshape(t1222, (1, 32, 512, 128))  # t1225: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1222
  t1226 = torch.reshape(t1223, (1, 32, 512, 128))  # t1226: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1226 = ltorch.reshape(t1223, (1, 32, 512, 128))  # t1226: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1226 = prims.reshape(t1223, (1, 32, 512, 128))  # t1226: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1223
  t1227 = torch_slice_prim_impl(t1224, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1227: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t1242 = torch_slice_prim_impl(t1225, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1242: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t1257 = torch_slice_prim_impl(t1224, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1257: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t1224
  t1259 = torch_slice_prim_impl(t1225, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1259: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t1225
  t1228 = torch_slice_prim_impl(t1227, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1228: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1229 = torch_slice_prim_impl(t1227, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1229: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1243 = torch_slice_prim_impl(t1242, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1243: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1244 = torch_slice_prim_impl(t1242, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1244: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  [t1232, t1247] = nvFusion51(t1227, t1229, t1242, t1244)
    # t1230 = prims.convert_element_type(t1229, dtypes.float32)  # t1230: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1231 = prims.neg(t1230)  # t1231: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1232 = prims.convert_element_type(t1231, dtypes.bfloat16)  # t1232: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
    # t1245 = prims.convert_element_type(t1244, dtypes.float32)  # t1245: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1246 = prims.neg(t1245)  # t1246: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1247 = prims.convert_element_type(t1246, dtypes.bfloat16)  # t1247: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  del t1229, t1244
  t1233 = torch.cat((t1232, t1228), -1)  # t1233: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1233 = ltorch.cat((t1232, t1228), -1)  # t1233: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1233 = prims.cat((t1232, t1228), -1)  # t1233: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1232, t1228
  t1248 = torch.cat((t1247, t1243), -1)  # t1248: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1248 = ltorch.cat((t1247, t1243), -1)  # t1248: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1248 = prims.cat((t1247, t1243), -1)  # t1248: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1247, t1243
  [t1241, t1256] = nvFusion52(t1227, t1233, t1242, t1248, t154, t157)
    # t1235 = prims.convert_element_type(t1227, dtypes.float32)  # t1235: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1250 = prims.convert_element_type(t1242, dtypes.float32)  # t1250: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1236 = prims.mul(t1235, t154)  # t1236: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1238 = prims.convert_element_type(t1233, dtypes.float32)  # t1238: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1239 = prims.mul(t1238, t157)  # t1239: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1240 = prims.add(t1236, t1239)  # t1240: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1241 = prims.convert_element_type(t1240, dtypes.bfloat16)  # t1241: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1251 = prims.mul(t1250, t154)  # t1251: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1253 = prims.convert_element_type(t1248, dtypes.float32)  # t1253: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1254 = prims.mul(t1253, t157)  # t1254: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1255 = prims.add(t1251, t1254)  # t1255: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1256 = prims.convert_element_type(t1255, dtypes.bfloat16)  # t1256: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1227, t1233, t1242, t1248
  t1258 = torch.cat((t1241, t1257), -1)  # t1258: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1258 = ltorch.cat((t1241, t1257), -1)  # t1258: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1258 = prims.cat((t1241, t1257), -1)  # t1258: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1241, t1257
  t1260 = torch.cat((t1256, t1259), -1)  # t1260: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1260 = ltorch.cat((t1256, t1259), -1)  # t1260: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1260 = prims.cat((t1256, t1259), -1)  # t1260: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1256, t1259
  (t1261, t1262, t1263, t1264, _, _, t1265, t1266, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(t1258, t1260, t1226, 0.0, True, scale=0.08838834764831843)
  t1268 = torch.permute(t1261, (0, 2, 1, 3))  # t1268: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
    # t1268 = ltorch.permute(t1261, (0, 2, 1, 3))  # t1268: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
      # t1268 = prims.transpose(t1261, (0, 2, 1, 3))  # t1268: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
  t1269 = torch.reshape(t1268, (1, 512, 4096))  # t1269: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1269 = ltorch.reshape(t1268, (1, 512, 4096))  # t1269: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1269 = prims.reshape(t1268, (1, 512, 4096))  # t1269: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t1268
  t1270 = torch.nn.functional.linear(t1269, t105, None)  # t1270: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1270 = ltorch.linear(t1269, t105, None)  # t1270: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1270 = prims.linear(t1269, t105, None)  # t1270: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t1274, t1281, t1289] = nvFusion53(t1202, t1270, t1285)
    # t1272 = prims.convert_element_type(t1202, dtypes.float32)  # t1272: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1271 = prims.convert_element_type(t1270, dtypes.float32)  # t1271: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1273 = prims.add(t1271, t1272)  # t1273: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1274 = prims.convert_element_type(t1273, dtypes.bfloat16)  # t1274: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1276 = prims.mul(t1273, t1273)  # t1276: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1277 = prims.sum(t1276, (2,))  # t1277: &#34;cuda:0 f32[1, 512]&#34;
    # t1278 = prims.broadcast_in_dim(t1277, [1, 512, 1], [0, 1])  # t1278: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1279 = prims.div(t1278, 4096.0)  # t1279: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1280 = prims.add(t1279, 1e-05)  # t1280: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1281 = prims.rsqrt(t1280)  # t1281: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1282 = prims.broadcast_in_dim(t1281, (1, 512, 4096), (0, 1, 2))  # t1282: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1283 = prims.mul(t1273, t1282)  # t1283: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1287 = prims.convert_element_type(t1285, dtypes.float32)  # t1287: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1288 = prims.mul(t1283, t1287)  # t1288: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1289 = prims.convert_element_type(t1288, dtypes.bfloat16)  # t1289: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t1290 = torch.nn.functional.linear(t1289, t29, None)  # t1290: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t1290 = ltorch.linear(t1289, t29, None)  # t1290: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t1290 = prims.linear(t1289, t29, None)  # t1290: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t1291 = torch.nn.functional.linear(t1289, t45, None)  # t1291: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t1291 = ltorch.linear(t1289, t45, None)  # t1291: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t1291 = prims.linear(t1289, t45, None)  # t1291: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  [t1305] = nvFusion54(t1290, t1291)
    # t1292 = prims.convert_element_type(t1290, dtypes.float32)  # t1292: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1293 = prims.neg(t1292)  # t1293: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1294 = prims.exp(t1293)  # t1294: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1295 = prims.add(1.0, t1294)  # t1295: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1296 = prims.reciprocal(t1295)  # t1296: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1300 = prims.mul(t1292, t1296)  # t1300: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1303 = prims.convert_element_type(t1291, dtypes.float32)  # t1303: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1304 = prims.mul(t1300, t1303)  # t1304: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1305 = prims.convert_element_type(t1304, dtypes.bfloat16)  # t1305: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t1306 = torch.nn.functional.linear(t1305, t106, None)  # t1306: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1306 = ltorch.linear(t1305, t106, None)  # t1306: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1306 = prims.linear(t1305, t106, None)  # t1306: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t1310, t1317, t1325] = nvFusion55(t1274, t1306, t1321)
    # t1308 = prims.convert_element_type(t1274, dtypes.float32)  # t1308: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1307 = prims.convert_element_type(t1306, dtypes.float32)  # t1307: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1309 = prims.add(t1307, t1308)  # t1309: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1310 = prims.convert_element_type(t1309, dtypes.bfloat16)  # t1310: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1312 = prims.mul(t1309, t1309)  # t1312: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1313 = prims.sum(t1312, (2,))  # t1313: &#34;cuda:0 f32[1, 512]&#34;
    # t1314 = prims.broadcast_in_dim(t1313, [1, 512, 1], [0, 1])  # t1314: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1315 = prims.div(t1314, 4096.0)  # t1315: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1316 = prims.add(t1315, 1e-05)  # t1316: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1317 = prims.rsqrt(t1316)  # t1317: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1318 = prims.broadcast_in_dim(t1317, (1, 512, 4096), (0, 1, 2))  # t1318: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1319 = prims.mul(t1309, t1318)  # t1319: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1323 = prims.convert_element_type(t1321, dtypes.float32)  # t1323: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1324 = prims.mul(t1319, t1323)  # t1324: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1325 = prims.convert_element_type(t1324, dtypes.bfloat16)  # t1325: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t1326 = torch.nn.functional.linear(t1325, t14, None)  # t1326: &#34;cuda:0 bf16[1, 512, 12288]&#34;
    # t1326 = ltorch.linear(t1325, t14, None)  # t1326: &#34;cuda:0 bf16[1, 512, 12288]&#34;
      # t1326 = prims.linear(t1325, t14, None)  # t1326: &#34;cuda:0 bf16[1, 512, 12288]&#34;
  t1327 = torch.reshape(t1326, (1, 512, 32, 3, 128))  # t1327: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
    # t1327 = ltorch.reshape(t1326, (1, 512, 32, 3, 128))  # t1327: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
      # t1327 = prims.reshape(t1326, (1, 512, 32, 3, 128))  # t1327: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
  del t1326
  t1328 = torch.permute(t1327, (0, 2, 3, 1, 4))  # t1328: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
    # t1328 = ltorch.permute(t1327, (0, 2, 3, 1, 4))  # t1328: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
      # t1328 = prims.transpose(t1327, (0, 2, 3, 1, 4))  # t1328: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
  del t1327
  (t1329, t1330, t1331) = torch.split(t1328, (1, 1, 1), 2)
    # (t1329, t1330, t1331) = ltorch.split(t1328, (1, 1, 1), 2)
      # t1329 = prims.slice_prim(t1328, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1329: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t1330 = prims.slice_prim(t1328, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1330: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t1331 = prims.slice_prim(t1328, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1331: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
  del t1328
  t1332 = torch.reshape(t1329, (1, 32, 512, 128))  # t1332: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1332 = ltorch.reshape(t1329, (1, 32, 512, 128))  # t1332: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1332 = prims.reshape(t1329, (1, 32, 512, 128))  # t1332: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1329
  t1333 = torch.reshape(t1330, (1, 32, 512, 128))  # t1333: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1333 = ltorch.reshape(t1330, (1, 32, 512, 128))  # t1333: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1333 = prims.reshape(t1330, (1, 32, 512, 128))  # t1333: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1330
  t1334 = torch.reshape(t1331, (1, 32, 512, 128))  # t1334: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1334 = ltorch.reshape(t1331, (1, 32, 512, 128))  # t1334: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1334 = prims.reshape(t1331, (1, 32, 512, 128))  # t1334: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1331
  t1335 = torch_slice_prim_impl(t1332, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1335: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t1350 = torch_slice_prim_impl(t1333, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1350: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t1365 = torch_slice_prim_impl(t1332, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1365: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t1332
  t1367 = torch_slice_prim_impl(t1333, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1367: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t1333
  t1336 = torch_slice_prim_impl(t1335, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1336: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1337 = torch_slice_prim_impl(t1335, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1337: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1351 = torch_slice_prim_impl(t1350, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1351: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1352 = torch_slice_prim_impl(t1350, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1352: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  [t1340, t1355] = nvFusion56(t1335, t1337, t1350, t1352)
    # t1338 = prims.convert_element_type(t1337, dtypes.float32)  # t1338: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1339 = prims.neg(t1338)  # t1339: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1340 = prims.convert_element_type(t1339, dtypes.bfloat16)  # t1340: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
    # t1353 = prims.convert_element_type(t1352, dtypes.float32)  # t1353: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1354 = prims.neg(t1353)  # t1354: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1355 = prims.convert_element_type(t1354, dtypes.bfloat16)  # t1355: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  del t1337, t1352
  t1341 = torch.cat((t1340, t1336), -1)  # t1341: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1341 = ltorch.cat((t1340, t1336), -1)  # t1341: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1341 = prims.cat((t1340, t1336), -1)  # t1341: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1340, t1336
  t1356 = torch.cat((t1355, t1351), -1)  # t1356: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1356 = ltorch.cat((t1355, t1351), -1)  # t1356: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1356 = prims.cat((t1355, t1351), -1)  # t1356: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1355, t1351
  [t1349, t1364] = nvFusion57(t1335, t1341, t1350, t1356, t154, t157)
    # t1343 = prims.convert_element_type(t1335, dtypes.float32)  # t1343: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1358 = prims.convert_element_type(t1350, dtypes.float32)  # t1358: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1344 = prims.mul(t1343, t154)  # t1344: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1346 = prims.convert_element_type(t1341, dtypes.float32)  # t1346: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1347 = prims.mul(t1346, t157)  # t1347: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1348 = prims.add(t1344, t1347)  # t1348: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1349 = prims.convert_element_type(t1348, dtypes.bfloat16)  # t1349: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1359 = prims.mul(t1358, t154)  # t1359: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1361 = prims.convert_element_type(t1356, dtypes.float32)  # t1361: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1362 = prims.mul(t1361, t157)  # t1362: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1363 = prims.add(t1359, t1362)  # t1363: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1364 = prims.convert_element_type(t1363, dtypes.bfloat16)  # t1364: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1335, t1341, t1350, t1356
  t1366 = torch.cat((t1349, t1365), -1)  # t1366: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1366 = ltorch.cat((t1349, t1365), -1)  # t1366: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1366 = prims.cat((t1349, t1365), -1)  # t1366: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1349, t1365
  t1368 = torch.cat((t1364, t1367), -1)  # t1368: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1368 = ltorch.cat((t1364, t1367), -1)  # t1368: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1368 = prims.cat((t1364, t1367), -1)  # t1368: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1364, t1367
  (t1369, t1370, t1371, t1372, _, _, t1373, t1374, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(t1366, t1368, t1334, 0.0, True, scale=0.08838834764831843)
  t1376 = torch.permute(t1369, (0, 2, 1, 3))  # t1376: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
    # t1376 = ltorch.permute(t1369, (0, 2, 1, 3))  # t1376: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
      # t1376 = prims.transpose(t1369, (0, 2, 1, 3))  # t1376: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
  t1377 = torch.reshape(t1376, (1, 512, 4096))  # t1377: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1377 = ltorch.reshape(t1376, (1, 512, 4096))  # t1377: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1377 = prims.reshape(t1376, (1, 512, 4096))  # t1377: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t1376
  t1378 = torch.nn.functional.linear(t1377, t107, None)  # t1378: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1378 = ltorch.linear(t1377, t107, None)  # t1378: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1378 = prims.linear(t1377, t107, None)  # t1378: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t1382, t1389, t1397] = nvFusion58(t1310, t1378, t1393)
    # t1380 = prims.convert_element_type(t1310, dtypes.float32)  # t1380: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1379 = prims.convert_element_type(t1378, dtypes.float32)  # t1379: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1381 = prims.add(t1379, t1380)  # t1381: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1382 = prims.convert_element_type(t1381, dtypes.bfloat16)  # t1382: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1384 = prims.mul(t1381, t1381)  # t1384: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1385 = prims.sum(t1384, (2,))  # t1385: &#34;cuda:0 f32[1, 512]&#34;
    # t1386 = prims.broadcast_in_dim(t1385, [1, 512, 1], [0, 1])  # t1386: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1387 = prims.div(t1386, 4096.0)  # t1387: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1388 = prims.add(t1387, 1e-05)  # t1388: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1389 = prims.rsqrt(t1388)  # t1389: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1390 = prims.broadcast_in_dim(t1389, (1, 512, 4096), (0, 1, 2))  # t1390: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1391 = prims.mul(t1381, t1390)  # t1391: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1395 = prims.convert_element_type(t1393, dtypes.float32)  # t1395: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1396 = prims.mul(t1391, t1395)  # t1396: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1397 = prims.convert_element_type(t1396, dtypes.bfloat16)  # t1397: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t1398 = torch.nn.functional.linear(t1397, t30, None)  # t1398: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t1398 = ltorch.linear(t1397, t30, None)  # t1398: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t1398 = prims.linear(t1397, t30, None)  # t1398: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t1399 = torch.nn.functional.linear(t1397, t46, None)  # t1399: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t1399 = ltorch.linear(t1397, t46, None)  # t1399: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t1399 = prims.linear(t1397, t46, None)  # t1399: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  [t1413] = nvFusion59(t1398, t1399)
    # t1400 = prims.convert_element_type(t1398, dtypes.float32)  # t1400: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1401 = prims.neg(t1400)  # t1401: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1402 = prims.exp(t1401)  # t1402: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1403 = prims.add(1.0, t1402)  # t1403: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1404 = prims.reciprocal(t1403)  # t1404: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1408 = prims.mul(t1400, t1404)  # t1408: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1411 = prims.convert_element_type(t1399, dtypes.float32)  # t1411: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1412 = prims.mul(t1408, t1411)  # t1412: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1413 = prims.convert_element_type(t1412, dtypes.bfloat16)  # t1413: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t1414 = torch.nn.functional.linear(t1413, t108, None)  # t1414: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1414 = ltorch.linear(t1413, t108, None)  # t1414: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1414 = prims.linear(t1413, t108, None)  # t1414: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t1418, t1425, t1433] = nvFusion60(t1382, t1414, t1429)
    # t1416 = prims.convert_element_type(t1382, dtypes.float32)  # t1416: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1415 = prims.convert_element_type(t1414, dtypes.float32)  # t1415: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1417 = prims.add(t1415, t1416)  # t1417: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1418 = prims.convert_element_type(t1417, dtypes.bfloat16)  # t1418: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1420 = prims.mul(t1417, t1417)  # t1420: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1421 = prims.sum(t1420, (2,))  # t1421: &#34;cuda:0 f32[1, 512]&#34;
    # t1422 = prims.broadcast_in_dim(t1421, [1, 512, 1], [0, 1])  # t1422: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1423 = prims.div(t1422, 4096.0)  # t1423: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1424 = prims.add(t1423, 1e-05)  # t1424: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1425 = prims.rsqrt(t1424)  # t1425: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1426 = prims.broadcast_in_dim(t1425, (1, 512, 4096), (0, 1, 2))  # t1426: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1427 = prims.mul(t1417, t1426)  # t1427: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1431 = prims.convert_element_type(t1429, dtypes.float32)  # t1431: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1432 = prims.mul(t1427, t1431)  # t1432: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1433 = prims.convert_element_type(t1432, dtypes.bfloat16)  # t1433: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t1434 = torch.nn.functional.linear(t1433, t15, None)  # t1434: &#34;cuda:0 bf16[1, 512, 12288]&#34;
    # t1434 = ltorch.linear(t1433, t15, None)  # t1434: &#34;cuda:0 bf16[1, 512, 12288]&#34;
      # t1434 = prims.linear(t1433, t15, None)  # t1434: &#34;cuda:0 bf16[1, 512, 12288]&#34;
  t1435 = torch.reshape(t1434, (1, 512, 32, 3, 128))  # t1435: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
    # t1435 = ltorch.reshape(t1434, (1, 512, 32, 3, 128))  # t1435: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
      # t1435 = prims.reshape(t1434, (1, 512, 32, 3, 128))  # t1435: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
  del t1434
  t1436 = torch.permute(t1435, (0, 2, 3, 1, 4))  # t1436: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
    # t1436 = ltorch.permute(t1435, (0, 2, 3, 1, 4))  # t1436: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
      # t1436 = prims.transpose(t1435, (0, 2, 3, 1, 4))  # t1436: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
  del t1435
  (t1437, t1438, t1439) = torch.split(t1436, (1, 1, 1), 2)
    # (t1437, t1438, t1439) = ltorch.split(t1436, (1, 1, 1), 2)
      # t1437 = prims.slice_prim(t1436, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1437: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t1438 = prims.slice_prim(t1436, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1438: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t1439 = prims.slice_prim(t1436, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1439: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
  del t1436
  t1440 = torch.reshape(t1437, (1, 32, 512, 128))  # t1440: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1440 = ltorch.reshape(t1437, (1, 32, 512, 128))  # t1440: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1440 = prims.reshape(t1437, (1, 32, 512, 128))  # t1440: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1437
  t1441 = torch.reshape(t1438, (1, 32, 512, 128))  # t1441: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1441 = ltorch.reshape(t1438, (1, 32, 512, 128))  # t1441: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1441 = prims.reshape(t1438, (1, 32, 512, 128))  # t1441: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1438
  t1442 = torch.reshape(t1439, (1, 32, 512, 128))  # t1442: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1442 = ltorch.reshape(t1439, (1, 32, 512, 128))  # t1442: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1442 = prims.reshape(t1439, (1, 32, 512, 128))  # t1442: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1439
  t1443 = torch_slice_prim_impl(t1440, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1443: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t1458 = torch_slice_prim_impl(t1441, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1458: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t1473 = torch_slice_prim_impl(t1440, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1473: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t1440
  t1475 = torch_slice_prim_impl(t1441, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1475: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t1441
  t1444 = torch_slice_prim_impl(t1443, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1444: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1445 = torch_slice_prim_impl(t1443, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1445: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1459 = torch_slice_prim_impl(t1458, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1459: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1460 = torch_slice_prim_impl(t1458, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1460: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  [t1448, t1463] = nvFusion61(t1443, t1445, t1458, t1460)
    # t1446 = prims.convert_element_type(t1445, dtypes.float32)  # t1446: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1447 = prims.neg(t1446)  # t1447: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1448 = prims.convert_element_type(t1447, dtypes.bfloat16)  # t1448: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
    # t1461 = prims.convert_element_type(t1460, dtypes.float32)  # t1461: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1462 = prims.neg(t1461)  # t1462: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1463 = prims.convert_element_type(t1462, dtypes.bfloat16)  # t1463: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  del t1445, t1460
  t1464 = torch.cat((t1463, t1459), -1)  # t1464: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1464 = ltorch.cat((t1463, t1459), -1)  # t1464: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1464 = prims.cat((t1463, t1459), -1)  # t1464: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1463, t1459
  t1449 = torch.cat((t1448, t1444), -1)  # t1449: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1449 = ltorch.cat((t1448, t1444), -1)  # t1449: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1449 = prims.cat((t1448, t1444), -1)  # t1449: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1448, t1444
  [t1457, t1472] = nvFusion62(t1443, t1449, t1458, t1464, t154, t157)
    # t1451 = prims.convert_element_type(t1443, dtypes.float32)  # t1451: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1466 = prims.convert_element_type(t1458, dtypes.float32)  # t1466: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1467 = prims.mul(t1466, t154)  # t1467: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1469 = prims.convert_element_type(t1464, dtypes.float32)  # t1469: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1470 = prims.mul(t1469, t157)  # t1470: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1471 = prims.add(t1467, t1470)  # t1471: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1472 = prims.convert_element_type(t1471, dtypes.bfloat16)  # t1472: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1452 = prims.mul(t1451, t154)  # t1452: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1454 = prims.convert_element_type(t1449, dtypes.float32)  # t1454: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1455 = prims.mul(t1454, t157)  # t1455: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1456 = prims.add(t1452, t1455)  # t1456: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1457 = prims.convert_element_type(t1456, dtypes.bfloat16)  # t1457: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1443, t1449, t1458, t1464
  t1476 = torch.cat((t1472, t1475), -1)  # t1476: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1476 = ltorch.cat((t1472, t1475), -1)  # t1476: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1476 = prims.cat((t1472, t1475), -1)  # t1476: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1472, t1475
  t1474 = torch.cat((t1457, t1473), -1)  # t1474: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1474 = ltorch.cat((t1457, t1473), -1)  # t1474: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1474 = prims.cat((t1457, t1473), -1)  # t1474: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1457, t1473
  (t1477, t1478, t1479, t1480, _, _, t1481, t1482, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(t1474, t1476, t1442, 0.0, True, scale=0.08838834764831843)
  t1484 = torch.permute(t1477, (0, 2, 1, 3))  # t1484: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
    # t1484 = ltorch.permute(t1477, (0, 2, 1, 3))  # t1484: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
      # t1484 = prims.transpose(t1477, (0, 2, 1, 3))  # t1484: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
  t1485 = torch.reshape(t1484, (1, 512, 4096))  # t1485: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1485 = ltorch.reshape(t1484, (1, 512, 4096))  # t1485: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1485 = prims.reshape(t1484, (1, 512, 4096))  # t1485: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t1484
  t1486 = torch.nn.functional.linear(t1485, t109, None)  # t1486: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1486 = ltorch.linear(t1485, t109, None)  # t1486: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1486 = prims.linear(t1485, t109, None)  # t1486: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t1490, t1497, t1505] = nvFusion63(t1418, t1486, t1501)
    # t1488 = prims.convert_element_type(t1418, dtypes.float32)  # t1488: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1487 = prims.convert_element_type(t1486, dtypes.float32)  # t1487: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1489 = prims.add(t1487, t1488)  # t1489: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1490 = prims.convert_element_type(t1489, dtypes.bfloat16)  # t1490: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1492 = prims.mul(t1489, t1489)  # t1492: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1493 = prims.sum(t1492, (2,))  # t1493: &#34;cuda:0 f32[1, 512]&#34;
    # t1494 = prims.broadcast_in_dim(t1493, [1, 512, 1], [0, 1])  # t1494: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1495 = prims.div(t1494, 4096.0)  # t1495: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1496 = prims.add(t1495, 1e-05)  # t1496: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1497 = prims.rsqrt(t1496)  # t1497: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1498 = prims.broadcast_in_dim(t1497, (1, 512, 4096), (0, 1, 2))  # t1498: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1499 = prims.mul(t1489, t1498)  # t1499: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1503 = prims.convert_element_type(t1501, dtypes.float32)  # t1503: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1504 = prims.mul(t1499, t1503)  # t1504: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1505 = prims.convert_element_type(t1504, dtypes.bfloat16)  # t1505: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t1506 = torch.nn.functional.linear(t1505, t31, None)  # t1506: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t1506 = ltorch.linear(t1505, t31, None)  # t1506: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t1506 = prims.linear(t1505, t31, None)  # t1506: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t1507 = torch.nn.functional.linear(t1505, t47, None)  # t1507: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t1507 = ltorch.linear(t1505, t47, None)  # t1507: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t1507 = prims.linear(t1505, t47, None)  # t1507: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  [t1521] = nvFusion64(t1506, t1507)
    # t1508 = prims.convert_element_type(t1506, dtypes.float32)  # t1508: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1509 = prims.neg(t1508)  # t1509: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1510 = prims.exp(t1509)  # t1510: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1511 = prims.add(1.0, t1510)  # t1511: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1512 = prims.reciprocal(t1511)  # t1512: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1516 = prims.mul(t1508, t1512)  # t1516: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1519 = prims.convert_element_type(t1507, dtypes.float32)  # t1519: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1520 = prims.mul(t1516, t1519)  # t1520: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1521 = prims.convert_element_type(t1520, dtypes.bfloat16)  # t1521: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t1522 = torch.nn.functional.linear(t1521, t110, None)  # t1522: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1522 = ltorch.linear(t1521, t110, None)  # t1522: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1522 = prims.linear(t1521, t110, None)  # t1522: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t1526, t1533, t1541] = nvFusion65(t1490, t1522, t1537)
    # t1524 = prims.convert_element_type(t1490, dtypes.float32)  # t1524: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1523 = prims.convert_element_type(t1522, dtypes.float32)  # t1523: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1525 = prims.add(t1523, t1524)  # t1525: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1526 = prims.convert_element_type(t1525, dtypes.bfloat16)  # t1526: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1528 = prims.mul(t1525, t1525)  # t1528: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1529 = prims.sum(t1528, (2,))  # t1529: &#34;cuda:0 f32[1, 512]&#34;
    # t1530 = prims.broadcast_in_dim(t1529, [1, 512, 1], [0, 1])  # t1530: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1531 = prims.div(t1530, 4096.0)  # t1531: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1532 = prims.add(t1531, 1e-05)  # t1532: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1533 = prims.rsqrt(t1532)  # t1533: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1534 = prims.broadcast_in_dim(t1533, (1, 512, 4096), (0, 1, 2))  # t1534: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1535 = prims.mul(t1525, t1534)  # t1535: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1539 = prims.convert_element_type(t1537, dtypes.float32)  # t1539: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1540 = prims.mul(t1535, t1539)  # t1540: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1541 = prims.convert_element_type(t1540, dtypes.bfloat16)  # t1541: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t1542 = torch.nn.functional.linear(t1541, t16, None)  # t1542: &#34;cuda:0 bf16[1, 512, 12288]&#34;
    # t1542 = ltorch.linear(t1541, t16, None)  # t1542: &#34;cuda:0 bf16[1, 512, 12288]&#34;
      # t1542 = prims.linear(t1541, t16, None)  # t1542: &#34;cuda:0 bf16[1, 512, 12288]&#34;
  t1543 = torch.reshape(t1542, (1, 512, 32, 3, 128))  # t1543: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
    # t1543 = ltorch.reshape(t1542, (1, 512, 32, 3, 128))  # t1543: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
      # t1543 = prims.reshape(t1542, (1, 512, 32, 3, 128))  # t1543: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
  del t1542
  t1544 = torch.permute(t1543, (0, 2, 3, 1, 4))  # t1544: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
    # t1544 = ltorch.permute(t1543, (0, 2, 3, 1, 4))  # t1544: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
      # t1544 = prims.transpose(t1543, (0, 2, 3, 1, 4))  # t1544: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
  del t1543
  (t1545, t1546, t1547) = torch.split(t1544, (1, 1, 1), 2)
    # (t1545, t1546, t1547) = ltorch.split(t1544, (1, 1, 1), 2)
      # t1545 = prims.slice_prim(t1544, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1545: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t1546 = prims.slice_prim(t1544, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1546: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t1547 = prims.slice_prim(t1544, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1547: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
  del t1544
  t1548 = torch.reshape(t1545, (1, 32, 512, 128))  # t1548: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1548 = ltorch.reshape(t1545, (1, 32, 512, 128))  # t1548: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1548 = prims.reshape(t1545, (1, 32, 512, 128))  # t1548: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1545
  t1549 = torch.reshape(t1546, (1, 32, 512, 128))  # t1549: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1549 = ltorch.reshape(t1546, (1, 32, 512, 128))  # t1549: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1549 = prims.reshape(t1546, (1, 32, 512, 128))  # t1549: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1546
  t1550 = torch.reshape(t1547, (1, 32, 512, 128))  # t1550: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1550 = ltorch.reshape(t1547, (1, 32, 512, 128))  # t1550: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1550 = prims.reshape(t1547, (1, 32, 512, 128))  # t1550: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1547
  t1551 = torch_slice_prim_impl(t1548, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1551: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t1566 = torch_slice_prim_impl(t1549, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1566: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t1581 = torch_slice_prim_impl(t1548, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1581: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t1548
  t1583 = torch_slice_prim_impl(t1549, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1583: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t1549
  t1552 = torch_slice_prim_impl(t1551, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1552: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1553 = torch_slice_prim_impl(t1551, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1553: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1567 = torch_slice_prim_impl(t1566, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1567: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1568 = torch_slice_prim_impl(t1566, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1568: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  [t1556, t1571] = nvFusion66(t1551, t1553, t1566, t1568)
    # t1554 = prims.convert_element_type(t1553, dtypes.float32)  # t1554: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1555 = prims.neg(t1554)  # t1555: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1556 = prims.convert_element_type(t1555, dtypes.bfloat16)  # t1556: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
    # t1569 = prims.convert_element_type(t1568, dtypes.float32)  # t1569: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1570 = prims.neg(t1569)  # t1570: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1571 = prims.convert_element_type(t1570, dtypes.bfloat16)  # t1571: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  del t1553, t1568
  t1572 = torch.cat((t1571, t1567), -1)  # t1572: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1572 = ltorch.cat((t1571, t1567), -1)  # t1572: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1572 = prims.cat((t1571, t1567), -1)  # t1572: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1571, t1567
  t1557 = torch.cat((t1556, t1552), -1)  # t1557: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1557 = ltorch.cat((t1556, t1552), -1)  # t1557: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1557 = prims.cat((t1556, t1552), -1)  # t1557: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1556, t1552
  [t1565, t1580] = nvFusion67(t154, t1551, t1557, t1566, t157, t1572)
    # t1559 = prims.convert_element_type(t1551, dtypes.float32)  # t1559: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1574 = prims.convert_element_type(t1566, dtypes.float32)  # t1574: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1575 = prims.mul(t1574, t154)  # t1575: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1577 = prims.convert_element_type(t1572, dtypes.float32)  # t1577: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1578 = prims.mul(t1577, t157)  # t1578: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1579 = prims.add(t1575, t1578)  # t1579: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1580 = prims.convert_element_type(t1579, dtypes.bfloat16)  # t1580: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1560 = prims.mul(t1559, t154)  # t1560: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1562 = prims.convert_element_type(t1557, dtypes.float32)  # t1562: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1563 = prims.mul(t1562, t157)  # t1563: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1564 = prims.add(t1560, t1563)  # t1564: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1565 = prims.convert_element_type(t1564, dtypes.bfloat16)  # t1565: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1551, t1557, t1566, t1572
  t1584 = torch.cat((t1580, t1583), -1)  # t1584: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1584 = ltorch.cat((t1580, t1583), -1)  # t1584: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1584 = prims.cat((t1580, t1583), -1)  # t1584: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1580, t1583
  t1582 = torch.cat((t1565, t1581), -1)  # t1582: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1582 = ltorch.cat((t1565, t1581), -1)  # t1582: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1582 = prims.cat((t1565, t1581), -1)  # t1582: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1565, t1581
  (t1585, t1586, t1587, t1588, _, _, t1589, t1590, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(t1582, t1584, t1550, 0.0, True, scale=0.08838834764831843)
  t1592 = torch.permute(t1585, (0, 2, 1, 3))  # t1592: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
    # t1592 = ltorch.permute(t1585, (0, 2, 1, 3))  # t1592: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
      # t1592 = prims.transpose(t1585, (0, 2, 1, 3))  # t1592: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
  t1593 = torch.reshape(t1592, (1, 512, 4096))  # t1593: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1593 = ltorch.reshape(t1592, (1, 512, 4096))  # t1593: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1593 = prims.reshape(t1592, (1, 512, 4096))  # t1593: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t1592
  t1594 = torch.nn.functional.linear(t1593, t111, None)  # t1594: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1594 = ltorch.linear(t1593, t111, None)  # t1594: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1594 = prims.linear(t1593, t111, None)  # t1594: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t1598, t1605, t1613] = nvFusion68(t1526, t1594, t1609)
    # t1596 = prims.convert_element_type(t1526, dtypes.float32)  # t1596: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1595 = prims.convert_element_type(t1594, dtypes.float32)  # t1595: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1597 = prims.add(t1595, t1596)  # t1597: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1598 = prims.convert_element_type(t1597, dtypes.bfloat16)  # t1598: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1600 = prims.mul(t1597, t1597)  # t1600: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1601 = prims.sum(t1600, (2,))  # t1601: &#34;cuda:0 f32[1, 512]&#34;
    # t1602 = prims.broadcast_in_dim(t1601, [1, 512, 1], [0, 1])  # t1602: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1603 = prims.div(t1602, 4096.0)  # t1603: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1604 = prims.add(t1603, 1e-05)  # t1604: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1605 = prims.rsqrt(t1604)  # t1605: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1606 = prims.broadcast_in_dim(t1605, (1, 512, 4096), (0, 1, 2))  # t1606: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1607 = prims.mul(t1597, t1606)  # t1607: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1611 = prims.convert_element_type(t1609, dtypes.float32)  # t1611: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1612 = prims.mul(t1607, t1611)  # t1612: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1613 = prims.convert_element_type(t1612, dtypes.bfloat16)  # t1613: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t1614 = torch.nn.functional.linear(t1613, t32, None)  # t1614: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t1614 = ltorch.linear(t1613, t32, None)  # t1614: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t1614 = prims.linear(t1613, t32, None)  # t1614: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t1615 = torch.nn.functional.linear(t1613, t48, None)  # t1615: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t1615 = ltorch.linear(t1613, t48, None)  # t1615: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t1615 = prims.linear(t1613, t48, None)  # t1615: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  [t1629] = nvFusion69(t1614, t1615)
    # t1616 = prims.convert_element_type(t1614, dtypes.float32)  # t1616: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1617 = prims.neg(t1616)  # t1617: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1618 = prims.exp(t1617)  # t1618: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1619 = prims.add(1.0, t1618)  # t1619: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1620 = prims.reciprocal(t1619)  # t1620: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1624 = prims.mul(t1616, t1620)  # t1624: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1627 = prims.convert_element_type(t1615, dtypes.float32)  # t1627: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1628 = prims.mul(t1624, t1627)  # t1628: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1629 = prims.convert_element_type(t1628, dtypes.bfloat16)  # t1629: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t1630 = torch.nn.functional.linear(t1629, t112, None)  # t1630: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1630 = ltorch.linear(t1629, t112, None)  # t1630: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1630 = prims.linear(t1629, t112, None)  # t1630: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t1634, t1641, t1649] = nvFusion70(t1598, t1630, t1645)
    # t1632 = prims.convert_element_type(t1598, dtypes.float32)  # t1632: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1631 = prims.convert_element_type(t1630, dtypes.float32)  # t1631: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1633 = prims.add(t1631, t1632)  # t1633: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1634 = prims.convert_element_type(t1633, dtypes.bfloat16)  # t1634: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1636 = prims.mul(t1633, t1633)  # t1636: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1637 = prims.sum(t1636, (2,))  # t1637: &#34;cuda:0 f32[1, 512]&#34;
    # t1638 = prims.broadcast_in_dim(t1637, [1, 512, 1], [0, 1])  # t1638: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1639 = prims.div(t1638, 4096.0)  # t1639: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1640 = prims.add(t1639, 1e-05)  # t1640: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1641 = prims.rsqrt(t1640)  # t1641: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1642 = prims.broadcast_in_dim(t1641, (1, 512, 4096), (0, 1, 2))  # t1642: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1643 = prims.mul(t1633, t1642)  # t1643: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1647 = prims.convert_element_type(t1645, dtypes.float32)  # t1647: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1648 = prims.mul(t1643, t1647)  # t1648: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1649 = prims.convert_element_type(t1648, dtypes.bfloat16)  # t1649: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t1650 = torch.nn.functional.linear(t1649, t17, None)  # t1650: &#34;cuda:0 bf16[1, 512, 12288]&#34;
    # t1650 = ltorch.linear(t1649, t17, None)  # t1650: &#34;cuda:0 bf16[1, 512, 12288]&#34;
      # t1650 = prims.linear(t1649, t17, None)  # t1650: &#34;cuda:0 bf16[1, 512, 12288]&#34;
  t1651 = torch.reshape(t1650, (1, 512, 32, 3, 128))  # t1651: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
    # t1651 = ltorch.reshape(t1650, (1, 512, 32, 3, 128))  # t1651: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
      # t1651 = prims.reshape(t1650, (1, 512, 32, 3, 128))  # t1651: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
  del t1650
  t1652 = torch.permute(t1651, (0, 2, 3, 1, 4))  # t1652: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
    # t1652 = ltorch.permute(t1651, (0, 2, 3, 1, 4))  # t1652: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
      # t1652 = prims.transpose(t1651, (0, 2, 3, 1, 4))  # t1652: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
  del t1651
  (t1653, t1654, t1655) = torch.split(t1652, (1, 1, 1), 2)
    # (t1653, t1654, t1655) = ltorch.split(t1652, (1, 1, 1), 2)
      # t1653 = prims.slice_prim(t1652, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1653: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t1654 = prims.slice_prim(t1652, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1654: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t1655 = prims.slice_prim(t1652, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1655: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
  del t1652
  t1656 = torch.reshape(t1653, (1, 32, 512, 128))  # t1656: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1656 = ltorch.reshape(t1653, (1, 32, 512, 128))  # t1656: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1656 = prims.reshape(t1653, (1, 32, 512, 128))  # t1656: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1653
  t1657 = torch.reshape(t1654, (1, 32, 512, 128))  # t1657: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1657 = ltorch.reshape(t1654, (1, 32, 512, 128))  # t1657: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1657 = prims.reshape(t1654, (1, 32, 512, 128))  # t1657: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1654
  t1658 = torch.reshape(t1655, (1, 32, 512, 128))  # t1658: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1658 = ltorch.reshape(t1655, (1, 32, 512, 128))  # t1658: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1658 = prims.reshape(t1655, (1, 32, 512, 128))  # t1658: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1655
  t1689 = torch_slice_prim_impl(t1656, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1689: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  t1691 = torch_slice_prim_impl(t1657, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1691: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  t1659 = torch_slice_prim_impl(t1656, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1659: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1656
  t1674 = torch_slice_prim_impl(t1657, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1674: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1657
  t1660 = torch_slice_prim_impl(t1659, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1660: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1661 = torch_slice_prim_impl(t1659, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1661: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1675 = torch_slice_prim_impl(t1674, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1675: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1676 = torch_slice_prim_impl(t1674, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1676: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  [t1664, t1679] = nvFusion71(t1659, t1661, t1674, t1676)
    # t1662 = prims.convert_element_type(t1661, dtypes.float32)  # t1662: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1663 = prims.neg(t1662)  # t1663: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1664 = prims.convert_element_type(t1663, dtypes.bfloat16)  # t1664: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
    # t1677 = prims.convert_element_type(t1676, dtypes.float32)  # t1677: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1678 = prims.neg(t1677)  # t1678: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1679 = prims.convert_element_type(t1678, dtypes.bfloat16)  # t1679: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  del t1661, t1676
  t1680 = torch.cat((t1679, t1675), -1)  # t1680: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1680 = ltorch.cat((t1679, t1675), -1)  # t1680: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1680 = prims.cat((t1679, t1675), -1)  # t1680: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1679, t1675
  t1665 = torch.cat((t1664, t1660), -1)  # t1665: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1665 = ltorch.cat((t1664, t1660), -1)  # t1665: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1665 = prims.cat((t1664, t1660), -1)  # t1665: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1664, t1660
  [t1673, t1688] = nvFusion72(t154, t157, t1659, t1665, t1674, t1680)
    # t1667 = prims.convert_element_type(t1659, dtypes.float32)  # t1667: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1682 = prims.convert_element_type(t1674, dtypes.float32)  # t1682: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1683 = prims.mul(t1682, t154)  # t1683: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1685 = prims.convert_element_type(t1680, dtypes.float32)  # t1685: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1686 = prims.mul(t1685, t157)  # t1686: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1687 = prims.add(t1683, t1686)  # t1687: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1688 = prims.convert_element_type(t1687, dtypes.bfloat16)  # t1688: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1668 = prims.mul(t1667, t154)  # t1668: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1670 = prims.convert_element_type(t1665, dtypes.float32)  # t1670: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1671 = prims.mul(t1670, t157)  # t1671: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1672 = prims.add(t1668, t1671)  # t1672: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1673 = prims.convert_element_type(t1672, dtypes.bfloat16)  # t1673: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1659, t1665, t1674, t1680
  t1692 = torch.cat((t1688, t1691), -1)  # t1692: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1692 = ltorch.cat((t1688, t1691), -1)  # t1692: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1692 = prims.cat((t1688, t1691), -1)  # t1692: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1688, t1691
  t1690 = torch.cat((t1673, t1689), -1)  # t1690: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1690 = ltorch.cat((t1673, t1689), -1)  # t1690: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1690 = prims.cat((t1673, t1689), -1)  # t1690: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1673, t1689
  (t1693, t1694, t1695, t1696, _, _, t1697, t1698, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(t1690, t1692, t1658, 0.0, True, scale=0.08838834764831843)
  t1700 = torch.permute(t1693, (0, 2, 1, 3))  # t1700: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
    # t1700 = ltorch.permute(t1693, (0, 2, 1, 3))  # t1700: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
      # t1700 = prims.transpose(t1693, (0, 2, 1, 3))  # t1700: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
  t1701 = torch.reshape(t1700, (1, 512, 4096))  # t1701: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1701 = ltorch.reshape(t1700, (1, 512, 4096))  # t1701: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1701 = prims.reshape(t1700, (1, 512, 4096))  # t1701: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t1700
  t1702 = torch.nn.functional.linear(t1701, t113, None)  # t1702: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1702 = ltorch.linear(t1701, t113, None)  # t1702: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1702 = prims.linear(t1701, t113, None)  # t1702: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t1706, t1713, t1721] = nvFusion73(t1634, t1702, t1717)
    # t1704 = prims.convert_element_type(t1634, dtypes.float32)  # t1704: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1703 = prims.convert_element_type(t1702, dtypes.float32)  # t1703: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1705 = prims.add(t1703, t1704)  # t1705: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1706 = prims.convert_element_type(t1705, dtypes.bfloat16)  # t1706: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1708 = prims.mul(t1705, t1705)  # t1708: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1709 = prims.sum(t1708, (2,))  # t1709: &#34;cuda:0 f32[1, 512]&#34;
    # t1710 = prims.broadcast_in_dim(t1709, [1, 512, 1], [0, 1])  # t1710: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1711 = prims.div(t1710, 4096.0)  # t1711: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1712 = prims.add(t1711, 1e-05)  # t1712: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1713 = prims.rsqrt(t1712)  # t1713: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1714 = prims.broadcast_in_dim(t1713, (1, 512, 4096), (0, 1, 2))  # t1714: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1715 = prims.mul(t1705, t1714)  # t1715: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1719 = prims.convert_element_type(t1717, dtypes.float32)  # t1719: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1720 = prims.mul(t1715, t1719)  # t1720: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1721 = prims.convert_element_type(t1720, dtypes.bfloat16)  # t1721: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t1722 = torch.nn.functional.linear(t1721, t33, None)  # t1722: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t1722 = ltorch.linear(t1721, t33, None)  # t1722: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t1722 = prims.linear(t1721, t33, None)  # t1722: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t1723 = torch.nn.functional.linear(t1721, t49, None)  # t1723: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t1723 = ltorch.linear(t1721, t49, None)  # t1723: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t1723 = prims.linear(t1721, t49, None)  # t1723: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  [t1737] = nvFusion74(t1722, t1723)
    # t1724 = prims.convert_element_type(t1722, dtypes.float32)  # t1724: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1725 = prims.neg(t1724)  # t1725: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1726 = prims.exp(t1725)  # t1726: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1727 = prims.add(1.0, t1726)  # t1727: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1728 = prims.reciprocal(t1727)  # t1728: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1732 = prims.mul(t1724, t1728)  # t1732: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1735 = prims.convert_element_type(t1723, dtypes.float32)  # t1735: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1736 = prims.mul(t1732, t1735)  # t1736: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1737 = prims.convert_element_type(t1736, dtypes.bfloat16)  # t1737: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t1738 = torch.nn.functional.linear(t1737, t114, None)  # t1738: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1738 = ltorch.linear(t1737, t114, None)  # t1738: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1738 = prims.linear(t1737, t114, None)  # t1738: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t1742, t1749, t1757] = nvFusion75(t1706, t1738, t1753)
    # t1740 = prims.convert_element_type(t1706, dtypes.float32)  # t1740: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1739 = prims.convert_element_type(t1738, dtypes.float32)  # t1739: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1741 = prims.add(t1739, t1740)  # t1741: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1742 = prims.convert_element_type(t1741, dtypes.bfloat16)  # t1742: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1744 = prims.mul(t1741, t1741)  # t1744: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1745 = prims.sum(t1744, (2,))  # t1745: &#34;cuda:0 f32[1, 512]&#34;
    # t1746 = prims.broadcast_in_dim(t1745, [1, 512, 1], [0, 1])  # t1746: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1747 = prims.div(t1746, 4096.0)  # t1747: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1748 = prims.add(t1747, 1e-05)  # t1748: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1749 = prims.rsqrt(t1748)  # t1749: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1750 = prims.broadcast_in_dim(t1749, (1, 512, 4096), (0, 1, 2))  # t1750: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1751 = prims.mul(t1741, t1750)  # t1751: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1755 = prims.convert_element_type(t1753, dtypes.float32)  # t1755: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1756 = prims.mul(t1751, t1755)  # t1756: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1757 = prims.convert_element_type(t1756, dtypes.bfloat16)  # t1757: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t1758 = torch.nn.functional.linear(t1757, t18, None)  # t1758: &#34;cuda:0 bf16[1, 512, 12288]&#34;
    # t1758 = ltorch.linear(t1757, t18, None)  # t1758: &#34;cuda:0 bf16[1, 512, 12288]&#34;
      # t1758 = prims.linear(t1757, t18, None)  # t1758: &#34;cuda:0 bf16[1, 512, 12288]&#34;
  t1759 = torch.reshape(t1758, (1, 512, 32, 3, 128))  # t1759: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
    # t1759 = ltorch.reshape(t1758, (1, 512, 32, 3, 128))  # t1759: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
      # t1759 = prims.reshape(t1758, (1, 512, 32, 3, 128))  # t1759: &#34;cuda:0 bf16[1, 512, 32, 3, 128]&#34;
  del t1758
  t1760 = torch.permute(t1759, (0, 2, 3, 1, 4))  # t1760: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
    # t1760 = ltorch.permute(t1759, (0, 2, 3, 1, 4))  # t1760: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
      # t1760 = prims.transpose(t1759, (0, 2, 3, 1, 4))  # t1760: &#34;cuda:0 bf16[1, 32, 3, 512, 128]&#34;
  del t1759
  (t1761, t1762, t1763) = torch.split(t1760, (1, 1, 1), 2)
    # (t1761, t1762, t1763) = ltorch.split(t1760, (1, 1, 1), 2)
      # t1761 = prims.slice_prim(t1760, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t1761: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t1762 = prims.slice_prim(t1760, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t1762: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
      # t1763 = prims.slice_prim(t1760, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t1763: &#34;cuda:0 bf16[1, 32, 1, 512, 128]&#34;
  del t1760
  t1764 = torch.reshape(t1761, (1, 32, 512, 128))  # t1764: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1764 = ltorch.reshape(t1761, (1, 32, 512, 128))  # t1764: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1764 = prims.reshape(t1761, (1, 32, 512, 128))  # t1764: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1761
  t1765 = torch.reshape(t1762, (1, 32, 512, 128))  # t1765: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1765 = ltorch.reshape(t1762, (1, 32, 512, 128))  # t1765: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1765 = prims.reshape(t1762, (1, 32, 512, 128))  # t1765: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1762
  t1766 = torch.reshape(t1763, (1, 32, 512, 128))  # t1766: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1766 = ltorch.reshape(t1763, (1, 32, 512, 128))  # t1766: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1766 = prims.reshape(t1763, (1, 32, 512, 128))  # t1766: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1763
  t1767 = torch_slice_prim_impl(t1764, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1767: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t1782 = torch_slice_prim_impl(t1765, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t1782: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  t1797 = torch_slice_prim_impl(t1764, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1797: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t1764
  t1799 = torch_slice_prim_impl(t1765, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t1799: &#34;cuda:0 bf16[1, 32, 512, 0]&#34;
  del t1765
  t1768 = torch_slice_prim_impl(t1767, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1768: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1769 = torch_slice_prim_impl(t1767, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1769: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1783 = torch_slice_prim_impl(t1782, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t1783: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  t1784 = torch_slice_prim_impl(t1782, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t1784: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  [t1772, t1787] = nvFusion76(t1767, t1769, t1782, t1784)
    # t1770 = prims.convert_element_type(t1769, dtypes.float32)  # t1770: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1771 = prims.neg(t1770)  # t1771: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1772 = prims.convert_element_type(t1771, dtypes.bfloat16)  # t1772: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
    # t1785 = prims.convert_element_type(t1784, dtypes.float32)  # t1785: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1786 = prims.neg(t1785)  # t1786: &#34;cuda:0 f32[1, 32, 512, 64]&#34;
    # t1787 = prims.convert_element_type(t1786, dtypes.bfloat16)  # t1787: &#34;cuda:0 bf16[1, 32, 512, 64]&#34;
  del t1769, t1784
  t1788 = torch.cat((t1787, t1783), -1)  # t1788: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1788 = ltorch.cat((t1787, t1783), -1)  # t1788: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1788 = prims.cat((t1787, t1783), -1)  # t1788: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1787, t1783
  t1773 = torch.cat((t1772, t1768), -1)  # t1773: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1773 = ltorch.cat((t1772, t1768), -1)  # t1773: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1773 = prims.cat((t1772, t1768), -1)  # t1773: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1772, t1768
  [t1781, t1796] = nvFusion77(t154, t157, t1767, t1773, t1782, t1788)
    # t1775 = prims.convert_element_type(t1767, dtypes.float32)  # t1775: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1790 = prims.convert_element_type(t1782, dtypes.float32)  # t1790: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1791 = prims.mul(t1790, t154)  # t1791: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1793 = prims.convert_element_type(t1788, dtypes.float32)  # t1793: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1794 = prims.mul(t1793, t157)  # t1794: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1795 = prims.add(t1791, t1794)  # t1795: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1796 = prims.convert_element_type(t1795, dtypes.bfloat16)  # t1796: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1776 = prims.mul(t1775, t154)  # t1776: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1778 = prims.convert_element_type(t1773, dtypes.float32)  # t1778: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1779 = prims.mul(t1778, t157)  # t1779: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1780 = prims.add(t1776, t1779)  # t1780: &#34;cuda:0 f32[1, 32, 512, 128]&#34;
    # t1781 = prims.convert_element_type(t1780, dtypes.bfloat16)  # t1781: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1767, t1773, t1782, t1788
  t1800 = torch.cat((t1796, t1799), -1)  # t1800: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1800 = ltorch.cat((t1796, t1799), -1)  # t1800: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1800 = prims.cat((t1796, t1799), -1)  # t1800: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1796, t1799
  t1798 = torch.cat((t1781, t1797), -1)  # t1798: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
    # t1798 = ltorch.cat((t1781, t1797), -1)  # t1798: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
      # t1798 = prims.cat((t1781, t1797), -1)  # t1798: &#34;cuda:0 bf16[1, 32, 512, 128]&#34;
  del t1781, t1797
  (t1801, t1802, t1803, t1804, _, _, t1805, t1806, _) = sdpafx_grad_forward_scaled_dot_product_efficient_attention(t1798, t1800, t1766, 0.0, True, scale=0.08838834764831843)
  t1808 = torch.permute(t1801, (0, 2, 1, 3))  # t1808: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
    # t1808 = ltorch.permute(t1801, (0, 2, 1, 3))  # t1808: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
      # t1808 = prims.transpose(t1801, (0, 2, 1, 3))  # t1808: &#34;cuda:0 bf16[1, 512, 32, 128]&#34;
  t1809 = torch.reshape(t1808, (1, 512, 4096))  # t1809: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1809 = ltorch.reshape(t1808, (1, 512, 4096))  # t1809: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1809 = prims.reshape(t1808, (1, 512, 4096))  # t1809: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  del t1808
  t1810 = torch.nn.functional.linear(t1809, t115, None)  # t1810: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1810 = ltorch.linear(t1809, t115, None)  # t1810: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1810 = prims.linear(t1809, t115, None)  # t1810: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t1814, t1821, t1829] = nvFusion78(t1742, t1810, t1825)
    # t1812 = prims.convert_element_type(t1742, dtypes.float32)  # t1812: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1811 = prims.convert_element_type(t1810, dtypes.float32)  # t1811: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1813 = prims.add(t1811, t1812)  # t1813: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1814 = prims.convert_element_type(t1813, dtypes.bfloat16)  # t1814: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1816 = prims.mul(t1813, t1813)  # t1816: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1817 = prims.sum(t1816, (2,))  # t1817: &#34;cuda:0 f32[1, 512]&#34;
    # t1818 = prims.broadcast_in_dim(t1817, [1, 512, 1], [0, 1])  # t1818: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1819 = prims.div(t1818, 4096.0)  # t1819: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1820 = prims.add(t1819, 1e-05)  # t1820: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1821 = prims.rsqrt(t1820)  # t1821: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1822 = prims.broadcast_in_dim(t1821, (1, 512, 4096), (0, 1, 2))  # t1822: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1823 = prims.mul(t1813, t1822)  # t1823: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1827 = prims.convert_element_type(t1825, dtypes.float32)  # t1827: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1828 = prims.mul(t1823, t1827)  # t1828: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1829 = prims.convert_element_type(t1828, dtypes.bfloat16)  # t1829: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t1831 = torch.nn.functional.linear(t1829, t50, None)  # t1831: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t1831 = ltorch.linear(t1829, t50, None)  # t1831: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t1831 = prims.linear(t1829, t50, None)  # t1831: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t1830 = torch.nn.functional.linear(t1829, t34, None)  # t1830: &#34;cuda:0 bf16[1, 512, 11008]&#34;
    # t1830 = ltorch.linear(t1829, t34, None)  # t1830: &#34;cuda:0 bf16[1, 512, 11008]&#34;
      # t1830 = prims.linear(t1829, t34, None)  # t1830: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  [t1845] = nvFusion79(t1830, t1831)
    # t1832 = prims.convert_element_type(t1830, dtypes.float32)  # t1832: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1833 = prims.neg(t1832)  # t1833: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1834 = prims.exp(t1833)  # t1834: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1835 = prims.add(1.0, t1834)  # t1835: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1836 = prims.reciprocal(t1835)  # t1836: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1840 = prims.mul(t1832, t1836)  # t1840: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1843 = prims.convert_element_type(t1831, dtypes.float32)  # t1843: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1844 = prims.mul(t1840, t1843)  # t1844: &#34;cuda:0 f32[1, 512, 11008]&#34;
    # t1845 = prims.convert_element_type(t1844, dtypes.bfloat16)  # t1845: &#34;cuda:0 bf16[1, 512, 11008]&#34;
  t1846 = torch.nn.functional.linear(t1845, t116, None)  # t1846: &#34;cuda:0 bf16[1, 512, 4096]&#34;
    # t1846 = ltorch.linear(t1845, t116, None)  # t1846: &#34;cuda:0 bf16[1, 512, 4096]&#34;
      # t1846 = prims.linear(t1845, t116, None)  # t1846: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  [t1857, t1865] = nvFusion80(t1814, t1846, t1861)
    # t1848 = prims.convert_element_type(t1814, dtypes.float32)  # t1848: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1847 = prims.convert_element_type(t1846, dtypes.float32)  # t1847: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1849 = prims.add(t1847, t1848)  # t1849: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1852 = prims.mul(t1849, t1849)  # t1852: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1853 = prims.sum(t1852, (2,))  # t1853: &#34;cuda:0 f32[1, 512]&#34;
    # t1854 = prims.broadcast_in_dim(t1853, [1, 512, 1], [0, 1])  # t1854: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1855 = prims.div(t1854, 4096.0)  # t1855: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1856 = prims.add(t1855, 1e-05)  # t1856: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1857 = prims.rsqrt(t1856)  # t1857: &#34;cuda:0 f32[1, 512, 1]&#34;
    # t1858 = prims.broadcast_in_dim(t1857, (1, 512, 4096), (0, 1, 2))  # t1858: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1859 = prims.mul(t1849, t1858)  # t1859: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1863 = prims.convert_element_type(t1861, dtypes.float32)  # t1863: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1864 = prims.mul(t1859, t1863)  # t1864: &#34;cuda:0 f32[1, 512, 4096]&#34;
    # t1865 = prims.convert_element_type(t1864, dtypes.bfloat16)  # t1865: &#34;cuda:0 bf16[1, 512, 4096]&#34;
  t1866 = torch.nn.functional.linear(t1865, t51, None)  # t1866: &#34;cuda:0 bf16[1, 512, 32000]&#34;
    # t1866 = ltorch.linear(t1865, t51, None)  # t1866: &#34;cuda:0 bf16[1, 512, 32000]&#34;
      # t1866 = prims.linear(t1865, t51, None)  # t1866: &#34;cuda:0 bf16[1, 512, 32000]&#34;
  return {&#39;output&#39;: t1866, &#39;flat_args&#39;: [t0, t1, t2, t3, t4, t5, t6, t7, t8, t9, t10, t11, t12, t13, t14, t15, t16, t17, t18, t19, t20, t21, t22, t23, t24, t25, t26, t27, t28, t29, t30, t31, t32, t33, t34, t35, t36, t37, t38, t39, t40, t41, t42, t43, t44, t45, t46, t47, t48, t49, t50, t51, t52, t53, t54, t55, t56, t57, t58, t59, t60, t61, t62, t63, t64, t65, t66, t67, t68, t69, t70, t71, t72, t73, t74, t75, t76, t77, t78, t79, t80, t81, t82, t83, t84, t85, t86, t87, t88, t89, t90, t91, t92, t93, t94, t95, t96, t97, t98, t99, t100, t101, t102, t103, t104, t105, t106, t107, t108, t109, t110, t111, t112, t113, t114, t115, t116, t117], &#39;flat_output&#39;: (t1866,)}, ((t0, t10, t100, t1001, t101, t1010, t102, t103, t104, t1042, t1044, t1045, t1046, t1047, t1048, t1049, t105, t1050, t1053, t1054, t1058, t106, t1065, t1069, t107, t1073, t1074, t1075, t108, t1089, t109, t1090, t1094, t11, t110, t1101, t1105, t1109, t111, t1118, t112, t113, t114, t115, t1150, t1152, t1153, t1154, t1155, t1156, t1157, t1158, t116, t1161, t1162, t1166, t1173, t1177, t1181, t1182, t1183, t1197, t1198, t12, t1202, t1209, t1213, t1217, t122, t1226, t1258, t1260, t1261, t1262, t1263, t1264, t1265, t1266, t1269, t1270, t1274, t1281, t1285, t1289, t129, t1290, t1291, t13, t1305, t1306, t1310, t1317, t1321, t1325, t133, t1334, t1366, t1368, t1369, t137, t1370, t1371, t1372, t1373, t1374, t1377, t1378, t1382, t1389, t1393, t1397, t1398, t1399, t14, t1413, t1414, t1418, t1425, t1429, t1433, t1442, t146, t1474, t1476, t1477, t1478, t1479, t1480, t1481, t1482, t1485, t1486, t1490, t1497, t15, t1501, t1505, t1506, t1507, t1521, t1522, t1526, t1533, t1537, t154, t1541, t1550, t157, t1582, t1584, t1585, t1586, t1587, t1588, t1589, t1590, t1593, t1594, t1598, t16, t1605, t1609, t1613, t1614, t1615, t1629, t1630, t1634, t1641, t1645, t1649, t1658, t1690, t1692, t1693, t1694, t1695, t1696, t1697, t1698, t17, t1701, t1702, t1706, t1713, t1717, t1721, t1722, t1723, t1737, t1738, t1742, t1749, t1753, t1757, t1766, t178, t1798, t18, t180, t1800, t1801, t1802, t1803, t1804, t1805, t1806, t1809, t181, t1810, t1814, t182, t1821, t1825, t1829, t183, t1830, t1831, t184, t1845, t1846, t185, t1857, t186, t1861, t1865, t189, t19, t190, t194, t20, t201, t205, t209, t21, t210, t211, t22, t225, t226, t23, t230, t237, t24, t241, t245, t25, t254, t26, t27, t28, t286, t288, t289, t29, t290, t291, t292, t293, t294, t297, t298, t3, t30, t302, t309, t31, t313, t317, t318, t319, t32, t33, t333, t334, t338, t34, t345, t349, t35, t353, t36, t362, t37, t38, t39, t394, t396, t397, t398, t399, t4, t40, t400, t401, t402, t405, t406, t41, t410, t417, t42, t421, t425, t426, t427, t43, t44, t441, t442, t446, t45, t453, t457, t46, t461, t47, t470, t48, t49, t5, t50, t502, t504, t505, t506, t507, t508, t509, t51, t510, t513, t514, t518, t525, t529, t533, t534, t535, t549, t550, t554, t561, t565, t569, t578, t6, t610, t612, t613, t614, t615, t616, t617, t618, t621, t622, t626, t633, t637, t641, t642, t643, t657, t658, t662, t669, t673, t677, t686, t7, t718, t720, t721, t722, t723, t724, t725, t726, t729, t730, t734, t741, t745, t749, t750, t751, t765, t766, t770, t777, t781, t785, t794, t8, t826, t828, t829, t830, t831, t832, t833, t834, t837, t838, t842, t849, t85, t853, t857, t858, t859, t86, t87, t873, t874, t878, t88, t885, t889, t89, t893, t9, t90, t902, t91, t92, t93, t934, t936, t937, t938, t939, t94, t940, t941, t942, t945, t946, t95, t950, t957, t96, t961, t965, t966, t967, t97, t98, t981, t982, t986, t99, t993, t997), (False, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, 4096.0, 4096.0, 0.0, 0.08838834764831843, 4096.0, 4096.0, 0.0, 0.08838834764831843, 4096.0, 4096.0, 0.0, 0.08838834764831843, 4096.0, 4096.0, 0.0, 0.08838834764831843, 4096.0, 4096.0, 0.0, 0.08838834764831843, 4096.0, 4096.0, 0.0, 0.08838834764831843, 0.0, 4096.0, 4096.0, 0.08838834764831843, 0.0, 0.08838834764831843, 4096.0, 4096.0, 0.0, 0.08838834764831843, 4096.0, 4096.0, 4096.0, 0.0, 0.08838834764831843, 4096.0, 4096.0, 4096.0, 4096.0, 0.0, 0.08838834764831843, 4096.0, 4096.0, 0.0, 0.08838834764831843, 4096.0, 4096.0, 0.0, 0.08838834764831843, 4096.0, 4096.0, 0.0, 0.08838834764831843, 4096.0, 4096.0, 0.0, 0.08838834764831843, 4096.0, 4096.0, 0.0, 0.08838834764831843, 32000, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2))
</pre></div></div>
</div>
<p>Well, that is quite a bit to look through. But here is a key thing: The function now returns a bunch of things. This is because Thunder applies the same treatment to the backward and to this end saves information from the forward. You can see a hint of this because the output has a <code class="docutils literal notranslate"><span class="pre">ThunderFunctionBackward</span></code> on as its <code class="docutils literal notranslate"><span class="pre">grad_fn</span></code>. (You can see the backward trace with <code class="docutils literal notranslate"><span class="pre">thunder.last_backward_traces(thunder_model)[-1]</span></code>).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">actual</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
tensor([[[ 0.4160, -0.4668,  1.1016,  ...,  0.5430,  1.2656,  0.2891],
         [ 0.3320, -0.0557,  1.7891,  ...,  1.0703,  1.0078,  1.2266],
         [ 0.6836, -0.2871,  0.9531,  ...,  0.0806,  0.7070,  0.8477],
         ...,
         [ 0.7695, -0.1260,  0.7266,  ...,  0.1118, -0.0238, -1.2656],
         [-0.7773, -0.5547, -0.3047,  ..., -0.1807,  0.1895,  0.6875],
         [ 0.8867,  0.4766,  0.3984,  ...,  0.0815, -0.0879,  0.3477]]],
       device=&#39;cuda:0&#39;, grad_fn=&lt;ThunderFunctionBackward&gt;)
</pre></div></div>
</div>
<p>Let us clean up a bit.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">del</span> <span class="n">actual</span><span class="p">,</span> <span class="n">expected</span>
<span class="kn">import</span> <span class="nn">gc</span>
<span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">();</span>
</pre></div>
</div>
</div>
<p>But is it faster? Yes!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">timeit</span> r = m(inp); torch.autograd.grad(r.sum(), m.parameters()); torch.cuda.synchronize()
<span class="o">%</span><span class="k">timeit</span> r = thunder_model(inp); torch.autograd.grad(r.sum(), m.parameters()); torch.cuda.synchronize()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
240 ms ± 105 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
208 ms ± 147 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)
</pre></div></div>
</div>
<p>So far, so good! Thunder should work with LitGPT today and we busy are adding the support required to run other models as well!</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">del</span> <span class="n">m</span><span class="p">,</span> <span class="n">thunder_model</span>
<span class="kn">import</span> <span class="nn">gc</span>
<span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
</section>
<section id="Distributed-with-Thunder">
<h2>Distributed with Thunder<a class="headerlink" href="#Distributed-with-Thunder" title="Permalink to this heading">¶</a></h2>
<p>Those Large Language Models are called Large for a reason, and memory in a single GPU is invariably small. So we need multiple.</p>
<p>Happily Thunder sports an FSDP interface to use multiple cards in our box.</p>
<p>You still need to setup the process group, but as far as the model is concerned,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">thunder</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">thunder</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">fsdp</span><span class="p">(</span><span class="n">model</span><span class="p">))</span>
</pre></div>
</div>
<p>is all you need. Because it is tricky to run multiprocessing from Notebooks, we write a small example into a file and run it though <code class="docutils literal notranslate"><span class="pre">torch-run</span></code>.</p>
<p>Check out our LitGPT Thunder examples for complete distributed training and finetuning!</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%writefile</span> zero_to_thunder_fsdp_simple_example.py
<span class="kn">from</span> <span class="nn">thunder.tests.lit_gpt_model</span> <span class="kn">import</span> <span class="n">GPT</span><span class="p">,</span> <span class="n">Config</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">torch</span><span class="o">,</span> <span class="nn">torch.distributed</span>
<span class="kn">import</span> <span class="nn">thunder</span><span class="o">,</span> <span class="nn">thunder.distributed</span>

<span class="c1"># Create Model</span>
<span class="c1"># NOTE: We create the model on CPU.</span>
<span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_default_dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
<span class="n">cfg</span> <span class="o">=</span> <span class="n">Config</span><span class="o">.</span><span class="n">from_name</span><span class="p">(</span><span class="s1">&#39;Llama-2-7b-hf&#39;</span><span class="p">)</span>
<span class="n">cfg</span><span class="o">.</span><span class="n">n_layer</span> <span class="o">=</span> <span class="mi">8</span> <span class="c1"># fewer layers</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>

<span class="c1"># Setup for distributed</span>
<span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">)</span>
<span class="n">rank</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;LOCAL_RANK&quot;</span><span class="p">])</span>

<span class="n">device</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;cuda:</span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2">&quot;</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1024</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span>

<span class="c1"># thunder.distributed.fsdp takes care of moving the parameter</span>
<span class="c1"># shard to the correct GPU for the current process.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">thunder</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">thunder</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">fsdp</span><span class="p">(</span><span class="n">model</span><span class="p">))</span> <span class="c1">#  &lt;---------------------------------------</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;rank </span><span class="si">{</span><span class="n">rank</span><span class="si">}</span><span class="s2"> computing&quot;</span><span class="p">)</span>
<span class="c1"># Run the forward pass.</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">res</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Overwriting zero_to_thunder_fsdp_simple_example.py
</pre></div></div>
</div>
<p>Now we can launch it. Note that you need two GPUs for this to run correctly.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>torchrun<span class="w"> </span>--nproc_per_node<span class="o">=</span><span class="m">2</span><span class="w"> </span>zero_to_thunder_fsdp_simple_example.py
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
W0320 15:06:06.538000 140013994370240 torch/distributed/run.py:757]
W0320 15:06:06.538000 140013994370240 torch/distributed/run.py:757] *****************************************
W0320 15:06:06.538000 140013994370240 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed.
W0320 15:06:06.538000 140013994370240 torch/distributed/run.py:757] *****************************************
rank 1 computing
rank 0 computing
</pre></div></div>
</div>
<p>So there. FSDP with just wrapping the model in <code class="docutils literal notranslate"><span class="pre">fsdp</span></code>.</p>
</section>
<section id="Extending-Thunder">
<h2>Extending Thunder<a class="headerlink" href="#Extending-Thunder" title="Permalink to this heading">¶</a></h2>
<p>But we promised that thunder is extensible. Let’s find out what’s up with that.</p>
<p>Specifically, we will incorporate the fast rope embedding kernel from the great <a class="reference external" href="https://github.com/unslothai/unsloth/">Unsloth project</a> into our model (note that NVFuser also creates a fused kernel for this).</p>
<p>In Thunder, extensions (as well as most builtin optimizations which use the exact same mechanism) work with <em>executors</em> handling operations. Let us define one.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">my_ex</span> <span class="o">=</span> <span class="n">thunder</span><span class="o">.</span><span class="n">extend</span><span class="o">.</span><span class="n">OperatorExecutor</span><span class="p">(</span><span class="s1">&#39;my_ex&#39;</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="s1">&#39;0.0.1&#39;</span><span class="p">)</span>
<span class="n">thunder</span><span class="o">.</span><span class="n">extend</span><span class="o">.</span><span class="n">register_executor</span><span class="p">(</span><span class="n">my_ex</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
my_ex
</pre></div></div>
</div>
<p>For our base implementation, we take the code from <a class="reference external" href="https://github.com/Lightning-AI/litgpt/blob/be6139e1fd4b240d253efd58124457496d23d173/litgpt/model.py#L355-L361">LitGPT’s implementation</a></p>
<p>In thunder, we define a <em>meta</em> function that only defines the metadata (like shapes) of outputs and the actual implementation for each operator and then register the pair with our executor using the <code class="docutils literal notranslate"><span class="pre">register_operator</span></code> function. Because we will demonstrate Thunder’s ability to divert functions in the model, we make a version here that will not be diverted.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">lit_gpt</span>
<span class="k">def</span> <span class="nf">apply_rope_copy</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">cos</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">sin</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">head_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x1</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="p">:</span> <span class="n">head_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">]</span>  <span class="c1"># (B, nh, T, hs/2)</span>
    <span class="n">x2</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">head_size</span> <span class="o">//</span> <span class="mi">2</span> <span class="p">:]</span>  <span class="c1"># (B, nh, T, hs/2)</span>
    <span class="n">rotated</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="o">-</span><span class="n">x2</span><span class="p">,</span> <span class="n">x1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, nh, T, hs)</span>
    <span class="n">roped</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">*</span> <span class="n">cos</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">rotated</span> <span class="o">*</span> <span class="n">sin</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">roped</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</pre></div>
</div>
</div>
<section id="Registering-operators">
<h3>Registering operators<a class="headerlink" href="#Registering-operators" title="Permalink to this heading">¶</a></h3>
<p>Say we have a function <code class="docutils literal notranslate"><span class="pre">apply_rope</span></code> applying the RoPE transformation in PyTorch.</p>
<p>In thunder, we define a <em>meta</em> function that only defines the metadata (like shapes) of outputs and the actual implementation for each operator and then register the pair with our executor using the <code class="docutils literal notranslate"><span class="pre">register_operator</span></code> function and tell it to use the new symbol instead of the original function <code class="docutils literal notranslate"><span class="pre">lit_gpt.model.apply_rope</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span><span class="o">,</span> <span class="nn">thunder</span>
<span class="kn">from</span> <span class="nn">thunder.tests.lit_gpt_model</span> <span class="kn">import</span> <span class="n">GPT</span>
<span class="kn">from</span> <span class="nn">thunder</span> <span class="kn">import</span> <span class="n">TensorProxy</span>

<span class="k">def</span> <span class="nf">apply_rope_impl</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">cos</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">sin</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">lit_gpt</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">apply_rope</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">apply_rope_meta</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">TensorProxy</span><span class="p">,</span> <span class="n">cos</span><span class="p">:</span> <span class="n">TensorProxy</span><span class="p">,</span> <span class="n">sin</span><span class="p">:</span> <span class="n">TensorProxy</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorProxy</span><span class="p">:</span>
    <span class="k">return</span> <span class="n">TensorProxy</span><span class="p">(</span><span class="n">like</span><span class="o">=</span><span class="n">x</span><span class="p">)</span>

<span class="n">apply_rope</span> <span class="o">=</span> <span class="n">my_ex</span><span class="o">.</span><span class="n">register_operator</span><span class="p">(</span><span class="s1">&#39;apply_rope&#39;</span><span class="p">,</span> <span class="n">like</span><span class="o">=</span><span class="n">apply_rope_meta</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">apply_rope_impl</span><span class="p">,</span>
                                    <span class="n">replaces</span><span class="o">=</span><span class="n">lit_gpt</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">apply_rope</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Testing-our-new-operator">
<h3>Testing our new operator<a class="headerlink" href="#Testing-our-new-operator" title="Permalink to this heading">¶</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">):</span> <span class="n">m</span> <span class="o">=</span> <span class="n">GPT</span><span class="o">.</span><span class="n">from_name</span><span class="p">(</span><span class="s1">&#39;llama2-like&#39;</span><span class="p">);</span> <span class="n">Q</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="mi">4096</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">test_apply_rope</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">lit_gpt</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">apply_rope</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">cos</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">sin</span><span class="p">)</span>

<span class="n">thunder_apply_rope</span> <span class="o">=</span> <span class="n">thunder</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">test_apply_rope</span><span class="p">,</span> <span class="n">executors</span><span class="o">=</span><span class="p">(</span><span class="n">my_ex</span><span class="p">,)</span> <span class="o">+</span> <span class="n">thunder</span><span class="o">.</span><span class="n">get_default_executors</span><span class="p">())</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">test_apply_rope</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">m</span><span class="p">);</span> <span class="n">actual</span> <span class="o">=</span> <span class="n">thunder_apply_rope</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">m</span><span class="p">);</span> <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;deviation:&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">expected</span> <span class="o">-</span> <span class="n">actual</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="n">thunder</span><span class="o">.</span><span class="n">last_traces</span><span class="p">(</span><span class="n">thunder_apply_rope</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
deviation: 0.0
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
# Constructed by Delete Last Used (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast()
def computation(x, t_1_cos, t_1_sin):
  # x: &#34;cuda:0 bf16[2, 128, 4096, 16]&#34;
  # t_1_cos: &#34;cuda:0 f32[4096, 16]&#34;
  # t_1_sin: &#34;cuda:0 f32[4096, 16]&#34;
  t2 = apply_rope(x, t_1_cos, t_1_sin)  # t2: &#34;cuda:0 bf16[2, 128, 4096, 16]&#34;
  del x, t_1_cos, t_1_sin
  return t2
</pre></div></div>
</div>
</section>
<section id="Optimized-kernels">
<h3>Optimized kernels<a class="headerlink" href="#Optimized-kernels" title="Permalink to this heading">¶</a></h3>
<p>But why did we do this? Well, we can now layer a faster implementation on top. For this we take the <a class="reference external" href="https://github.com/unslothai/unsloth/blob/42076f6580e71522ed1c122043edfba595be64e4/unsloth/kernels/rope_embedding.py">unsloth fast rope embedding</a> kernels. We take the bits that were in the forward and backward of the <code class="docutils literal notranslate"><span class="pre">autograd.Function</span></code> into our implementation functions. Note that we include the transpositions in our setup in order to have compatibility to the LitGPT implementation. This
change in memory layout of the operands can have a large effect on the runtime though, so our timings are likely not representative of the ones the Unsloth project gets in their use of the same triton kernels.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Copyright 2023-present Daniel Han-Chen &amp; the Unsloth team. All rights reserved.</span>
<span class="c1">#</span>
<span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
<span class="c1"># you may not use this file except in compliance with the License.</span>
<span class="c1"># You may obtain a copy of the License at</span>
<span class="c1">#</span>
<span class="c1">#     http://www.apache.org/licenses/LICENSE-2.0</span>
<span class="c1">#</span>
<span class="c1"># Unless required by applicable law or agreed to in writing, software</span>
<span class="c1"># distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span>
<span class="c1"># WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span>
<span class="c1"># See the License for the specific language governing permissions and</span>
<span class="c1"># limitations under the License.</span>

<span class="kn">import</span> <span class="nn">triton</span>
<span class="kn">import</span> <span class="nn">triton.language</span> <span class="k">as</span> <span class="nn">tl</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="n">MAX_FUSED_SIZE</span> <span class="o">=</span> <span class="mi">65536</span>
<span class="n">next_power_of_2</span> <span class="o">=</span> <span class="n">triton</span><span class="o">.</span><span class="n">next_power_of_2</span>

<span class="k">def</span> <span class="nf">calculate_settings</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">BLOCK_SIZE</span> <span class="o">=</span> <span class="n">next_power_of_2</span><span class="p">(</span><span class="n">n</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">BLOCK_SIZE</span> <span class="o">&gt;</span> <span class="n">MAX_FUSED_SIZE</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Cannot launch Triton kernel since n = </span><span class="si">{</span><span class="n">n</span><span class="si">}</span><span class="s2"> exceeds &quot;</span>\
                           <span class="sa">f</span><span class="s2">&quot;the maximum CUDA blocksize = </span><span class="si">{</span><span class="n">MAX_FUSED_SIZE</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
    <span class="n">num_warps</span> <span class="o">=</span> <span class="mi">4</span>
    <span class="k">if</span>   <span class="n">BLOCK_SIZE</span> <span class="o">&gt;=</span> <span class="mi">32768</span><span class="p">:</span> <span class="n">num_warps</span> <span class="o">=</span> <span class="mi">32</span>
    <span class="k">elif</span> <span class="n">BLOCK_SIZE</span> <span class="o">&gt;=</span>  <span class="mi">8192</span><span class="p">:</span> <span class="n">num_warps</span> <span class="o">=</span> <span class="mi">16</span>
    <span class="k">elif</span> <span class="n">BLOCK_SIZE</span> <span class="o">&gt;=</span>  <span class="mi">2048</span><span class="p">:</span> <span class="n">num_warps</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="k">return</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">num_warps</span>

<span class="nd">@triton</span><span class="o">.</span><span class="n">heuristics</span><span class="p">({</span><span class="s2">&quot;BACKWARD_PASS&quot;</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">args</span><span class="p">:</span> <span class="n">args</span><span class="p">[</span><span class="s2">&quot;BACKWARD_PASS&quot;</span><span class="p">],})</span>
<span class="nd">@triton</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span> <span class="nf">_rope_embedding</span><span class="p">(</span>
    <span class="n">Q</span><span class="p">,</span>     <span class="n">Q_row_stride</span><span class="p">,</span>
    <span class="n">cos</span><span class="p">,</span> <span class="n">cos_row_stride</span><span class="p">,</span>
    <span class="n">sin</span><span class="p">,</span> <span class="n">sin_row_stride</span><span class="p">,</span>
    <span class="n">seqlen</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span>
    <span class="n">BACKWARD_PASS</span><span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
    <span class="n">BLOCK_SIZE</span> <span class="p">:</span> <span class="n">tl</span><span class="o">.</span><span class="n">constexpr</span><span class="p">,</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates the RoPE Embedding quickly</span>
<span class="sd">        RoPE is Q * cos + rotate_half(Q) * sin</span>
<span class="sd">        See our blog post for more info</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">row_position</span>  <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">group_head_position</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">program_id</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">col_offsets</span>  <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">BLOCK_SIZE</span><span class="p">)</span>
    <span class="n">half_head_dim</span> <span class="o">=</span> <span class="n">head_dim</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">col_offsets</span> <span class="o">&lt;</span> <span class="n">half_head_dim</span>

    <span class="n">sin1</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">sin</span> <span class="o">+</span> <span class="p">(</span><span class="n">row_position</span> <span class="o">%</span> <span class="n">seqlen</span><span class="p">)</span><span class="o">*</span><span class="n">sin_row_stride</span> <span class="o">+</span> \
                   <span class="n">half_head_dim</span><span class="o">*</span><span class="mi">0</span> <span class="o">+</span> <span class="n">col_offsets</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">,</span> <span class="n">other</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">cos1</span> <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">cos</span> <span class="o">+</span> <span class="p">(</span><span class="n">row_position</span> <span class="o">%</span> <span class="n">seqlen</span><span class="p">)</span><span class="o">*</span><span class="n">cos_row_stride</span> <span class="o">+</span> \
                   <span class="n">half_head_dim</span><span class="o">*</span><span class="mi">0</span> <span class="o">+</span> <span class="n">col_offsets</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">,</span> <span class="n">other</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">BACKWARD_PASS</span><span class="p">:</span>
        <span class="c1"># See our blog post for more info.</span>
        <span class="n">sin1</span> <span class="o">=</span> <span class="o">-</span><span class="n">sin1</span>
    <span class="k">pass</span>

    <span class="n">head_start</span> <span class="o">=</span> <span class="n">group_head_position</span> <span class="o">*</span> <span class="n">group_size</span>
    <span class="n">head_end</span> <span class="o">=</span> <span class="nb">min</span><span class="p">((</span><span class="n">head_start</span> <span class="o">+</span> <span class="n">group_size</span><span class="p">),</span> <span class="n">n_heads</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">head_start</span><span class="p">,</span> <span class="n">head_end</span><span class="p">):</span>
        <span class="n">offs_q1</span> <span class="o">=</span> <span class="n">row_position</span> <span class="o">*</span> <span class="n">Q_row_stride</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">head_dim</span> <span class="o">+</span> <span class="n">col_offsets</span>
        <span class="n">offs_q2</span> <span class="o">=</span> <span class="n">row_position</span> <span class="o">*</span> <span class="n">Q_row_stride</span> <span class="o">+</span> <span class="n">i</span> <span class="o">*</span> <span class="n">head_dim</span> <span class="o">+</span> <span class="n">col_offsets</span> <span class="o">+</span> <span class="n">half_head_dim</span>

        <span class="c1"># For Gemma - sometimes RoPE must be done in float32 and not bfloat16</span>
        <span class="n">Q1</span>   <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">Q</span> <span class="o">+</span> <span class="n">offs_q1</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">,</span> <span class="n">other</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">sin1</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">Q2</span>   <span class="o">=</span> <span class="n">tl</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">Q</span> <span class="o">+</span> <span class="n">offs_q2</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">,</span> <span class="n">other</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">sin1</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

        <span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">Q</span> <span class="o">+</span> <span class="n">offs_q1</span><span class="p">,</span> <span class="n">Q1</span><span class="o">*</span><span class="n">cos1</span> <span class="o">-</span> <span class="n">Q2</span><span class="o">*</span><span class="n">sin1</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">)</span>
        <span class="n">tl</span><span class="o">.</span><span class="n">store</span><span class="p">(</span><span class="n">Q</span> <span class="o">+</span> <span class="n">offs_q2</span><span class="p">,</span> <span class="n">Q2</span><span class="o">*</span><span class="n">cos1</span> <span class="o">+</span> <span class="n">Q1</span><span class="o">*</span><span class="n">sin1</span><span class="p">,</span> <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">)</span>
    <span class="k">pass</span>
<span class="k">pass</span>


<span class="k">def</span> <span class="nf">fast_rope_embedding_forward</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">):</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
    <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span> <span class="o">=</span> <span class="n">cos</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">sin</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
    <span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">head_dim</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="o">*</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">*</span><span class="n">head_dim</span><span class="p">)</span>
    <span class="n">n_rows</span><span class="p">,</span> <span class="n">n_cols</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">shape</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">seq_len</span> <span class="o">&lt;=</span> <span class="n">cos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="c1"># [TODO] Changing blocksize to head_dim//2 seems to have</span>
    <span class="c1"># some concurrency / un-deterministic issues.</span>
    <span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">num_warps</span> <span class="o">=</span> <span class="n">calculate_settings</span><span class="p">(</span><span class="n">head_dim</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span> <span class="c1"># (head_dim//2)</span>
    <span class="n">group_size</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1"># 4 or 8, too large group_size can hurt performance.</span>
    <span class="n">n_groups</span> <span class="o">=</span> <span class="n">triton</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>

    <span class="n">grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_rows</span><span class="p">,</span> <span class="n">n_groups</span><span class="p">,</span> <span class="p">)</span>
    <span class="n">_rope_embedding</span><span class="p">[</span><span class="n">grid</span><span class="p">](</span>
          <span class="n">Q</span><span class="p">,</span>   <span class="n">Q</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
        <span class="n">cos</span><span class="p">,</span> <span class="n">cos</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
        <span class="n">sin</span><span class="p">,</span> <span class="n">sin</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
        <span class="n">seq_len</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span>
        <span class="n">BACKWARD_PASS</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">BLOCK_SIZE</span> <span class="o">=</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span>
        <span class="n">num_warps</span>  <span class="o">=</span> <span class="n">num_warps</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">Q</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Q</span><span class="p">,</span> <span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">num_warps</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">fast_rope_embedding_backward</span><span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">num_warps</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">,</span> <span class="n">dY</span><span class="p">):</span>
    <span class="n">dY</span> <span class="o">=</span> <span class="n">dY</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">head_dim</span> <span class="o">=</span> <span class="n">dY</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">dY</span> <span class="o">=</span> <span class="n">dY</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">batch</span><span class="o">*</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">*</span><span class="n">head_dim</span><span class="p">)</span>
    <span class="c1"># Must be reshape not view</span>
    <span class="n">n_rows</span><span class="p">,</span> <span class="n">n_cols</span> <span class="o">=</span> <span class="n">dY</span><span class="o">.</span><span class="n">shape</span>

    <span class="n">group_size</span> <span class="o">=</span> <span class="mi">4</span> <span class="c1"># 4 or 8, too large group_size can hurt performance.</span>
    <span class="n">n_groups</span> <span class="o">=</span> <span class="n">triton</span><span class="o">.</span><span class="n">cdiv</span><span class="p">(</span><span class="n">n_heads</span><span class="p">,</span> <span class="n">group_size</span><span class="p">)</span>

    <span class="n">grid</span> <span class="o">=</span> <span class="p">(</span><span class="n">n_rows</span><span class="p">,</span> <span class="n">n_groups</span><span class="p">,</span> <span class="p">)</span>
    <span class="n">_rope_embedding</span><span class="p">[</span><span class="n">grid</span><span class="p">](</span>
        <span class="n">dY</span><span class="p">,</span>  <span class="n">dY</span> <span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
        <span class="n">cos</span><span class="p">,</span> <span class="n">cos</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
        <span class="n">sin</span><span class="p">,</span> <span class="n">sin</span><span class="o">.</span><span class="n">stride</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
        <span class="n">seq_len</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">,</span> <span class="n">group_size</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span>
        <span class="n">BACKWARD_PASS</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">BLOCK_SIZE</span> <span class="o">=</span> <span class="n">BLOCK_SIZE</span><span class="p">,</span>
        <span class="n">num_warps</span>  <span class="o">=</span> <span class="n">num_warps</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">dY</span> <span class="o">=</span> <span class="n">dY</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">head_dim</span><span class="p">)</span>
    <span class="n">dY</span> <span class="o">=</span> <span class="n">dY</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dY</span>
<br/></pre></div>
</div>
</div>
<p>We also define the corresponding meta functions.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fast_rope_embedding_forward_meta</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">):</span>
    <span class="n">batch</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">head_dim</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">n_rows</span><span class="p">,</span> <span class="n">n_cols</span> <span class="o">=</span> <span class="n">batch</span><span class="o">*</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">*</span><span class="n">head_dim</span>
    <span class="k">assert</span><span class="p">(</span><span class="n">seq_len</span> <span class="o">&lt;=</span> <span class="n">cos</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">num_warps</span> <span class="o">=</span> <span class="n">calculate_settings</span><span class="p">(</span><span class="n">head_dim</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">TensorProxy</span><span class="p">(</span><span class="n">like</span><span class="o">=</span><span class="n">Q</span><span class="p">),</span> <span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">num_warps</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">fast_rope_embedding_backward_meta</span><span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">num_warps</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">,</span> <span class="n">dY</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">TensorProxy</span><span class="p">(</span><span class="n">like</span><span class="o">=</span><span class="n">dY</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Register-optimized-operators">
<h3>Register optimized operators<a class="headerlink" href="#Register-optimized-operators" title="Permalink to this heading">¶</a></h3>
<p>Just like the <code class="docutils literal notranslate"><span class="pre">apply_rope</span></code> before, we can register operators for the optimized forward and backward.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">unsloth_apply_rope_forward</span> <span class="o">=</span> <span class="n">my_ex</span><span class="o">.</span><span class="n">register_operator</span><span class="p">(</span><span class="s1">&#39;unsloth_apply_rope_forward&#39;</span><span class="p">,</span>
    <span class="n">meta</span><span class="o">=</span><span class="n">fast_rope_embedding_forward_meta</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">fast_rope_embedding_forward</span><span class="p">)</span>
<span class="n">unsloth_apply_rope_backward</span> <span class="o">=</span> <span class="n">my_ex</span><span class="o">.</span><span class="n">register_operator</span><span class="p">(</span><span class="s1">&#39;unsloth_apply_rope_backward&#39;</span><span class="p">,</span>
    <span class="n">meta</span><span class="o">=</span><span class="n">fast_rope_embedding_backward_meta</span><span class="p">,</span> <span class="n">fn</span><span class="o">=</span><span class="n">fast_rope_embedding_backward</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="Implementations-for-operators">
<h3>Implementations for operators<a class="headerlink" href="#Implementations-for-operators" title="Permalink to this heading">¶</a></h3>
<p>Do we need to divert <code class="docutils literal notranslate"><span class="pre">apply_rope</span></code> again? No! We can register the specialized kernel as an <em>implementation</em> of our base <code class="docutils literal notranslate"><span class="pre">apply_rope</span></code> operator. For this we need an <em>execution transform</em> - which is a fancy word for a function that implements the original operator (<code class="docutils literal notranslate"><span class="pre">apply_ropw</span></code>) in terms of our new operator - so it has the call signature of the <code class="docutils literal notranslate"><span class="pre">apply_rope</span></code>. Because - like many fast implementations - the unsloth rope embedding does not implement the operator in full generality (well,
actually they mainly want a 4d tensor input), we implement a checker function, too: It takes the arguments of the operator we want specialize and returns a bool whether our implementation handles the given inputs.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">apply_rope_to_unsloth</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">TensorProxy</span><span class="p">,</span> <span class="n">cos</span><span class="p">:</span> <span class="n">TensorProxy</span><span class="p">,</span> <span class="n">sin</span><span class="p">:</span> <span class="n">TensorProxy</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">TensorProxy</span><span class="p">:</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span>
    <span class="n">res</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">unsloth_apply_rope_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">res</span>

<span class="k">def</span> <span class="nf">apply_rope_to_unsloth_checker</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">TensorProxy</span><span class="p">,</span> <span class="n">cos</span><span class="p">:</span> <span class="n">TensorProxy</span><span class="p">,</span> <span class="n">sin</span><span class="p">:</span> <span class="n">TensorProxy</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">!=</span> <span class="mi">4</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">devicetype</span> <span class="o">==</span> <span class="n">thunder</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">DeviceType</span><span class="o">.</span><span class="n">CUDA</span> <span class="ow">and</span>
            <span class="n">cos</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">devicetype</span> <span class="o">==</span> <span class="n">thunder</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">DeviceType</span><span class="o">.</span><span class="n">CUDA</span> <span class="ow">and</span>
           <span class="n">cos</span><span class="o">.</span><span class="n">device</span><span class="o">.</span><span class="n">devicetype</span> <span class="o">==</span> <span class="n">thunder</span><span class="o">.</span><span class="n">devices</span><span class="o">.</span><span class="n">DeviceType</span><span class="o">.</span><span class="n">CUDA</span><span class="p">)</span>

<span class="n">my_ex</span><span class="o">.</span><span class="n">register_implementation</span><span class="p">(</span><span class="n">apply_rope</span><span class="p">,</span>
                              <span class="n">checker</span><span class="o">=</span><span class="n">apply_rope_to_unsloth_checker</span><span class="p">,</span>
                              <span class="n">execution_transform</span><span class="o">=</span><span class="n">apply_rope_to_unsloth</span><span class="p">)</span>
<br/></pre></div>
</div>
</div>
<p>So let us give it a try! Works great…</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">thunder_apply_rope</span> <span class="o">=</span> <span class="n">thunder</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">test_apply_rope</span><span class="p">,</span> <span class="n">executors</span><span class="o">=</span><span class="p">(</span><span class="n">my_ex</span><span class="p">,)</span> <span class="o">+</span> <span class="n">thunder</span><span class="o">.</span><span class="n">get_default_executors</span><span class="p">())</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">test_apply_rope</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">thunder_apply_rope</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;deviation:&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">expected</span> <span class="o">-</span> <span class="n">actual</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>

<span class="n">thunder</span><span class="o">.</span><span class="n">last_traces</span><span class="p">(</span><span class="n">thunder_apply_rope</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
deviation: 0.015625
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
# Constructed by Delete Last Used (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast()
def computation(x, t_1_cos, t_1_sin):
  # x: &#34;cuda:0 bf16[2, 128, 4096, 16]&#34;
  # t_1_cos: &#34;cuda:0 f32[4096, 16]&#34;
  # t_1_sin: &#34;cuda:0 f32[4096, 16]&#34;
  (t2, (_, _)) = unsloth_apply_rope_forward(x, t_1_cos, t_1_sin)
  del x, t_1_cos, t_1_sin
  return t2
</pre></div></div>
</div>
<p>And this is also automatic when we instantiate a larger llama2-like model:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">set_default_dtype</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">):</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">GPT</span><span class="p">(</span><span class="n">Config</span><span class="o">.</span><span class="n">from_name</span><span class="p">(</span><span class="s1">&#39;llama2-like&#39;</span><span class="p">))</span>

<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">p</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="kc">False</span><span class="p">)</span>

<span class="n">thunder_model</span> <span class="o">=</span> <span class="n">thunder</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">executors</span><span class="o">=</span><span class="p">(</span><span class="n">my_ex</span><span class="p">,)</span> <span class="o">+</span> <span class="n">thunder</span><span class="o">.</span><span class="n">get_default_executors</span><span class="p">())</span>

<span class="n">inp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">128</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">thunder_model</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
<span class="n">expected</span> <span class="o">=</span> <span class="n">m</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;deviation:&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">actual</span> <span class="o">-</span> <span class="n">expected</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
deviation: 5.960464477539062e-07
</pre></div></div>
</div>
<p>By peeking into the trace, we can see that it actually used the unsloth apply rope:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">s</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span><span class="n">thunder</span><span class="o">.</span><span class="n">last_traces</span><span class="p">(</span><span class="n">thunder_model</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span> <span class="k">if</span> <span class="s1">&#39;apply_rope&#39;</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
[&#39;  (q_roped, (_, _)) = unsloth_apply_rope_forward(t55, cos, sin)&#39;,
 &#39;  (k_roped, (_, _)) = unsloth_apply_rope_forward(t57, cos, sin)&#39;,
 &#39;  (t165, (_, _)) = unsloth_apply_rope_forward(t164, cos, sin)&#39;,
 &#39;  (t167, (_, _)) = unsloth_apply_rope_forward(t166, cos, sin)&#39;]
</pre></div></div>
</div>
</section>
<section id="But-what-about-the-backward?">
<h3>But what about the backward?<a class="headerlink" href="#But-what-about-the-backward?" title="Permalink to this heading">¶</a></h3>
<p>Well, we have to connect forward and backward with a grad transformation. With our specialized ops, this is very simple, we compute the forward, call <code class="docutils literal notranslate"><span class="pre">get_grad</span></code> for the output, compute the backward, and put it on the input with <code class="docutils literal notranslate"><span class="pre">put_grads</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[27]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">thunder.core.transforms</span> <span class="kn">import</span> <span class="n">get_grad</span><span class="p">,</span> <span class="n">put_grads</span>

<span class="k">def</span> <span class="nf">unsloth_apply_rope_grad</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">TensorProxy</span><span class="p">,</span> <span class="n">cos</span><span class="p">:</span> <span class="n">TensorProxy</span><span class="p">,</span> <span class="n">sin</span><span class="p">:</span> <span class="n">TensorProxy</span><span class="p">):</span>
    <span class="n">res</span><span class="p">,</span> <span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">num_warps</span><span class="p">)</span> <span class="o">=</span> <span class="n">unsloth_apply_rope_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">)</span>
    <span class="n">grad_res</span> <span class="o">=</span> <span class="n">get_grad</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
    <span class="n">grad_x</span> <span class="o">=</span> <span class="n">unsloth_apply_rope_backward</span><span class="p">(</span><span class="n">BLOCK_SIZE</span><span class="p">,</span> <span class="n">num_warps</span><span class="p">,</span> <span class="n">cos</span><span class="p">,</span> <span class="n">sin</span><span class="p">,</span> <span class="n">grad_res</span><span class="p">)</span>
    <span class="n">put_grads</span><span class="p">((</span><span class="n">x</span><span class="p">,),</span> <span class="p">(</span><span class="n">grad_x</span><span class="p">,))</span>
    <span class="k">return</span> <span class="n">res</span>

<span class="n">my_ex</span><span class="o">.</span><span class="n">register_implementation</span><span class="p">(</span><span class="n">apply_rope</span><span class="p">,</span> <span class="n">checker</span><span class="o">=</span><span class="n">apply_rope_to_unsloth_checker</span><span class="p">,</span>
                              <span class="n">execution_transform</span><span class="o">=</span><span class="n">apply_rope_to_unsloth</span><span class="p">,</span>
                              <span class="n">grad_transform</span><span class="o">=</span><span class="n">unsloth_apply_rope_grad</span>
                              <span class="p">)</span>
<br/><br/></pre></div>
</div>
</div>
<p>Note that the parts are not actually executed at the same time in the actual computation, but just during tracing.</p>
<p>And let us try our function using the optimized backward</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[28]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Q</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>

<span class="n">thunder_apply_rope</span> <span class="o">=</span> <span class="n">thunder</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">test_apply_rope</span><span class="p">,</span> <span class="n">executors</span><span class="o">=</span><span class="p">(</span><span class="n">my_ex</span><span class="p">,)</span> <span class="o">+</span> <span class="n">thunder</span><span class="o">.</span><span class="n">get_default_executors</span><span class="p">())</span>

<span class="n">expected</span> <span class="o">=</span> <span class="n">test_apply_rope</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
<span class="n">go</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">expected</span><span class="p">)</span>
<span class="n">gr_expected</span><span class="p">,</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">expected</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">go</span><span class="p">)</span>
<span class="n">actual</span> <span class="o">=</span> <span class="n">thunder_apply_rope</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
<span class="n">gr_actual</span><span class="p">,</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">actual</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">go</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;res deviation:&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">expected</span> <span class="o">-</span> <span class="n">actual</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;grad deviation:&quot;</span><span class="p">,</span> <span class="p">(</span><span class="n">gr_expected</span> <span class="o">-</span> <span class="n">gr_actual</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
res deviation: 0.015625
grad deviation: 0.0078125
</pre></div></div>
</div>
<p>And with <code class="docutils literal notranslate"><span class="pre">last_backward_traces</span></code> we can check that our module is using the unsloth backward:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">thunder</span><span class="o">.</span><span class="n">last_backward_traces</span><span class="p">(</span><span class="n">thunder_apply_rope</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
# Constructed by Delete Last Used (took 0 milliseconds)
import torch
from thunder.executors.torchex import no_autocast

@torch.no_grad()
@no_autocast()
def backward_fn(saved_for_backward, cotangents):
  # saved_for_backward: &#34;Collection&#34;
  # cotangents: &#34;Collection&#34;
  C0, \
  _, \
  = saved_for_backward
  clear_collection(saved_for_backward)
  del saved_for_backward
  t4, \
  = cotangents
  clear_collection(cotangents)
  del cotangents
  t1, \
  t2, \
  = C0
  clear_collection(C0)
  del C0
  t3 = unsloth_apply_rope_backward(8, 4, t1, t2, t4)  # t3: &#34;cuda:0 bf16[2, 128, 4096, 16]&#34;
  del t1, t2, t4
  return (t3, None, None)
</pre></div></div>
</div>
</section>
<section id="Comparing-and-exploring-optimizations">
<h3>Comparing and exploring optimizations<a class="headerlink" href="#Comparing-and-exploring-optimizations" title="Permalink to this heading">¶</a></h3>
<p>It is also straightforward to compare potential optimizations.</p>
<p>Note again, that our use of the unsloth kernel might not result in the same performance as the unsloth project sees due to differences in the hardware used, software environment, or memory layout of the operands.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_apply_rope_copy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">apply_rope_copy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">cos</span><span class="p">,</span> <span class="n">m</span><span class="o">.</span><span class="n">sin</span><span class="p">)</span>

<span class="n">test_apply_rope_myex</span> <span class="o">=</span> <span class="n">thunder</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">test_apply_rope</span><span class="p">,</span> <span class="n">executors</span><span class="o">=</span><span class="p">(</span><span class="n">my_ex</span><span class="p">,)</span> <span class="o">+</span> <span class="n">thunder</span><span class="o">.</span><span class="n">get_default_executors</span><span class="p">())</span>
<span class="n">test_apply_rope_nvfuser</span> <span class="o">=</span> <span class="n">thunder</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">test_apply_rope_copy</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">test_apply_rope</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">m</span><span class="p">);</span> <span class="n">gr</span><span class="p">,</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">go</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">test_apply_rope_myex</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">m</span><span class="p">);</span> <span class="n">gr</span><span class="p">,</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">go</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">test_apply_rope_nvfuser</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">m</span><span class="p">);</span> <span class="n">gr</span><span class="p">,</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">go</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;eager&quot;</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> y = test_apply_rope(Q, m); gr, = torch.autograd.grad(y, Q, go); torch.cuda.synchronize()
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;thunder + unsloth&quot;</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> y = test_apply_rope_myex(Q, m); gr, = torch.autograd.grad(y, Q, go); torch.cuda.synchronize()
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;thunder default (nvfuser)&quot;</span><span class="p">)</span>
<span class="o">%</span><span class="k">timeit</span> y = test_apply_rope_nvfuser(Q, m); gr, = torch.autograd.grad(y, Q, go); torch.cuda.synchronize()
<br/></pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
eager
3.84 ms ± 3.46 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
thunder + unsloth
6.69 ms ± 3.45 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
thunder default (nvfuser)
1.4 ms ± 4.98 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)
</pre></div></div>
</div>
<p>That’s it!</p>
</section>
</section>
<section id="Conclusion">
<h2>Conclusion<a class="headerlink" href="#Conclusion" title="Permalink to this heading">¶</a></h2>
<p>To wrap up, we hope you got a taste of</p>
<ul class="simple">
<li><p>Getting things going with Thunder:</p>
<ul>
<li><p>Applying Thunder through <code class="docutils literal notranslate"><span class="pre">thunder.jit</span></code> and</p></li>
<li><p>using FSDP by just wrapping the model in <code class="docutils literal notranslate"><span class="pre">thunder.distributed.fsdp</span></code> before compilation.</p></li>
</ul>
</li>
<li><p>See what’s going on inspecting traces:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">thunder.last_traces</span></code> for the forward traces,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">thunder.last_backward_traces</span></code> for the backward,</p></li>
</ul>
</li>
<li><p>Extending Thunder:</p>
<ul>
<li><p>registering operators with the <code class="docutils literal notranslate"><span class="pre">OperatorExecutor</span></code>,</p></li>
<li><p>defining implementations with custom forward and backward to include optimized kernels.</p></li>
</ul>
</li>
</ul>
<p>Keep in mind that Thunder is still experimental and only expected to work with the limited set of models we have tested it with. You will find bugs and missing pieces. Naturally, we would love for you to help us fix these! You can find us on the <a class="reference external" href="https://lightning.ai/forums/c/thunder">Thunder section of the Lightning forums</a> or in the <code class="docutils literal notranslate"><span class="pre">#thunder</span></code> channel on the <a class="reference external" href="https://pytorch-lightning.slack.com/">PyTorch-Lightning slack</a>.</p>
<p>Do check out our LitGPT studios and the other tutorial notebooks.</p>
</section>
</section>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../basic/inspecting_traces.html" class="btn btn-neutral float-right" title="Thunder step by step" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="../basic/overview.html" class="btn btn-neutral" title="Thunder Overview" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  

  

    <hr>

  

  <div role="contentinfo">
    <p>
        &copy; Copyright 2024 Lightning-AI et al.

    </p>
  </div>
    
      <div>
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
      </div>
     

</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Zero to Thunder</a><ul>
<li><a class="reference internal" href="#Compiling-a-first-module-with-Thunder">Compiling a first module with Thunder</a></li>
<li><a class="reference internal" href="#Compiling-a-more-complex-model">Compiling a more complex model</a></li>
<li><a class="reference internal" href="#Distributed-with-Thunder">Distributed with Thunder</a></li>
<li><a class="reference internal" href="#Extending-Thunder">Extending Thunder</a><ul>
<li><a class="reference internal" href="#Registering-operators">Registering operators</a></li>
<li><a class="reference internal" href="#Testing-our-new-operator">Testing our new operator</a></li>
<li><a class="reference internal" href="#Optimized-kernels">Optimized kernels</a></li>
<li><a class="reference internal" href="#Register-optimized-operators">Register optimized operators</a></li>
<li><a class="reference internal" href="#Implementations-for-operators">Implementations for operators</a></li>
<li><a class="reference internal" href="#But-what-about-the-backward?">But what about the backward?</a></li>
<li><a class="reference internal" href="#Comparing-and-exploring-optimizations">Comparing and exploring optimizations</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Conclusion">Conclusion</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  

  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="../_static/sphinx_highlight.js"></script>
         <script src="../_static/clipboard.min.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script src="../_static/copybutton.js"></script>
         <script>let toggleHintShow = 'Click to show';</script>
         <script>let toggleHintHide = 'Click to hide';</script>
         <script>let toggleOpenOnPrint = 'true';</script>
         <script src="../_static/togglebutton.js"></script>
         <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->

  <!-- <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources"> -->
    <!-- <div class="container"> -->
      <!-- <div class="row"> -->
        <!--
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://lightning-thunder.rtfd.io/en/latest">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://lightning-thunder.readthedocs.io/en/latest/#tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://lightning-thunder.readthedocs.io/en/latest/#community-examples">View Resources</a>
        </div>
        -->
      <!-- </div> -->
    <!-- </div> -->
  <!-- </div> -->

  <!--
  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://lightning-thunder.rtfd.io/en/latest/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://lightning-thunder.rtfd.io/en/latest/">PyTorch</a></li>
            <li><a href="https://lightning-thunder.readthedocs.io/en/latest/introduction_guide.html">Get Started</a></li>
            <li><a href="https://lightning-thunder.rtfd.io/en/latest/">Features</a></li>
            <li><a href="">Ecosystem</a></li>
            <li><a href="https://www.Lightning-AI.ai/blog">Blog</a></li>
            <li><a href="https://github.com/Lightning-AI/lightning-thunder/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://lightning-thunder.readthedocs.io/en/latest/#community-examples">Resources</a></li>
            <li><a href="https://lightning-thunder.readthedocs.io/en/latest/#tutorials">Tutorials</a></li>
            <li><a href="https://lightning-thunder.rtfd.io/en/latest">Docs</a></li>
            <li><a href="https://pytorch-lightning.slack.com" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/Lightning-AI/lightning-thunder/issues" target="_blank">Github Issues</a></li>
            <li><a href="" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/LightningAI" target="_blank" class="twitter"></a>
            <a href="" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>
  -->

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. Read PyTorch Lightning's <a href="https://pytorchlightning.ai/privacy-policy">Privacy Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://lightning-thunder.rtfd.io/en/latest/" aria-label="PyTorch Lightning"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://lightning-thunder.readthedocs.io/en/latest/introduction_guide.html">Get Started</a>
          </li>

          <li>
            <a href="https://www.Lightning-AI.ai/blog">Blog</a>
          </li>

          <li class="resources-mobile-menu-title">
            Ecosystem
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://lightning.ai/docs/pytorch/stable/">PyTorch Lightning</a>
            </li>

            <li>
              <a href="https://lightning.ai/docs/fabric/stable/">Lightning Fabric</a>
            </li>

            <li>
              <a href="https://torchmetrics.readthedocs.io/en/stable/">TorchMetrics</a>
            </li>

            <li>
              <a href="https://lightning.ai/docs/fabric/stable/">Fabric</a>
            </li>
          </ul>

          <!--<li class="resources-mobile-menu-title">
            Resources
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://lightning-thunder.readthedocs.io/en/latest/#community-examples">Developer Resources</a>
            </li>

            <li>
              <a href="https://lightning-thunder.rtfd.io/en/latest/">About</a>
            </li>

            <li>
              <a href="">Models (Beta)</a>
            </li>

            <li>
              <a href="">Community</a>
            </li>

            <li>
              <a href="">Forums</a>
            </li>
          </ul>-->

          <li>
            <a href="https://github.com/Lightning-AI/lightning-thunder">Github</a>
          </li>

          <li>
            <a href="https://www.lightning.ai/">Lightning.ai</a>
          </li>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>

  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-PQBQ3CV"
  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->
 </body>
</html>