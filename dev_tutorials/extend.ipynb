{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to use thunder's extend submodule to add new operations and custom grad and execution transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "\n",
    "import thunder\n",
    "import thunder.torch as ltorch\n",
    "from thunder.core.devices import DeviceType\n",
    "from thunder.core.proxies import TensorProxy\n",
    "from thunder.core.transforms import grad, put_grads, get_grad\n",
    "\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from thunder.extend import OperatorExecutor, register_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "myex"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Registers a new operator executor\n",
    "myex = OperatorExecutor(\"myex\", version=\"0.1\")\n",
    "register_executor(myex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our operator executor will use the \"multimul\" function as a new example operator.\n",
    "#   This function uses NumPy to perform two multiplications of four inputs.\n",
    "#   This functions very contrived, but will be useful to illustrate the extend submodule's capabilities.\n",
    "def multimul_impl(\n",
    "        a: Number | torch.Tensor, \n",
    "        b: Number | torch.Tensor,\n",
    "        c: Number | torch.Tensor,\n",
    "        d: Number | torch.Tensor,) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    return np.multiply(a, b), np.multiply(c, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0687, -0.7520],\n",
       "         [ 0.1435, -0.1413]]),\n",
       " tensor([[-0.0687, -0.7520],\n",
       "         [ 0.1435, -0.1413]]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can verify that multimul is a valid Python function that operates on PyTorch tensors -- at least PyTorch tensors on the CPU.\n",
    "a = torch.randn((2, 2))\n",
    "b = torch.randn((2, 2))\n",
    "multimul_impl(a, b, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To let thunder use multimul we need to define how it propagates metadata. This can be done by directly defining a \"meta function\", \n",
    "# of by defining a traceable \"like\" function that describes what multimul does in terms of existing thunder operations. \n",
    "#   The \"like\" function can be used for metadata propagation AND transforming the new operator, as we'll see below.\n",
    "#   In this case, the \"like\" function just describes the two multiplications that multimul performs.\n",
    "def multimul_like(\n",
    "        a: Number | TensorProxy, \n",
    "        b: Number | TensorProxy,\n",
    "        c: Number | TensorProxy,\n",
    "        d: Number | TensorProxy,\n",
    "):\n",
    "    return a * b, c * d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"register_operator\" method of operator executor's returns a \"Symbol\" object for multimul that can be called directly\n",
    "#   from compiled thunder code.\n",
    "multimul = myex.register_operator('multimul', like=multimul_like, fn=multimul_impl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0687, -0.7520],\n",
       "         [ 0.1435, -0.1413]]),\n",
       " tensor([[-0.0687, -0.7520],\n",
       "         [ 0.1435, -0.1413]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of calling the new multimul symbol\n",
    "def foo(a, b, c, d):\n",
    "    return multimul(a, b, c, d)\n",
    "\n",
    "cfoo = thunder.compile(foo, executors_list=[myex])\n",
    "cfoo(a, b, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 0 milliseconds)\n",
       "import torch\n",
       "\n",
       "@torch.no_grad()\n",
       "def foo(a, b, c, d):\n",
       "  # a: \"cpu f32[2, 2]\" \n",
       "  # b: \"cpu f32[2, 2]\" \n",
       "  # c: \"cpu f32[2, 2]\" \n",
       "  # d: \"cpu f32[2, 2]\" \n",
       "  (t0, t1) = multimul(a, b, c, d)\n",
       "    # t0 = ltorch.mul(a, b)  # t0: \"cpu f32[2, 2]\"\n",
       "      # t0 = prims.mul(a, b)  # t0: \"cpu f32[2, 2]\"\n",
       "    # t1 = ltorch.mul(c, d)  # t1: \"cpu f32[2, 2]\"\n",
       "      # t1 = prims.mul(c, d)  # t1: \"cpu f32[2, 2]\"\n",
       "  del (a, b, c, d)\n",
       "  return (t0, t1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The symbol is recorded, like other operations, into thunder's trace\n",
    "thunder.last_traces(cfoo)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Constructed by Delete Last Used (took 0 milliseconds)\n",
      "import torch\n",
      "\n",
      "@torch.no_grad()\n",
      "def foo(a, b, c, d):\n",
      "  # b: \"cpu f32[2, 2]\" \n",
      "  t10 = torch.full((2, 2), 1, device=\"cpu\", dtype=torch.float32)  # t10: \"cpu f32[2, 2]\"\n",
      "    # t10 = ltorch.full((2, 2), 1, device=\"cpu\", dtype=torch.float32)  # t10: \"cpu f32[2, 2]\"\n",
      "      # t10 = prims.full((2, 2), 1, device=devices.Device(\"cpu\"), dtype=dtypes.float32)  # t10: \"cpu f32[2, 2]\"\n",
      "  t4 = torch.mul(b, t10)  # t4: \"cpu f32[2, 2]\"\n",
      "    # t4 = ltorch.mul(b, t10)  # t4: \"cpu f32[2, 2]\"\n",
      "      # t4 = prims.mul(b, t10)  # t4: \"cpu f32[2, 2]\"\n",
      "  del b\n",
      "  # a: \"cpu f32[2, 2]\" \n",
      "  t5 = torch.mul(a, t10)  # t5: \"cpu f32[2, 2]\"\n",
      "    # t5 = ltorch.mul(a, t10)  # t5: \"cpu f32[2, 2]\"\n",
      "      # t5 = prims.mul(a, t10)  # t5: \"cpu f32[2, 2]\"\n",
      "  del (a, t10)\n",
      "  # d: \"cpu f32[2, 2]\" \n",
      "  t11 = torch.full((2, 2), 1, device=\"cpu\", dtype=torch.float32)  # t11: \"cpu f32[2, 2]\"\n",
      "    # t11 = ltorch.full((2, 2), 1, device=\"cpu\", dtype=torch.float32)  # t11: \"cpu f32[2, 2]\"\n",
      "      # t11 = prims.full((2, 2), 1, device=devices.Device(\"cpu\"), dtype=dtypes.float32)  # t11: \"cpu f32[2, 2]\"\n",
      "  t8 = torch.mul(d, t11)  # t8: \"cpu f32[2, 2]\"\n",
      "    # t8 = ltorch.mul(d, t11)  # t8: \"cpu f32[2, 2]\"\n",
      "      # t8 = prims.mul(d, t11)  # t8: \"cpu f32[2, 2]\"\n",
      "  del d\n",
      "  # c: \"cpu f32[2, 2]\" \n",
      "  t9 = torch.mul(c, t11)  # t9: \"cpu f32[2, 2]\"\n",
      "    # t9 = ltorch.mul(c, t11)  # t9: \"cpu f32[2, 2]\"\n",
      "      # t9 = prims.mul(c, t11)  # t9: \"cpu f32[2, 2]\"\n",
      "  del (c, t11)\n",
      "  return [t4, t5, t8, t9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0768,  0.9247],\n",
       "        [ 0.3841, -0.4571]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multimul is even differentiable because its \"like\" function is differentiable\n",
    "a.requires_grad_(True)\n",
    "b.requires_grad_(True)\n",
    "\n",
    "cfoo_grad = grad(cfoo)\n",
    "cfoo_grad(a, b, a, b)\n",
    "print(thunder.last_traces(cfoo_grad)[-1])\n",
    "\n",
    "a.requires_grad_(False)\n",
    "b.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can tell thunder to execute existing operations using multimul by defining a transform\n",
    "#   from them to multimul, and a \"checker\" function that returns True when the \n",
    "#   transform is valid and False otherwise.\n",
    "\n",
    "# We can translate mul to multimul by ignoring the second multiplication\n",
    "def mul_to_multimul(a: Number | TensorProxy, b: Number | TensorProxy) -> TensorProxy:\n",
    "    result, _ = multimul(a, b, 0, 0)\n",
    "    return result\n",
    "\n",
    "# The \"checker\" function verifies that all inputs are CPU tensors or numbers, because NumPy\n",
    "#   can't handle other inputs\n",
    "def mul_to_multimul_checker(a: Number | TensorProxy, b: Number | TensorProxy) -> bool:\n",
    "    def is_cpu(x: Number | TensorProxy) -> bool:\n",
    "        if isinstance(a, TensorProxy):\n",
    "            return a.device.devicetype == DeviceType.CPU\n",
    "        return True\n",
    "\n",
    "    return all(is_cpu(x) for x in (a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The \"register_implementation\" method describes how to translate mul to multimul\n",
    "myex.register_implementation(ltorch.mul, checker=mul_to_multimul_checker, execution_transform=mul_to_multimul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 0 milliseconds)\n",
       "import torch\n",
       "\n",
       "@torch.no_grad()\n",
       "def bar(a, b):\n",
       "  # a: \"cpu f32[2, 2]\" \n",
       "  # b: \"cpu f32[2, 2]\" \n",
       "  (t0, _) = multimul(a, b, 0, 0)\n",
       "    # t0 = ltorch.mul(a, b)  # t0: \"cpu f32[2, 2]\"\n",
       "      # t0 = prims.mul(a, b)  # t0: \"cpu f32[2, 2]\"\n",
       "  del (a, b)\n",
       "  return t0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifies the implementation of mul using multimul, and shows the execution transform\n",
    "def bar(a, b):\n",
    "    return a * b\n",
    "cbar = thunder.compile(bar, executors_list=[myex])\n",
    "cbar(a, b)\n",
    "thunder.last_traces(cbar)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 0 milliseconds)\n",
       "import torch\n",
       "\n",
       "@torch.no_grad()\n",
       "def bar(a, b):\n",
       "  # b: \"cpu f32[2, 2]\" \n",
       "  t5 = torch.full((2, 2), 1, device=\"cpu\", dtype=torch.float32)  # t5: \"cpu f32[2, 2]\"\n",
       "    # t5 = ltorch.full((2, 2), 1, device=\"cpu\", dtype=torch.float32)  # t5: \"cpu f32[2, 2]\"\n",
       "      # t5 = prims.full((2, 2), 1, device=devices.Device(\"cpu\"), dtype=dtypes.float32)  # t5: \"cpu f32[2, 2]\"\n",
       "  (t3, _) = multimul(b, t5, 0, 0)\n",
       "    # t3 = ltorch.mul(b, t5)  # t3: \"cpu f32[2, 2]\"\n",
       "      # t3 = prims.mul(b, t5)  # t3: \"cpu f32[2, 2]\"\n",
       "  del b\n",
       "  # a: \"cpu f32[2, 2]\" \n",
       "  (t4, _) = multimul(a, t5, 0, 0)\n",
       "    # t4 = ltorch.mul(a, t5)  # t4: \"cpu f32[2, 2]\"\n",
       "      # t4 = prims.mul(a, t5)  # t4: \"cpu f32[2, 2]\"\n",
       "  del (a, t5)\n",
       "  return [t3, t4]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execution transforms happen AFTER semantic transforms like grad, so even when computing the grad\n",
    "#   of mul (which involves two multiplications to compute the grad) we still see multimul in the\n",
    "#   execution trace\n",
    "a.requires_grad_(True)\n",
    "b.requires_grad_(True)\n",
    "\n",
    "cbar_grad = grad(cbar)\n",
    "cbar_grad(a, b)\n",
    "thunder.last_traces(cbar_grad)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the above grad trace there are two multimuls, and both ignore one of their multiplications.\n",
    "#   It would be more efficient to perform just one multimul, and we can make this happen\n",
    "#   by defining a new grad transform for mul that calls multimul once.\n",
    "#   thunder's grad transforms are defined in a novel way that's not the focus of this notebook,\n",
    "#   but below we define the grad transform to use multimul.\n",
    "def mymul_grad(a: TensorProxy, b: TensorProxy) -> TensorProxy:\n",
    "    fwd = a * b\n",
    "\n",
    "    g = get_grad(fwd)\n",
    "    a_grad, b_grad = multimul(b, g, a, g)\n",
    "    put_grads((a, b), (a_grad, b_grad))\n",
    "\n",
    "    return fwd\n",
    "\n",
    "# Re-registers the implementation, including the execution transform and now a grad transform\n",
    "myex.register_implementation(ltorch.mul, checker=mul_to_multimul_checker, execution_transform=mul_to_multimul, grad_transform=mymul_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 0 milliseconds)\n",
       "import torch\n",
       "\n",
       "@torch.no_grad()\n",
       "def bar(a, b):\n",
       "  # a: \"cpu f32[2, 2]\" \n",
       "  t5 = torch.full((2, 2), 1, device=\"cpu\", dtype=torch.float32)  # t5: \"cpu f32[2, 2]\"\n",
       "    # t5 = ltorch.full((2, 2), 1, device=\"cpu\", dtype=torch.float32)  # t5: \"cpu f32[2, 2]\"\n",
       "      # t5 = prims.full((2, 2), 1, device=devices.Device(\"cpu\"), dtype=dtypes.float32)  # t5: \"cpu f32[2, 2]\"\n",
       "  # b: \"cpu f32[2, 2]\" \n",
       "  (t3, t4) = multimul(b, t5, a, t5)\n",
       "    # t3 = ltorch.mul(b, t5)  # t3: \"cpu f32[2, 2]\"\n",
       "      # t3 = prims.mul(b, t5)  # t3: \"cpu f32[2, 2]\"\n",
       "    # t4 = ltorch.mul(a, t5)  # t4: \"cpu f32[2, 2]\"\n",
       "      # t4 = prims.mul(a, t5)  # t4: \"cpu f32[2, 2]\"\n",
       "  del (b, t5, a)\n",
       "  return [t3, t4]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifies our new grad transform is used and that a single multimul call is made\n",
    "cbar_grad = grad(cbar)\n",
    "cbar_grad(a, b)\n",
    "thunder.last_traces(cbar_grad)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some operations may require inputs have particular properties (like be contiguous), or a transform may wish\n",
    "#   to interleave torch operations with new operations. The transform function supports this. Here\n",
    "#   we can see an example where the inputs to multimul are made contiguous before it's called\n",
    "def mul_to_contiguous_multimul(a: Number | TensorProxy, b: Number | TensorProxy) -> TensorProxy:\n",
    "    a = a.contiguous()\n",
    "    b = b.contiguous()\n",
    "    result, _ = multimul(a, b, 0, 0)\n",
    "    return result\n",
    "\n",
    "myex.register_implementation(ltorch.mul, checker=mul_to_multimul_checker, execution_transform=mul_to_contiguous_multimul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 0 milliseconds)\n",
       "from torch import Tensor\n",
       "import torch\n",
       "\n",
       "@torch.no_grad()\n",
       "def caz(a, b):\n",
       "  # a: \"cpu f32[2, 2]\" \n",
       "  # b: \"cpu f32[2, 2]\" \n",
       "  t1 = Tensor.contiguous(a, memory_format=_torch_memory_format_0)  # t1: \"cpu f32[2, 2]\"\n",
       "    # t1 = ltorch.contiguous(a, memory_format=_torch_memory_format_0)  # t1: \"cpu f32[2, 2]\"\n",
       "      # t1 = prims.stride_order(a, (1, 0))  # t1: \"cpu f32[2, 2]\"\n",
       "  del a\n",
       "  t2 = Tensor.contiguous(b, memory_format=_torch_memory_format_0)  # t2: \"cpu f32[2, 2]\"\n",
       "    # t2 = ltorch.contiguous(b, memory_format=_torch_memory_format_0)  # t2: \"cpu f32[2, 2]\"\n",
       "      # t2 = prims.stride_order(b, (1, 0))  # t2: \"cpu f32[2, 2]\"\n",
       "  del b\n",
       "  (t0, _) = multimul(t1, t2, 0, 0)\n",
       "    # t0 = ltorch.mul(t1, t2)  # t0: \"cpu f32[2, 2]\"\n",
       "      # t0 = prims.mul(t1, t2)  # t0: \"cpu f32[2, 2]\"\n",
       "  del (t1, t2)\n",
       "  return t0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifies the new \"prologue\" for multimul works as expected. Note that the contiguous operations are \n",
    "#   executed by PyTorch, and don't have to be executed by your executor\n",
    "a.requires_grad_(False)\n",
    "b.requires_grad_(False)\n",
    "\n",
    "def caz(a, b):\n",
    "    return a * b\n",
    "ccaz = thunder.compile(caz, executors_list=[myex])\n",
    "ccaz(a, b)\n",
    "thunder.last_traces(ccaz)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVIDIA's APEX cross-entropy executor is a good example of a real-world operator executor. It defines\n",
    "#   fast forward and backward functions for torch.nn.functional.cross_entropy. We can see its custom\n",
    "#   fwd and bwd operations below\n",
    "# NOTE This cell and the following cells require the apex executor be installed to run properly\n",
    "dtype = torch.float32\n",
    "device = 'cuda'\n",
    "logits = torch.randn([2048, 50257], device=device, dtype=ltorch.to_torch_dtype(dtype), requires_grad=False)\n",
    "labels = torch.randint(0, 50257, [2048], device=device)\n",
    "\n",
    "from thunder.executors.apex_entropyex import apex_ex\n",
    "\n",
    "def foo(logits, labels):\n",
    "    return torch.nn.functional.cross_entropy(logits, labels, reduction=\"mean\", ignore_index=-1)\n",
    "cfoo = thunder.compile(foo, executors_list=[apex_ex])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 0 milliseconds)\n",
       "import torch\n",
       "\n",
       "@torch.no_grad()\n",
       "def foo(logits, labels):\n",
       "  # logits: \"cuda:0 f32[2048, 50257]\" \n",
       "  # labels: \"cuda:0 i64[2048]\" \n",
       "  (t18, t20) = apex_cross_entropy(logits, labels, \"mean\", 0.0)\n",
       "  del (t20, logits, labels)\n",
       "  return t18"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows the forward operation\n",
    "cfoo(logits, labels)\n",
    "thunder.last_traces(cfoo)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 0 milliseconds)\n",
       "from torch import Tensor\n",
       "import torch\n",
       "\n",
       "@torch.no_grad()\n",
       "def foo(logits, labels):\n",
       "  # logits: \"cuda:0 f32[2048, 50257]\" \n",
       "  # labels: \"cuda:0 i64[2048]\" \n",
       "  (_, t20) = apex_cross_entropy(logits, labels, \"mean\", 0.0)\n",
       "  t25 = Tensor.contiguous(logits, memory_format=_torch_memory_format_0)  # t25: \"cuda:0 f32[2048, 50257]\"\n",
       "    # t25 = ltorch.contiguous(logits, memory_format=_torch_memory_format_0)  # t25: \"cuda:0 f32[2048, 50257]\"\n",
       "      # t25 = prims.stride_order(logits, (1, 0))  # t25: \"cuda:0 f32[2048, 50257]\"\n",
       "  del logits\n",
       "  t27 = torch.full((), 1, device=\"cuda:0\", dtype=torch.float32)  # t27: \"cuda:0 f32[]\"\n",
       "    # t27 = ltorch.full((), 1, device=\"cuda:0\", dtype=torch.float32)  # t27: \"cuda:0 f32[]\"\n",
       "      # t27 = prims.full((), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t27: \"cuda:0 f32[]\"\n",
       "  t31 = torch.unsqueeze(t27, 0)  # t31: \"cuda:0 f32[1]\"\n",
       "    # t31 = ltorch.unsqueeze(t27, 0)  # t31: \"cuda:0 f32[1]\"\n",
       "      # t31 = prims.broadcast_in_dim(t27, [1], [])  # t31: \"cuda:0 f32[1]\"\n",
       "  del t27\n",
       "  t22 = Tensor.expand(t31, [1])  # t22: \"cuda:0 f32[1]\"\n",
       "    # t22 = ltorch.expand(t31, [1])  # t22: \"cuda:0 f32[1]\"\n",
       "      # t22 = prims.broadcast_in_dim(t31, [1], (0,))  # t22: \"cuda:0 f32[1]\"\n",
       "  del t31\n",
       "  t23 = Tensor.expand(t22, [2048])  # t23: \"cuda:0 f32[2048]\"\n",
       "    # t23 = ltorch.expand(t22, [2048])  # t23: \"cuda:0 f32[2048]\"\n",
       "      # t23 = prims.broadcast_in_dim(t22, [2048], (0,))  # t23: \"cuda:0 f32[2048]\"\n",
       "  del t22\n",
       "  t24 = torch.mul(t23, 0.00048828125)  # t24: \"cuda:0 f32[2048]\"\n",
       "    # t24 = ltorch.mul(t23, 0.00048828125)  # t24: \"cuda:0 f32[2048]\"\n",
       "      # t24 = prims.mul(t23, 0.00048828125)  # t24: \"cuda:0 f32[2048]\"\n",
       "  del t23\n",
       "  t26 = apex_cross_entropy_backward(t24, t25, target=labels, max_log_sum_exp=t20, label_smoothing=0.0)  # t26: \"cuda:0 f32[2048, 50257]\"\n",
       "  del (t24, t25, t20, labels)\n",
       "  return [t26]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shows APEX's custom forward and backward operations, plus additional PyTorch operations between the two\n",
    "logits.requires_grad_(True)\n",
    "\n",
    "cfoo_grad = grad(cfoo)\n",
    "cfoo_grad(logits, labels)\n",
    "thunder.last_traces(cfoo_grad)[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3109",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
