trigger:
  tags:
    include: ['*']
  branches:
    include:
      - "main"
      - "release/*"
      - "refs/tags/*"

pr:
  branches:
    include: ['*']

jobs:
  - job: testing
    strategy:
      matrix:
        # todo: re-enable after PT 2.1 is out
        #'cuda 11.8 | stable':
        #  docker-image: 'pytorchlightning/lightning-thunder:ubuntu-cuda11.8.0-py3.10'
        #  agent-pool: 'lit-rtx-3090'
        #  CUDA_VERSION_MM: '118'
        'ubuntu22.04 | cuda 11.8 | python 3.10 | torch-RC':
          docker-image: 'pytorchlightning/lightning-thunder:ubuntu22.04-cuda11.8.0-py3.10'
          agent-pool:
          versions: 'RC'
          CUDA_VERSION_MM: '118'
        'ubuntu22.04 | cuda 11.8 | python 3.10 | torch-nightly':
          docker-image: 'pytorchlightning/lightning-thunder:ubuntu22.04-cuda11.8.0-py3.10'
          versions: 'nightly'
          CUDA_VERSION_MM: '118'
        # todo: re-enable after PT 2.1 is out
        #'cuda 12.1 | stable':
        #  docker-image: 'pytorchlightning/lightning-thunder:ubuntu22.04-cuda12.1.1-py3.10'
        #  agent-pool: 'lit-rtx-3090'
        #  CUDA_VERSION_MM: '121'
        'ubuntu22.04 | cuda 12.1 | python 3.10 | torch-RC':
          docker-image: 'pytorchlightning/lightning-thunder:ubuntu22.04-cuda12.1.1-py3.10'
          versions: 'RC'
          CUDA_VERSION_MM: '121'
        'ubuntu22.04 | cuda 12.1 | python 3.10 | torch-nightly':
          docker-image: 'pytorchlightning/lightning-thunder:ubuntu22.04-cuda12.1.1-py3.10'
          versions: 'nightly'
          CUDA_VERSION_MM: '121'
    # how long to run the job before automatically cancelling
    timeoutInMinutes: "35"
    # how much time to give 'run always even if cancelled tasks' before stopping them
    cancelTimeoutInMinutes: "2"
    pool: "lit-rtx-3090"
    variables:
      DEVICES: $( python -c 'name = "$(Agent.Name)" ; gpus = name.split("_")[-1] if "_" in name else "0"; print(gpus)' )
      TORCH_HOME: "/var/tmp/torch"
      PIP_CACHE_DIR: "/var/tmp/pip"
      PYTHONHASHSEED: "0"
      CI: "true"
    container:
      image: "$(docker-image)"
      options: "--gpus=all --shm-size=16g -v /usr/bin/docker:/tmp/docker:ro -v /var/tmp:/var/tmp"
    workspace:
      clean: all
    steps:

    - bash: |
        echo $(DEVICES)
        echo "CUDA_VERSION_MM=$CUDA_VERSION_MM"
        lspci | egrep 'VGA|3D'
        whereis nvidia
        nvidia-smi
        which python && which pip
        python --version
        pip --version
        pip list
        echo "##vso[task.setvariable variable=CUDA_VISIBLE_DEVICES]$(DEVICES)"
      displayName: 'Image info & NVIDIA'

    - bash: |
        pip install torch triton -U --pre -f "https://download.pytorch.org/whl/nightly/cu$CUDA_VERSION_MM/torch_nightly.html"
      condition: eq(variables['versions'], 'nightly')
      displayName: 'install PyTorch nightly'

    - bash: |
        pip install torch triton -U -f "https://download.pytorch.org/whl/test/cu$CUDA_VERSION_MM/torch_test.html"
      condition: eq(variables['versions'], 'RC')
      displayName: 'install PyTorch RC'

    - bash: |
        # pip install -e .[test] -U \
        #   --pre -f https://download.pytorch.org/whl/nightly/cu117/torch_nightly.html
        pip install -r requirements/test.txt
        python setup.py develop
      displayName: 'Install package & ...'
    - bash: |
        pip install "nvfuser-cu$CUDA_VERSION_MM" --pre
        patch-nvfuser
      condition: ne(variables['versions'], 'stable')
      displayName: 'Overwrite nvFuser'

    - bash: |
        set -e
        pip list
        python -c "import torch ; assert torch.cuda.is_available(), 'missing GPU'"
        python -c "import torch ; v = torch.__version__ ; assert str(v).startswith('2'), v"
      displayName: 'Sanity check / details'

    - bash: |
        set -e
        coverage run --source thunder -m \
          pytest thunder/tests/ -v --random-order-seed=42 --durations=250 --numprocesses=9 \
            --ignore="thunder/tests/test_networks.py"
        # these test ned to run in single thread as they occurs with CUDA OOM
        coverage run --source thunder -m \
          pytest \
            thunder/tests/test_networks.py \
            -v --random-order-seed=42 --durations=250 --numprocesses=3
      displayName: 'Testing'

    # todo: enable when we will care about coverage
    #- bash: |
    #    coverage report
    #    coverage xml
    #
    #    # https://docs.codecov.com/docs/codecov-uploader
    #    curl -Os https://uploader.codecov.io/latest/linux/codecov
    #    chmod +x codecov
    #    ./codecov --token=$(CODECOV_TOKEN) --commit=$(Build.SourceVersion) \
    #      --flags=unittests,gpu,cu$CUDA_VERSION_MM,$(versions), --name="codecov-gpu" --env=OS,PYTHON
    #  displayName: 'Statistics'

    # todo for Mike as he promised some time ago already
    #- bash: |
    #     python benchmarks/ops_benchmark.py nanogpt-gelu
    #     python benchmarks/nvfuser_benchmarks.py nanogpt-mlp -x thunder
    #     python benchmarks/nvfuser_benchmarks.py nanogpt-gelu -x thunder
    #  displayName: 'Benchmarks'
