{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FSDP Tutorial\n",
    "\n",
    "In this tutorial, we will walk through the implementation of Fully Sharded Data Parallel (FSDP) with Zero2 sharding strategy in `thunder`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction\n",
    "\n",
    "In recent times, the LLM models have grown so large that all the model parameters don't fit on a single GPU. To circumvent this problem, there are various strategies like Tensor Parallel, Pipeline Parallel, Fully Sharded Data Parallel, etc to train these large models. In this tutorial, we discuss and implement Zero2 strategy for Fully Sharded Data Parallel (FSDP).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Zero2 strategy for FSDP?\n",
    "\n",
    "In this strategy, we shard the model parameters across all the availabe GPUs. That is each GPU holds onto only a chunk of the parameter. During the forward pass, all GPUs call `all_gather` communication primitive to gather the parameters from other GPUs. Unlike Zero3 strategy which frees the parameter after forward pass, we save these unsharded parameters for backward pass. This is to save the overhead of extra communication. In the backward pass, we utilize the saved parameters and compute the gradients. Once the gradients are computed, we use `reduce_scatter` communication primitive to reduce (average) the gradients across all GPUs and scatter those gradients so that a given GPU holds only a chunk of gradient.\n",
    "\n",
    "For more information on FSDP, we recommend reading\n",
    "1. PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel - [Link](https://arxiv.org/abs/2304.11277)\n",
    "2. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models - [Link](https://arxiv.org/abs/1910.02054)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.distributed\n",
    "import thunder\n",
    "import thunder.distributed\n",
    "from IPython.display import Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example we will have a simple model `Linear(Tanh(Linear(x)))` which will be sharded over 2 GPUs\n",
    "\n",
    "**NOTE**: We are generating the abstract trace so we don't actually need a system with 2 GPUs for this. It is only required when we execute this trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "dim = 64\n",
    "def create_model():\n",
    "    layers = [torch.nn.Linear(dim, dim, bias=False),\n",
    "              torch.nn.Tanh(),\n",
    "              torch.nn.Linear(dim, dim, bias=False)]\n",
    "    return torch.nn.Sequential(*layers).to(device)\n",
    "\n",
    "# Model\n",
    "model = create_model()\n",
    "# Input\n",
    "x = torch.randn(dim, dim, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_as_highlighted_code(trace):\n",
    "    return Code(str(trace), language=\"python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1 : Configuration\n",
    "\n",
    "For our implementation of FSDP, we will generate the trace where we are sharding our model over 2 GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FSDP Config \n",
    "# Usually these values are set in the environment by `torchrun` but for this example\n",
    "# we will set them ourselves\n",
    "world_size = 2  # We have two processes.\n",
    "global_rank = 0  # Current process is the very first process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Function to shard parameters\n",
    "\n",
    "Next step is to write a function which will actually shard the parameters over 0-dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: We shard over 0th dimension of the param.\n",
    "def shard_param(param: torch.Tensor, rank: int, world_size: int, name: str) -> None:\n",
    "    # We will keep it simple and error if param's 0th dim is not divisible by ``world_size``.\n",
    "    # Alternative is that we can pad our parameters so that they are divisible by `world_size`.\n",
    "    assert param.shape[0] % world_size == 0,(\n",
    "        f\"Current sharding requires the first dimension of the parameter {name!r} ({param.shape[0]})\"\n",
    "        f\" to be divisible by the world size ({world_size})\"\n",
    "    )\n",
    "    chunk_size = param.shape[0] // world_size\n",
    "\n",
    "    # rank helps us determine which chunk of the parameter we will hold.\n",
    "    shard = param.data.narrow(0, chunk_size * rank, chunk_size).clone()\n",
    "    param.data = shard\n",
    "\n",
    "# Shard each parameter of the model\n",
    "for param_name, param in model.named_parameters():\n",
    "    shard_param(param, global_rank, world_size, param_name)\n",
    "    # Mark the param to denote that it is sharded.\n",
    "    # This is required by the synchronization primitive we will use below.\n",
    "    param.ddp_type = thunder.core.proxies.DDPType.FULLY_SHARDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=64, out_features=64, bias=False)\n",
       "  (1): Tanh()\n",
       "  (2): Linear(in_features=64, out_features=64, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify our model looks as expected\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us verify that we have actually sharded the parameters.\n",
    "# Checking if the weight of 1st Linear layer is sharded over 0th dim.\n",
    "assert model[0].weight.shape == (dim / world_size, dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Add an operation to synchronize the parameters before calling the model.forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create a process group. This is needed because the synchronization primitive `synchronize` that we will use to gather and scatter our weights in forward and backward requires a process group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a process group\n",
    "options = torch.distributed.distributed_c10d.ProcessGroup.Options(backend=\"nccl\")\n",
    "process_group = torch.distributed.distributed_c10d.ProcessGroup(torch.distributed.distributed_c10d.Store(),\n",
    "                                                     global_rank, world_size, options)\n",
    "torch.distributed.distributed_c10d.GroupMember.WORLD = process_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `preprocess` gives us the functional version of the model which\n",
    "# takes as inputs all the parameters and the expected arguments.\n",
    "# NOTE: `thunder.common.preprocess` is not meant for general use\n",
    "#       and used only for brevity of code. It will be updated\n",
    "#       to a newer mechanism which is meant to be public facing. \n",
    "functional_forward = thunder.common.preprocess(model, is_module=True)\n",
    "\n",
    "# This function creates a model with synchronization\n",
    "# before calling the forward pass.\n",
    "def model_with_syncs(*params, x):\n",
    "    # We call `prims.synchronize` on all the parameters.\n",
    "    # This is essentially calling `all_gather` so that we have the complete\n",
    "    # parameter before we actually to the forward computation.\n",
    "    unsharded_params = []\n",
    "    for param in params:\n",
    "        unsharded_params.append(thunder.distributed.prims.synchronize(param, process_group))\n",
    "\n",
    "    return functional_forward(*unsharded_params, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now see what the trace of our model looks like with all the synchronization.\n",
    "\n",
    "Two main observations regarding the below trace \n",
    "1. We can observe the `prims.synchronize` that we inserted using `model_with_syncs`.\n",
    "2. Output of the `prims.synchronize` have the shape of unsharded (original) parameter.\n",
    "\n",
    "With this, we have implemented the FSDP for the forward pass of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\"># Constructed by Dead Code Elimination (took 0 milliseconds)</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder.distributed.prims</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder.torch</span> <span class=\"k\">as</span> <span class=\"nn\">ltorch</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">thunder.executors.torchex</span> <span class=\"kn\">import</span> <span class=\"n\">no_autocast</span>\n",
       "\n",
       "<span class=\"nd\">@torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">()</span>\n",
       "<span class=\"nd\">@no_autocast</span><span class=\"p\">()</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">model_with_syncs</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">params</span><span class=\"p\">,</span> <span class=\"n\">x</span><span class=\"p\">):</span>\n",
       "  <span class=\"c1\"># params </span>\n",
       "  <span class=\"c1\"># x </span>\n",
       "  <span class=\"n\">t0</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"n\">t1</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"o\">=</span> <span class=\"n\">params</span>\n",
       "  <span class=\"n\">t2</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">synchronize</span><span class=\"p\">(</span><span class=\"n\">t0</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_0</span><span class=\"p\">)</span>  <span class=\"c1\"># t2</span>\n",
       "  <span class=\"n\">t3</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">synchronize</span><span class=\"p\">(</span><span class=\"n\">t1</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_0</span><span class=\"p\">)</span>  <span class=\"c1\"># t3</span>\n",
       "  <span class=\"n\">t4</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">x</span><span class=\"p\">,</span> <span class=\"n\">t2</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>  <span class=\"c1\"># t4</span>\n",
       "    <span class=\"c1\"># t4 = prims.linear(x, t2, None)  # t4</span>\n",
       "  <span class=\"n\">t5</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">tanh</span><span class=\"p\">(</span><span class=\"n\">t4</span><span class=\"p\">)</span>  <span class=\"c1\"># t5</span>\n",
       "    <span class=\"c1\"># t5 = prims.tanh(t4)  # t5</span>\n",
       "  <span class=\"n\">t6</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">t5</span><span class=\"p\">,</span> <span class=\"n\">t3</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>  <span class=\"c1\"># t6</span>\n",
       "    <span class=\"c1\"># t6 = prims.linear(t5, t3, None)  # t6</span>\n",
       "  <span class=\"k\">return</span> <span class=\"n\">t6</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{} Constructed by Dead Code Elimination (took 0 milliseconds)}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{distributed}\\PY{n+nn}{.}\\PY{n+nn}{prims}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{torch} \\PY{k}{as} \\PY{n+nn}{ltorch}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{executors}\\PY{n+nn}{.}\\PY{n+nn}{torchex} \\PY{k+kn}{import} \\PY{n}{no\\PYZus{}autocast}\n",
       "\n",
       "\\PY{n+nd}{@torch}\\PY{o}{.}\\PY{n}{no\\PYZus{}grad}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{n+nd}{@no\\PYZus{}autocast}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{k}{def} \\PY{n+nf}{model\\PYZus{}with\\PYZus{}syncs}\\PY{p}{(}\\PY{o}{*}\\PY{n}{params}\\PY{p}{,} \\PY{n}{x}\\PY{p}{)}\\PY{p}{:}\n",
       "  \\PY{c+c1}{\\PYZsh{} params }\n",
       "  \\PY{c+c1}{\\PYZsh{} x }\n",
       "  \\PY{n}{t0}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{n}{t1}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{o}{=} \\PY{n}{params}\n",
       "  \\PY{n}{t2} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{synchronize}\\PY{p}{(}\\PY{n}{t0}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}0}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t2}\n",
       "  \\PY{n}{t3} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{synchronize}\\PY{p}{(}\\PY{n}{t1}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}0}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t3}\n",
       "  \\PY{n}{t4} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{linear}\\PY{p}{(}\\PY{n}{x}\\PY{p}{,} \\PY{n}{t2}\\PY{p}{,} \\PY{k+kc}{None}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t4}\n",
       "    \\PY{c+c1}{\\PYZsh{} t4 = prims.linear(x, t2, None)  \\PYZsh{} t4}\n",
       "  \\PY{n}{t5} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{tanh}\\PY{p}{(}\\PY{n}{t4}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t5}\n",
       "    \\PY{c+c1}{\\PYZsh{} t5 = prims.tanh(t4)  \\PYZsh{} t5}\n",
       "  \\PY{n}{t6} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{linear}\\PY{p}{(}\\PY{n}{t5}\\PY{p}{,} \\PY{n}{t3}\\PY{p}{,} \\PY{k+kc}{None}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t6}\n",
       "    \\PY{c+c1}{\\PYZsh{} t6 = prims.linear(t5, t3, None)  \\PYZsh{} t6}\n",
       "  \\PY{k}{return} \\PY{n}{t6}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "# Constructed by Dead Code Elimination (took 0 milliseconds)\n",
       "import thunder\n",
       "import thunder.distributed.prims\n",
       "import thunder.torch as ltorch\n",
       "import torch\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast()\n",
       "def model_with_syncs(*params, x):\n",
       "  # params \n",
       "  # x \n",
       "  t0, \\\n",
       "  t1, \\\n",
       "  = params\n",
       "  t2 = thunder.distributed.prims.synchronize(t0, _torch_distributed_distributed_c10d_ProcessGroup_0)  # t2\n",
       "  t3 = thunder.distributed.prims.synchronize(t1, _torch_distributed_distributed_c10d_ProcessGroup_0)  # t3\n",
       "  t4 = ltorch.linear(x, t2, None)  # t4\n",
       "    # t4 = prims.linear(x, t2, None)  # t4\n",
       "  t5 = ltorch.tanh(t4)  # t5\n",
       "    # t5 = prims.tanh(t4)  # t5\n",
       "  t6 = ltorch.linear(t5, t3, None)  # t6\n",
       "    # t6 = prims.linear(t5, t3, None)  # t6\n",
       "  return t6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace = thunder.trace()(model_with_syncs, *model.parameters(), x=x)\n",
    "\n",
    "wrap_as_highlighted_code(trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For backward, we don't have to do anything because `thunder` already knows how to compute the backward of `prims.synchronize`. We can verify that by using the `value_and_grad` transform to generate the complete forward and backward trace together.\n",
    "\n",
    "Observations for the trace below:\n",
    "1. `prims.synchronize` from previous trace is now decomposed into `prims.all_gather` and `prims.wait`. So, we can clearly see that we make a communication call to gather the parameter (which is asynchronous) and wait till we have the complete parameter.\n",
    "2. At the end of the trace (after the forward and the backward computation), we see calls to `prims.reduce_scatter` and `prims.wait`. This takes care of reducing the gradients across all the GPUs and sharding them. One thing to note, for averaging gradients with low dynamic range dtype like `float16`, if we naively sum the gradients across GPUs before dividing by `world_size`, it can lead to overflows. So we scale the gradient tensor with `world_size`, before calling `reduce_scatter` with `sum` reduction to effectively average the gradients without overflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\"># Constructed by Dead Code Elimination (took 0 milliseconds)</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder.core.devices</span> <span class=\"k\">as</span> <span class=\"nn\">devices</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder.core.dtypes</span> <span class=\"k\">as</span> <span class=\"nn\">dtypes</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder.core.prims</span> <span class=\"k\">as</span> <span class=\"nn\">prims</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder.distributed.prims</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">thunder.torch</span> <span class=\"k\">as</span> <span class=\"nn\">ltorch</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">thunder.executors.torchex</span> <span class=\"kn\">import</span> <span class=\"n\">no_autocast</span>\n",
       "\n",
       "<span class=\"nd\">@torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">()</span>\n",
       "<span class=\"nd\">@no_autocast</span><span class=\"p\">()</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">_value_and_grad</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n",
       "  <span class=\"c1\"># args </span>\n",
       "  <span class=\"c1\"># kwargs </span>\n",
       "  <span class=\"n\">t0</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"n\">t1</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"o\">=</span> <span class=\"n\">args</span>\n",
       "  <span class=\"n\">t2</span> <span class=\"o\">=</span> <span class=\"n\">kwargs</span><span class=\"p\">[</span><span class=\"s1\">&#39;x&#39;</span><span class=\"p\">]</span>\n",
       "  <span class=\"n\">t3</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">devices</span><span class=\"o\">.</span><span class=\"n\">Device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:0&quot;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">dtypes</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>  <span class=\"c1\"># t3</span>\n",
       "  <span class=\"n\">p4</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">all_gather</span><span class=\"p\">(</span><span class=\"n\">t0</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_0</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>  <span class=\"c1\"># p4</span>\n",
       "  <span class=\"n\">t5</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">wait</span><span class=\"p\">(</span><span class=\"n\">p4</span><span class=\"p\">)</span>  <span class=\"c1\"># t5</span>\n",
       "  <span class=\"n\">p6</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">all_gather</span><span class=\"p\">(</span><span class=\"n\">t1</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_0</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>  <span class=\"c1\"># p6</span>\n",
       "  <span class=\"n\">t7</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">wait</span><span class=\"p\">(</span><span class=\"n\">p6</span><span class=\"p\">)</span>  <span class=\"c1\"># t7</span>\n",
       "  <span class=\"n\">t8</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">t2</span><span class=\"p\">,</span> <span class=\"n\">t5</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>  <span class=\"c1\"># t8</span>\n",
       "  <span class=\"n\">t9</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">tanh</span><span class=\"p\">(</span><span class=\"n\">t8</span><span class=\"p\">)</span>  <span class=\"c1\"># t9</span>\n",
       "  <span class=\"n\">t10</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">t9</span><span class=\"p\">,</span> <span class=\"n\">t7</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>  <span class=\"c1\"># t10</span>\n",
       "  <span class=\"n\">t11</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t3</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">)</span>  <span class=\"c1\"># t11</span>\n",
       "    <span class=\"c1\"># t11 = prims.reshape(t3, (64, 64))  # t11</span>\n",
       "  <span class=\"n\">t12</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">t11</span><span class=\"p\">,</span> <span class=\"n\">t7</span><span class=\"p\">)</span>  <span class=\"c1\"># t12</span>\n",
       "    <span class=\"c1\"># t12 = prims.matmul(t11, t7)  # t12</span>\n",
       "  <span class=\"n\">t13</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t3</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">)</span>  <span class=\"c1\"># t13</span>\n",
       "    <span class=\"c1\"># t13 = prims.reshape(t3, (64, 64))  # t13</span>\n",
       "  <span class=\"n\">t14</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">(</span><span class=\"n\">t13</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>  <span class=\"c1\"># t14</span>\n",
       "  <span class=\"n\">t15</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t9</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">)</span>  <span class=\"c1\"># t15</span>\n",
       "    <span class=\"c1\"># t15 = prims.reshape(t9, (64, 64))  # t15</span>\n",
       "  <span class=\"n\">t16</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">t14</span><span class=\"p\">,</span> <span class=\"n\">t15</span><span class=\"p\">)</span>  <span class=\"c1\"># t16</span>\n",
       "    <span class=\"c1\"># t16 = prims.matmul(t14, t15)  # t16</span>\n",
       "  <span class=\"n\">t17</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">mul</span><span class=\"p\">(</span><span class=\"n\">t9</span><span class=\"p\">,</span> <span class=\"n\">t9</span><span class=\"p\">)</span>  <span class=\"c1\"># t17</span>\n",
       "    <span class=\"c1\"># t17 = prims.mul(t9, t9)  # t17</span>\n",
       "  <span class=\"n\">t18</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">sub</span><span class=\"p\">(</span><span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"n\">t17</span><span class=\"p\">,</span> <span class=\"n\">alpha</span><span class=\"o\">=</span><span class=\"kc\">None</span><span class=\"p\">)</span>  <span class=\"c1\"># t18</span>\n",
       "    <span class=\"c1\"># t18 = prims.sub(1.0, t17)  # t18</span>\n",
       "  <span class=\"n\">t19</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">mul</span><span class=\"p\">(</span><span class=\"n\">t12</span><span class=\"p\">,</span> <span class=\"n\">t18</span><span class=\"p\">)</span>  <span class=\"c1\"># t19</span>\n",
       "    <span class=\"c1\"># t19 = prims.mul(t12, t18)  # t19</span>\n",
       "  <span class=\"n\">t20</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t19</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">)</span>  <span class=\"c1\"># t20</span>\n",
       "    <span class=\"c1\"># t20 = prims.reshape(t19, (64, 64))  # t20</span>\n",
       "  <span class=\"n\">t21</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">t20</span><span class=\"p\">,</span> <span class=\"n\">t5</span><span class=\"p\">)</span>  <span class=\"c1\"># t21</span>\n",
       "    <span class=\"c1\"># t21 = prims.matmul(t20, t5)  # t21</span>\n",
       "  <span class=\"n\">t22</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t19</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">)</span>  <span class=\"c1\"># t22</span>\n",
       "    <span class=\"c1\"># t22 = prims.reshape(t19, (64, 64))  # t22</span>\n",
       "  <span class=\"n\">t23</span> <span class=\"o\">=</span> <span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">transpose</span><span class=\"p\">(</span><span class=\"n\">t22</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>  <span class=\"c1\"># t23</span>\n",
       "  <span class=\"n\">t24</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t2</span><span class=\"p\">,</span> <span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">)</span>  <span class=\"c1\"># t24</span>\n",
       "    <span class=\"c1\"># t24 = prims.reshape(t2, (64, 64))  # t24</span>\n",
       "  <span class=\"n\">t25</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">t23</span><span class=\"p\">,</span> <span class=\"n\">t24</span><span class=\"p\">)</span>  <span class=\"c1\"># t25</span>\n",
       "    <span class=\"c1\"># t25 = prims.matmul(t23, t24)  # t25</span>\n",
       "  <span class=\"n\">t26</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">true_divide</span><span class=\"p\">(</span><span class=\"n\">t16</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>  <span class=\"c1\"># t26</span>\n",
       "    <span class=\"c1\"># _ = prims.convert_element_type(2, float)</span>\n",
       "    <span class=\"c1\"># t26 = prims.div(t16, 2.0)  # t26</span>\n",
       "  <span class=\"n\">p27</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">reduce_scatter</span><span class=\"p\">(</span><span class=\"n\">t26</span><span class=\"p\">,</span> <span class=\"n\">_DistributedReduceOps_1</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_0</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>  <span class=\"c1\"># p27</span>\n",
       "  <span class=\"n\">t28</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">wait</span><span class=\"p\">(</span><span class=\"n\">p27</span><span class=\"p\">)</span>  <span class=\"c1\"># t28</span>\n",
       "  <span class=\"n\">t29</span> <span class=\"o\">=</span> <span class=\"n\">ltorch</span><span class=\"o\">.</span><span class=\"n\">true_divide</span><span class=\"p\">(</span><span class=\"n\">t25</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>  <span class=\"c1\"># t29</span>\n",
       "    <span class=\"c1\"># _ = prims.convert_element_type(2, float)</span>\n",
       "    <span class=\"c1\"># t29 = prims.div(t25, 2.0)  # t29</span>\n",
       "  <span class=\"n\">p30</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">reduce_scatter</span><span class=\"p\">(</span><span class=\"n\">t29</span><span class=\"p\">,</span> <span class=\"n\">_DistributedReduceOps_1</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_0</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>  <span class=\"c1\"># p30</span>\n",
       "  <span class=\"n\">t31</span> <span class=\"o\">=</span> <span class=\"n\">thunder</span><span class=\"o\">.</span><span class=\"n\">distributed</span><span class=\"o\">.</span><span class=\"n\">prims</span><span class=\"o\">.</span><span class=\"n\">wait</span><span class=\"p\">(</span><span class=\"n\">p30</span><span class=\"p\">)</span>  <span class=\"c1\"># t31</span>\n",
       "  <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">t10</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">t31</span><span class=\"p\">,</span> <span class=\"n\">t28</span><span class=\"p\">,</span> <span class=\"p\">{</span><span class=\"s1\">&#39;x&#39;</span><span class=\"p\">:</span> <span class=\"n\">t21</span><span class=\"p\">}))</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{} Constructed by Dead Code Elimination (took 0 milliseconds)}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{core}\\PY{n+nn}{.}\\PY{n+nn}{devices} \\PY{k}{as} \\PY{n+nn}{devices}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{core}\\PY{n+nn}{.}\\PY{n+nn}{dtypes} \\PY{k}{as} \\PY{n+nn}{dtypes}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{core}\\PY{n+nn}{.}\\PY{n+nn}{prims} \\PY{k}{as} \\PY{n+nn}{prims}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{distributed}\\PY{n+nn}{.}\\PY{n+nn}{prims}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{torch} \\PY{k}{as} \\PY{n+nn}{ltorch}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{executors}\\PY{n+nn}{.}\\PY{n+nn}{torchex} \\PY{k+kn}{import} \\PY{n}{no\\PYZus{}autocast}\n",
       "\n",
       "\\PY{n+nd}{@torch}\\PY{o}{.}\\PY{n}{no\\PYZus{}grad}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{n+nd}{@no\\PYZus{}autocast}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{k}{def} \\PY{n+nf}{\\PYZus{}value\\PYZus{}and\\PYZus{}grad}\\PY{p}{(}\\PY{o}{*}\\PY{n}{args}\\PY{p}{,} \\PY{o}{*}\\PY{o}{*}\\PY{n}{kwargs}\\PY{p}{)}\\PY{p}{:}\n",
       "  \\PY{c+c1}{\\PYZsh{} args }\n",
       "  \\PY{c+c1}{\\PYZsh{} kwargs }\n",
       "  \\PY{n}{t0}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{n}{t1}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{o}{=} \\PY{n}{args}\n",
       "  \\PY{n}{t2} \\PY{o}{=} \\PY{n}{kwargs}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{x}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\n",
       "  \\PY{n}{t3} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{full}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{64}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{devices}\\PY{o}{.}\\PY{n}{Device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:0}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{dtypes}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t3}\n",
       "  \\PY{n}{p4} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{all\\PYZus{}gather}\\PY{p}{(}\\PY{n}{t0}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}0}\\PY{p}{,} \\PY{k+kc}{True}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} p4}\n",
       "  \\PY{n}{t5} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{wait}\\PY{p}{(}\\PY{n}{p4}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t5}\n",
       "  \\PY{n}{p6} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{all\\PYZus{}gather}\\PY{p}{(}\\PY{n}{t1}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}0}\\PY{p}{,} \\PY{k+kc}{True}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} p6}\n",
       "  \\PY{n}{t7} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{wait}\\PY{p}{(}\\PY{n}{p6}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t7}\n",
       "  \\PY{n}{t8} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{linear}\\PY{p}{(}\\PY{n}{t2}\\PY{p}{,} \\PY{n}{t5}\\PY{p}{,} \\PY{k+kc}{None}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t8}\n",
       "  \\PY{n}{t9} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{tanh}\\PY{p}{(}\\PY{n}{t8}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t9}\n",
       "  \\PY{n}{t10} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{linear}\\PY{p}{(}\\PY{n}{t9}\\PY{p}{,} \\PY{n}{t7}\\PY{p}{,} \\PY{k+kc}{None}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t10}\n",
       "  \\PY{n}{t11} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t3}\\PY{p}{,} \\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t11}\n",
       "    \\PY{c+c1}{\\PYZsh{} t11 = prims.reshape(t3, (64, 64))  \\PYZsh{} t11}\n",
       "  \\PY{n}{t12} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n}{t11}\\PY{p}{,} \\PY{n}{t7}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t12}\n",
       "    \\PY{c+c1}{\\PYZsh{} t12 = prims.matmul(t11, t7)  \\PYZsh{} t12}\n",
       "  \\PY{n}{t13} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t3}\\PY{p}{,} \\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t13}\n",
       "    \\PY{c+c1}{\\PYZsh{} t13 = prims.reshape(t3, (64, 64))  \\PYZsh{} t13}\n",
       "  \\PY{n}{t14} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{transpose}\\PY{p}{(}\\PY{n}{t13}\\PY{p}{,} \\PY{p}{(}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{0}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t14}\n",
       "  \\PY{n}{t15} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t9}\\PY{p}{,} \\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t15}\n",
       "    \\PY{c+c1}{\\PYZsh{} t15 = prims.reshape(t9, (64, 64))  \\PYZsh{} t15}\n",
       "  \\PY{n}{t16} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n}{t14}\\PY{p}{,} \\PY{n}{t15}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t16}\n",
       "    \\PY{c+c1}{\\PYZsh{} t16 = prims.matmul(t14, t15)  \\PYZsh{} t16}\n",
       "  \\PY{n}{t17} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{mul}\\PY{p}{(}\\PY{n}{t9}\\PY{p}{,} \\PY{n}{t9}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t17}\n",
       "    \\PY{c+c1}{\\PYZsh{} t17 = prims.mul(t9, t9)  \\PYZsh{} t17}\n",
       "  \\PY{n}{t18} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{sub}\\PY{p}{(}\\PY{l+m+mf}{1.0}\\PY{p}{,} \\PY{n}{t17}\\PY{p}{,} \\PY{n}{alpha}\\PY{o}{=}\\PY{k+kc}{None}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t18}\n",
       "    \\PY{c+c1}{\\PYZsh{} t18 = prims.sub(1.0, t17)  \\PYZsh{} t18}\n",
       "  \\PY{n}{t19} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{mul}\\PY{p}{(}\\PY{n}{t12}\\PY{p}{,} \\PY{n}{t18}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t19}\n",
       "    \\PY{c+c1}{\\PYZsh{} t19 = prims.mul(t12, t18)  \\PYZsh{} t19}\n",
       "  \\PY{n}{t20} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t19}\\PY{p}{,} \\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t20}\n",
       "    \\PY{c+c1}{\\PYZsh{} t20 = prims.reshape(t19, (64, 64))  \\PYZsh{} t20}\n",
       "  \\PY{n}{t21} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n}{t20}\\PY{p}{,} \\PY{n}{t5}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t21}\n",
       "    \\PY{c+c1}{\\PYZsh{} t21 = prims.matmul(t20, t5)  \\PYZsh{} t21}\n",
       "  \\PY{n}{t22} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t19}\\PY{p}{,} \\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t22}\n",
       "    \\PY{c+c1}{\\PYZsh{} t22 = prims.reshape(t19, (64, 64))  \\PYZsh{} t22}\n",
       "  \\PY{n}{t23} \\PY{o}{=} \\PY{n}{prims}\\PY{o}{.}\\PY{n}{transpose}\\PY{p}{(}\\PY{n}{t22}\\PY{p}{,} \\PY{p}{(}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{0}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t23}\n",
       "  \\PY{n}{t24} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t2}\\PY{p}{,} \\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t24}\n",
       "    \\PY{c+c1}{\\PYZsh{} t24 = prims.reshape(t2, (64, 64))  \\PYZsh{} t24}\n",
       "  \\PY{n}{t25} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n}{t23}\\PY{p}{,} \\PY{n}{t24}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t25}\n",
       "    \\PY{c+c1}{\\PYZsh{} t25 = prims.matmul(t23, t24)  \\PYZsh{} t25}\n",
       "  \\PY{n}{t26} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{true\\PYZus{}divide}\\PY{p}{(}\\PY{n}{t16}\\PY{p}{,} \\PY{l+m+mi}{2}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t26}\n",
       "    \\PY{c+c1}{\\PYZsh{} \\PYZus{} = prims.convert\\PYZus{}element\\PYZus{}type(2, float)}\n",
       "    \\PY{c+c1}{\\PYZsh{} t26 = prims.div(t16, 2.0)  \\PYZsh{} t26}\n",
       "  \\PY{n}{p27} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{reduce\\PYZus{}scatter}\\PY{p}{(}\\PY{n}{t26}\\PY{p}{,} \\PY{n}{\\PYZus{}DistributedReduceOps\\PYZus{}1}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}0}\\PY{p}{,} \\PY{k+kc}{True}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} p27}\n",
       "  \\PY{n}{t28} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{wait}\\PY{p}{(}\\PY{n}{p27}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t28}\n",
       "  \\PY{n}{t29} \\PY{o}{=} \\PY{n}{ltorch}\\PY{o}{.}\\PY{n}{true\\PYZus{}divide}\\PY{p}{(}\\PY{n}{t25}\\PY{p}{,} \\PY{l+m+mi}{2}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t29}\n",
       "    \\PY{c+c1}{\\PYZsh{} \\PYZus{} = prims.convert\\PYZus{}element\\PYZus{}type(2, float)}\n",
       "    \\PY{c+c1}{\\PYZsh{} t29 = prims.div(t25, 2.0)  \\PYZsh{} t29}\n",
       "  \\PY{n}{p30} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{reduce\\PYZus{}scatter}\\PY{p}{(}\\PY{n}{t29}\\PY{p}{,} \\PY{n}{\\PYZus{}DistributedReduceOps\\PYZus{}1}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}0}\\PY{p}{,} \\PY{k+kc}{True}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} p30}\n",
       "  \\PY{n}{t31} \\PY{o}{=} \\PY{n}{thunder}\\PY{o}{.}\\PY{n}{distributed}\\PY{o}{.}\\PY{n}{prims}\\PY{o}{.}\\PY{n}{wait}\\PY{p}{(}\\PY{n}{p30}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t31}\n",
       "  \\PY{k}{return} \\PY{p}{(}\\PY{n}{t10}\\PY{p}{,} \\PY{p}{(}\\PY{n}{t31}\\PY{p}{,} \\PY{n}{t28}\\PY{p}{,} \\PY{p}{\\PYZob{}}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{x}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{n}{t21}\\PY{p}{\\PYZcb{}}\\PY{p}{)}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "# Constructed by Dead Code Elimination (took 0 milliseconds)\n",
       "import thunder\n",
       "import thunder.core.devices as devices\n",
       "import thunder.core.dtypes as dtypes\n",
       "import thunder.core.prims as prims\n",
       "import thunder.distributed.prims\n",
       "import thunder.torch as ltorch\n",
       "import torch\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast()\n",
       "def _value_and_grad(*args, **kwargs):\n",
       "  # args \n",
       "  # kwargs \n",
       "  t0, \\\n",
       "  t1, \\\n",
       "  = args\n",
       "  t2 = kwargs['x']\n",
       "  t3 = prims.full((64, 64), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t3\n",
       "  p4 = thunder.distributed.prims.all_gather(t0, _torch_distributed_distributed_c10d_ProcessGroup_0, True)  # p4\n",
       "  t5 = thunder.distributed.prims.wait(p4)  # t5\n",
       "  p6 = thunder.distributed.prims.all_gather(t1, _torch_distributed_distributed_c10d_ProcessGroup_0, True)  # p6\n",
       "  t7 = thunder.distributed.prims.wait(p6)  # t7\n",
       "  t8 = prims.linear(t2, t5, None)  # t8\n",
       "  t9 = prims.tanh(t8)  # t9\n",
       "  t10 = prims.linear(t9, t7, None)  # t10\n",
       "  t11 = ltorch.reshape(t3, -1, 64)  # t11\n",
       "    # t11 = prims.reshape(t3, (64, 64))  # t11\n",
       "  t12 = ltorch.matmul(t11, t7)  # t12\n",
       "    # t12 = prims.matmul(t11, t7)  # t12\n",
       "  t13 = ltorch.reshape(t3, -1, 64)  # t13\n",
       "    # t13 = prims.reshape(t3, (64, 64))  # t13\n",
       "  t14 = prims.transpose(t13, (1, 0))  # t14\n",
       "  t15 = ltorch.reshape(t9, -1, 64)  # t15\n",
       "    # t15 = prims.reshape(t9, (64, 64))  # t15\n",
       "  t16 = ltorch.matmul(t14, t15)  # t16\n",
       "    # t16 = prims.matmul(t14, t15)  # t16\n",
       "  t17 = ltorch.mul(t9, t9)  # t17\n",
       "    # t17 = prims.mul(t9, t9)  # t17\n",
       "  t18 = ltorch.sub(1.0, t17, alpha=None)  # t18\n",
       "    # t18 = prims.sub(1.0, t17)  # t18\n",
       "  t19 = ltorch.mul(t12, t18)  # t19\n",
       "    # t19 = prims.mul(t12, t18)  # t19\n",
       "  t20 = ltorch.reshape(t19, -1, 64)  # t20\n",
       "    # t20 = prims.reshape(t19, (64, 64))  # t20\n",
       "  t21 = ltorch.matmul(t20, t5)  # t21\n",
       "    # t21 = prims.matmul(t20, t5)  # t21\n",
       "  t22 = ltorch.reshape(t19, -1, 64)  # t22\n",
       "    # t22 = prims.reshape(t19, (64, 64))  # t22\n",
       "  t23 = prims.transpose(t22, (1, 0))  # t23\n",
       "  t24 = ltorch.reshape(t2, -1, 64)  # t24\n",
       "    # t24 = prims.reshape(t2, (64, 64))  # t24\n",
       "  t25 = ltorch.matmul(t23, t24)  # t25\n",
       "    # t25 = prims.matmul(t23, t24)  # t25\n",
       "  t26 = ltorch.true_divide(t16, 2)  # t26\n",
       "    # _ = prims.convert_element_type(2, float)\n",
       "    # t26 = prims.div(t16, 2.0)  # t26\n",
       "  p27 = thunder.distributed.prims.reduce_scatter(t26, _DistributedReduceOps_1, _torch_distributed_distributed_c10d_ProcessGroup_0, True)  # p27\n",
       "  t28 = thunder.distributed.prims.wait(p27)  # t28\n",
       "  t29 = ltorch.true_divide(t25, 2)  # t29\n",
       "    # _ = prims.convert_element_type(2, float)\n",
       "    # t29 = prims.div(t25, 2.0)  # t29\n",
       "  p30 = thunder.distributed.prims.reduce_scatter(t29, _DistributedReduceOps_1, _torch_distributed_distributed_c10d_ProcessGroup_0, True)  # p30\n",
       "  t31 = thunder.distributed.prims.wait(p30)  # t31\n",
       "  return (t10, (t31, t28, {'x': t21}))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from thunder.core.transforms import value_and_grad\n",
    "\n",
    "forward_and_backward_model = value_and_grad(model_with_syncs)\n",
    "\n",
    "forward_backward_trace = thunder.trace()(forward_and_backward_model, *model.parameters(), x=x)\n",
    "\n",
    "wrap_as_highlighted_code(forward_backward_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above trace, only contains primitive which specifies the semantic of an operation abstractly but doesn't perform the actual computation.\n",
    "\n",
    "Now we will generate the execution trace which can actually perform the compute.\n",
    "\n",
    "In the execution trace generated below, we can see that all the primitives have been replaced with actually PyTorch operations. Also, our synchronization primitives have been replaced with PyTorch implementation provided by thunder i.e. `torch_all_gather_prim_impl`, `torch_reduce_scatter_prim_impl`, `torch_wait_prim_impl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { line-height: 125%; }\n",
       "td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }\n",
       "td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       "span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }\n",
       ".output_html .hll { background-color: #ffffcc }\n",
       ".output_html { background: #f8f8f8; }\n",
       ".output_html .c { color: #3D7B7B; font-style: italic } /* Comment */\n",
       ".output_html .err { border: 1px solid #FF0000 } /* Error */\n",
       ".output_html .k { color: #008000; font-weight: bold } /* Keyword */\n",
       ".output_html .o { color: #666666 } /* Operator */\n",
       ".output_html .ch { color: #3D7B7B; font-style: italic } /* Comment.Hashbang */\n",
       ".output_html .cm { color: #3D7B7B; font-style: italic } /* Comment.Multiline */\n",
       ".output_html .cp { color: #9C6500 } /* Comment.Preproc */\n",
       ".output_html .cpf { color: #3D7B7B; font-style: italic } /* Comment.PreprocFile */\n",
       ".output_html .c1 { color: #3D7B7B; font-style: italic } /* Comment.Single */\n",
       ".output_html .cs { color: #3D7B7B; font-style: italic } /* Comment.Special */\n",
       ".output_html .gd { color: #A00000 } /* Generic.Deleted */\n",
       ".output_html .ge { font-style: italic } /* Generic.Emph */\n",
       ".output_html .ges { font-weight: bold; font-style: italic } /* Generic.EmphStrong */\n",
       ".output_html .gr { color: #E40000 } /* Generic.Error */\n",
       ".output_html .gh { color: #000080; font-weight: bold } /* Generic.Heading */\n",
       ".output_html .gi { color: #008400 } /* Generic.Inserted */\n",
       ".output_html .go { color: #717171 } /* Generic.Output */\n",
       ".output_html .gp { color: #000080; font-weight: bold } /* Generic.Prompt */\n",
       ".output_html .gs { font-weight: bold } /* Generic.Strong */\n",
       ".output_html .gu { color: #800080; font-weight: bold } /* Generic.Subheading */\n",
       ".output_html .gt { color: #0044DD } /* Generic.Traceback */\n",
       ".output_html .kc { color: #008000; font-weight: bold } /* Keyword.Constant */\n",
       ".output_html .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */\n",
       ".output_html .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */\n",
       ".output_html .kp { color: #008000 } /* Keyword.Pseudo */\n",
       ".output_html .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */\n",
       ".output_html .kt { color: #B00040 } /* Keyword.Type */\n",
       ".output_html .m { color: #666666 } /* Literal.Number */\n",
       ".output_html .s { color: #BA2121 } /* Literal.String */\n",
       ".output_html .na { color: #687822 } /* Name.Attribute */\n",
       ".output_html .nb { color: #008000 } /* Name.Builtin */\n",
       ".output_html .nc { color: #0000FF; font-weight: bold } /* Name.Class */\n",
       ".output_html .no { color: #880000 } /* Name.Constant */\n",
       ".output_html .nd { color: #AA22FF } /* Name.Decorator */\n",
       ".output_html .ni { color: #717171; font-weight: bold } /* Name.Entity */\n",
       ".output_html .ne { color: #CB3F38; font-weight: bold } /* Name.Exception */\n",
       ".output_html .nf { color: #0000FF } /* Name.Function */\n",
       ".output_html .nl { color: #767600 } /* Name.Label */\n",
       ".output_html .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */\n",
       ".output_html .nt { color: #008000; font-weight: bold } /* Name.Tag */\n",
       ".output_html .nv { color: #19177C } /* Name.Variable */\n",
       ".output_html .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */\n",
       ".output_html .w { color: #bbbbbb } /* Text.Whitespace */\n",
       ".output_html .mb { color: #666666 } /* Literal.Number.Bin */\n",
       ".output_html .mf { color: #666666 } /* Literal.Number.Float */\n",
       ".output_html .mh { color: #666666 } /* Literal.Number.Hex */\n",
       ".output_html .mi { color: #666666 } /* Literal.Number.Integer */\n",
       ".output_html .mo { color: #666666 } /* Literal.Number.Oct */\n",
       ".output_html .sa { color: #BA2121 } /* Literal.String.Affix */\n",
       ".output_html .sb { color: #BA2121 } /* Literal.String.Backtick */\n",
       ".output_html .sc { color: #BA2121 } /* Literal.String.Char */\n",
       ".output_html .dl { color: #BA2121 } /* Literal.String.Delimiter */\n",
       ".output_html .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */\n",
       ".output_html .s2 { color: #BA2121 } /* Literal.String.Double */\n",
       ".output_html .se { color: #AA5D1F; font-weight: bold } /* Literal.String.Escape */\n",
       ".output_html .sh { color: #BA2121 } /* Literal.String.Heredoc */\n",
       ".output_html .si { color: #A45A77; font-weight: bold } /* Literal.String.Interpol */\n",
       ".output_html .sx { color: #008000 } /* Literal.String.Other */\n",
       ".output_html .sr { color: #A45A77 } /* Literal.String.Regex */\n",
       ".output_html .s1 { color: #BA2121 } /* Literal.String.Single */\n",
       ".output_html .ss { color: #19177C } /* Literal.String.Symbol */\n",
       ".output_html .bp { color: #008000 } /* Name.Builtin.Pseudo */\n",
       ".output_html .fm { color: #0000FF } /* Name.Function.Magic */\n",
       ".output_html .vc { color: #19177C } /* Name.Variable.Class */\n",
       ".output_html .vg { color: #19177C } /* Name.Variable.Global */\n",
       ".output_html .vi { color: #19177C } /* Name.Variable.Instance */\n",
       ".output_html .vm { color: #19177C } /* Name.Variable.Magic */\n",
       ".output_html .il { color: #666666 } /* Literal.Number.Integer.Long */</style><div class=\"highlight\"><pre><span></span><span class=\"c1\"># Constructed by Delete Last Used (took 0 milliseconds)</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch</span>\n",
       "<span class=\"kn\">import</span> <span class=\"nn\">torch.nn.functional</span>\n",
       "<span class=\"kn\">from</span> <span class=\"nn\">thunder.executors.torchex</span> <span class=\"kn\">import</span> <span class=\"n\">no_autocast</span>\n",
       "\n",
       "<span class=\"nd\">@torch</span><span class=\"o\">.</span><span class=\"n\">no_grad</span><span class=\"p\">()</span>\n",
       "<span class=\"nd\">@no_autocast</span><span class=\"p\">()</span>\n",
       "<span class=\"k\">def</span> <span class=\"nf\">_value_and_grad</span><span class=\"p\">(</span><span class=\"o\">*</span><span class=\"n\">args</span><span class=\"p\">,</span> <span class=\"o\">**</span><span class=\"n\">kwargs</span><span class=\"p\">):</span>\n",
       "  <span class=\"c1\"># args </span>\n",
       "  <span class=\"c1\"># kwargs </span>\n",
       "  <span class=\"n\">t0</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"n\">t1</span><span class=\"p\">,</span> \\\n",
       "  <span class=\"o\">=</span> <span class=\"n\">args</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">args</span>\n",
       "  <span class=\"n\">t2</span> <span class=\"o\">=</span> <span class=\"n\">kwargs</span><span class=\"p\">[</span><span class=\"s1\">&#39;x&#39;</span><span class=\"p\">]</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">kwargs</span>\n",
       "  <span class=\"n\">t3</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">full</span><span class=\"p\">((</span><span class=\"mi\">64</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">),</span> <span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"n\">device</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">device</span><span class=\"p\">(</span><span class=\"s2\">&quot;cuda:0&quot;</span><span class=\"p\">),</span> <span class=\"n\">dtype</span><span class=\"o\">=</span><span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">float32</span><span class=\"p\">)</span>  <span class=\"c1\"># t3</span>\n",
       "    <span class=\"c1\"># t3 = ltorch.full((64, 64), 1, device=torch.device(&quot;cuda:0&quot;), dtype=torch.float32)  # t3</span>\n",
       "      <span class=\"c1\"># t3 = prims.full((64, 64), 1, device=devices.Device(&quot;cuda:0&quot;), dtype=dtypes.float32)  # t3</span>\n",
       "  <span class=\"n\">p4</span> <span class=\"o\">=</span> <span class=\"n\">torch_all_gather_prim_impl</span><span class=\"p\">(</span><span class=\"n\">t0</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_2</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>  <span class=\"c1\"># p4</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t0</span>\n",
       "  <span class=\"n\">t5</span> <span class=\"o\">=</span> <span class=\"n\">torch_wait_prim_impl</span><span class=\"p\">(</span><span class=\"n\">p4</span><span class=\"p\">)</span>  <span class=\"c1\"># t5</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">p4</span>\n",
       "  <span class=\"n\">p6</span> <span class=\"o\">=</span> <span class=\"n\">torch_all_gather_prim_impl</span><span class=\"p\">(</span><span class=\"n\">t1</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_2</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>  <span class=\"c1\"># p6</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t1</span>\n",
       "  <span class=\"n\">t7</span> <span class=\"o\">=</span> <span class=\"n\">torch_wait_prim_impl</span><span class=\"p\">(</span><span class=\"n\">p6</span><span class=\"p\">)</span>  <span class=\"c1\"># t7</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">p6</span>\n",
       "  <span class=\"n\">t8</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">functional</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">t2</span><span class=\"p\">,</span> <span class=\"n\">t5</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>  <span class=\"c1\"># t8</span>\n",
       "    <span class=\"c1\"># t8 = ltorch.linear(t2, t5, None)  # t8</span>\n",
       "      <span class=\"c1\"># t8 = prims.linear(t2, t5, None)  # t8</span>\n",
       "  <span class=\"n\">t9</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">tanh</span><span class=\"p\">(</span><span class=\"n\">t8</span><span class=\"p\">)</span>  <span class=\"c1\"># t9</span>\n",
       "    <span class=\"c1\"># t9 = ltorch.tanh(t8)  # t9</span>\n",
       "      <span class=\"c1\"># t9 = prims.tanh(t8)  # t9</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t8</span>\n",
       "  <span class=\"n\">t10</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">nn</span><span class=\"o\">.</span><span class=\"n\">functional</span><span class=\"o\">.</span><span class=\"n\">linear</span><span class=\"p\">(</span><span class=\"n\">t9</span><span class=\"p\">,</span> <span class=\"n\">t7</span><span class=\"p\">,</span> <span class=\"kc\">None</span><span class=\"p\">)</span>  <span class=\"c1\"># t10</span>\n",
       "    <span class=\"c1\"># t10 = ltorch.linear(t9, t7, None)  # t10</span>\n",
       "      <span class=\"c1\"># t10 = prims.linear(t9, t7, None)  # t10</span>\n",
       "  <span class=\"n\">t11</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t3</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">))</span>  <span class=\"c1\"># t11</span>\n",
       "    <span class=\"c1\"># t11 = ltorch.reshape(t3, (-1, 64))  # t11</span>\n",
       "      <span class=\"c1\"># t11 = prims.reshape(t3, (64, 64))  # t11</span>\n",
       "  <span class=\"n\">t12</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">t11</span><span class=\"p\">,</span> <span class=\"n\">t7</span><span class=\"p\">)</span>  <span class=\"c1\"># t12</span>\n",
       "    <span class=\"c1\"># t12 = ltorch.matmul(t11, t7)  # t12</span>\n",
       "      <span class=\"c1\"># t12 = prims.matmul(t11, t7)  # t12</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t11</span><span class=\"p\">,</span> <span class=\"n\">t7</span>\n",
       "  <span class=\"n\">t13</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t3</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">))</span>  <span class=\"c1\"># t13</span>\n",
       "    <span class=\"c1\"># t13 = ltorch.reshape(t3, (-1, 64))  # t13</span>\n",
       "      <span class=\"c1\"># t13 = prims.reshape(t3, (64, 64))  # t13</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t3</span>\n",
       "  <span class=\"n\">t14</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">permute</span><span class=\"p\">(</span><span class=\"n\">t13</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>  <span class=\"c1\"># t14</span>\n",
       "    <span class=\"c1\"># t14 = ltorch.permute(t13, (1, 0))  # t14</span>\n",
       "      <span class=\"c1\"># t14 = prims.transpose(t13, (1, 0))  # t14</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t13</span>\n",
       "  <span class=\"n\">t15</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t9</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">))</span>  <span class=\"c1\"># t15</span>\n",
       "    <span class=\"c1\"># t15 = ltorch.reshape(t9, (-1, 64))  # t15</span>\n",
       "      <span class=\"c1\"># t15 = prims.reshape(t9, (64, 64))  # t15</span>\n",
       "  <span class=\"n\">t16</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">t14</span><span class=\"p\">,</span> <span class=\"n\">t15</span><span class=\"p\">)</span>  <span class=\"c1\"># t16</span>\n",
       "    <span class=\"c1\"># t16 = ltorch.matmul(t14, t15)  # t16</span>\n",
       "      <span class=\"c1\"># t16 = prims.matmul(t14, t15)  # t16</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t14</span><span class=\"p\">,</span> <span class=\"n\">t15</span>\n",
       "  <span class=\"n\">t17</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">mul</span><span class=\"p\">(</span><span class=\"n\">t9</span><span class=\"p\">,</span> <span class=\"n\">t9</span><span class=\"p\">)</span>  <span class=\"c1\"># t17</span>\n",
       "    <span class=\"c1\"># t17 = ltorch.mul(t9, t9)  # t17</span>\n",
       "      <span class=\"c1\"># t17 = prims.mul(t9, t9)  # t17</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t9</span>\n",
       "  <span class=\"n\">t18</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">sub</span><span class=\"p\">(</span><span class=\"mf\">1.0</span><span class=\"p\">,</span> <span class=\"n\">t17</span><span class=\"p\">)</span>  <span class=\"c1\"># t18</span>\n",
       "    <span class=\"c1\"># t18 = ltorch.sub(1.0, t17, alpha=None)  # t18</span>\n",
       "      <span class=\"c1\"># t18 = prims.sub(1.0, t17)  # t18</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t17</span>\n",
       "  <span class=\"n\">t19</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">mul</span><span class=\"p\">(</span><span class=\"n\">t12</span><span class=\"p\">,</span> <span class=\"n\">t18</span><span class=\"p\">)</span>  <span class=\"c1\"># t19</span>\n",
       "    <span class=\"c1\"># t19 = ltorch.mul(t12, t18)  # t19</span>\n",
       "      <span class=\"c1\"># t19 = prims.mul(t12, t18)  # t19</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t12</span><span class=\"p\">,</span> <span class=\"n\">t18</span>\n",
       "  <span class=\"n\">t20</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t19</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">))</span>  <span class=\"c1\"># t20</span>\n",
       "    <span class=\"c1\"># t20 = ltorch.reshape(t19, (-1, 64))  # t20</span>\n",
       "      <span class=\"c1\"># t20 = prims.reshape(t19, (64, 64))  # t20</span>\n",
       "  <span class=\"n\">t21</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">t20</span><span class=\"p\">,</span> <span class=\"n\">t5</span><span class=\"p\">)</span>  <span class=\"c1\"># t21</span>\n",
       "    <span class=\"c1\"># t21 = ltorch.matmul(t20, t5)  # t21</span>\n",
       "      <span class=\"c1\"># t21 = prims.matmul(t20, t5)  # t21</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t20</span><span class=\"p\">,</span> <span class=\"n\">t5</span>\n",
       "  <span class=\"n\">t22</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t19</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">))</span>  <span class=\"c1\"># t22</span>\n",
       "    <span class=\"c1\"># t22 = ltorch.reshape(t19, (-1, 64))  # t22</span>\n",
       "      <span class=\"c1\"># t22 = prims.reshape(t19, (64, 64))  # t22</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t19</span>\n",
       "  <span class=\"n\">t23</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">permute</span><span class=\"p\">(</span><span class=\"n\">t22</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">0</span><span class=\"p\">))</span>  <span class=\"c1\"># t23</span>\n",
       "    <span class=\"c1\"># t23 = ltorch.permute(t22, (1, 0))  # t23</span>\n",
       "      <span class=\"c1\"># t23 = prims.transpose(t22, (1, 0))  # t23</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t22</span>\n",
       "  <span class=\"n\">t24</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">reshape</span><span class=\"p\">(</span><span class=\"n\">t2</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"o\">-</span><span class=\"mi\">1</span><span class=\"p\">,</span> <span class=\"mi\">64</span><span class=\"p\">))</span>  <span class=\"c1\"># t24</span>\n",
       "    <span class=\"c1\"># t24 = ltorch.reshape(t2, (-1, 64))  # t24</span>\n",
       "      <span class=\"c1\"># t24 = prims.reshape(t2, (64, 64))  # t24</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t2</span>\n",
       "  <span class=\"n\">t25</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">matmul</span><span class=\"p\">(</span><span class=\"n\">t23</span><span class=\"p\">,</span> <span class=\"n\">t24</span><span class=\"p\">)</span>  <span class=\"c1\"># t25</span>\n",
       "    <span class=\"c1\"># t25 = ltorch.matmul(t23, t24)  # t25</span>\n",
       "      <span class=\"c1\"># t25 = prims.matmul(t23, t24)  # t25</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t23</span><span class=\"p\">,</span> <span class=\"n\">t24</span>\n",
       "  <span class=\"n\">t26</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">true_divide</span><span class=\"p\">(</span><span class=\"n\">t16</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>  <span class=\"c1\"># t26</span>\n",
       "    <span class=\"c1\"># t26 = ltorch.true_divide(t16, 2)  # t26</span>\n",
       "      <span class=\"c1\"># _ = prims.convert_element_type(2, float)</span>\n",
       "      <span class=\"c1\"># t26 = prims.div(t16, 2.0)  # t26</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t16</span>\n",
       "  <span class=\"n\">p27</span> <span class=\"o\">=</span> <span class=\"n\">torch_reduce_scatter_prim_impl</span><span class=\"p\">(</span><span class=\"n\">t26</span><span class=\"p\">,</span> <span class=\"n\">_DistributedReduceOps_3</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_2</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>  <span class=\"c1\"># p27</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t26</span>\n",
       "  <span class=\"n\">t28</span> <span class=\"o\">=</span> <span class=\"n\">torch_wait_prim_impl</span><span class=\"p\">(</span><span class=\"n\">p27</span><span class=\"p\">)</span>  <span class=\"c1\"># t28</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">p27</span>\n",
       "  <span class=\"n\">t29</span> <span class=\"o\">=</span> <span class=\"n\">torch</span><span class=\"o\">.</span><span class=\"n\">true_divide</span><span class=\"p\">(</span><span class=\"n\">t25</span><span class=\"p\">,</span> <span class=\"mi\">2</span><span class=\"p\">)</span>  <span class=\"c1\"># t29</span>\n",
       "    <span class=\"c1\"># t29 = ltorch.true_divide(t25, 2)  # t29</span>\n",
       "      <span class=\"c1\"># _ = prims.convert_element_type(2, float)</span>\n",
       "      <span class=\"c1\"># t29 = prims.div(t25, 2.0)  # t29</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t25</span>\n",
       "  <span class=\"n\">p30</span> <span class=\"o\">=</span> <span class=\"n\">torch_reduce_scatter_prim_impl</span><span class=\"p\">(</span><span class=\"n\">t29</span><span class=\"p\">,</span> <span class=\"n\">_DistributedReduceOps_3</span><span class=\"p\">,</span> <span class=\"n\">_torch_distributed_distributed_c10d_ProcessGroup_2</span><span class=\"p\">,</span> <span class=\"kc\">True</span><span class=\"p\">)</span>  <span class=\"c1\"># p30</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">t29</span>\n",
       "  <span class=\"n\">t31</span> <span class=\"o\">=</span> <span class=\"n\">torch_wait_prim_impl</span><span class=\"p\">(</span><span class=\"n\">p30</span><span class=\"p\">)</span>  <span class=\"c1\"># t31</span>\n",
       "  <span class=\"k\">del</span> <span class=\"n\">p30</span>\n",
       "  <span class=\"k\">return</span> <span class=\"p\">(</span><span class=\"n\">t10</span><span class=\"p\">,</span> <span class=\"p\">(</span><span class=\"n\">t31</span><span class=\"p\">,</span> <span class=\"n\">t28</span><span class=\"p\">,</span> <span class=\"p\">{</span><span class=\"s1\">&#39;x&#39;</span><span class=\"p\">:</span> <span class=\"n\">t21</span><span class=\"p\">}))</span>\n",
       "</pre></div>\n"
      ],
      "text/latex": [
       "\\begin{Verbatim}[commandchars=\\\\\\{\\}]\n",
       "\\PY{c+c1}{\\PYZsh{} Constructed by Delete Last Used (took 0 milliseconds)}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\n",
       "\\PY{k+kn}{import} \\PY{n+nn}{torch}\\PY{n+nn}{.}\\PY{n+nn}{nn}\\PY{n+nn}{.}\\PY{n+nn}{functional}\n",
       "\\PY{k+kn}{from} \\PY{n+nn}{thunder}\\PY{n+nn}{.}\\PY{n+nn}{executors}\\PY{n+nn}{.}\\PY{n+nn}{torchex} \\PY{k+kn}{import} \\PY{n}{no\\PYZus{}autocast}\n",
       "\n",
       "\\PY{n+nd}{@torch}\\PY{o}{.}\\PY{n}{no\\PYZus{}grad}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{n+nd}{@no\\PYZus{}autocast}\\PY{p}{(}\\PY{p}{)}\n",
       "\\PY{k}{def} \\PY{n+nf}{\\PYZus{}value\\PYZus{}and\\PYZus{}grad}\\PY{p}{(}\\PY{o}{*}\\PY{n}{args}\\PY{p}{,} \\PY{o}{*}\\PY{o}{*}\\PY{n}{kwargs}\\PY{p}{)}\\PY{p}{:}\n",
       "  \\PY{c+c1}{\\PYZsh{} args }\n",
       "  \\PY{c+c1}{\\PYZsh{} kwargs }\n",
       "  \\PY{n}{t0}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{n}{t1}\\PY{p}{,} \\PYZbs{}\n",
       "  \\PY{o}{=} \\PY{n}{args}\n",
       "  \\PY{k}{del} \\PY{n}{args}\n",
       "  \\PY{n}{t2} \\PY{o}{=} \\PY{n}{kwargs}\\PY{p}{[}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{x}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{]}\n",
       "  \\PY{k}{del} \\PY{n}{kwargs}\n",
       "  \\PY{n}{t3} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{full}\\PY{p}{(}\\PY{p}{(}\\PY{l+m+mi}{64}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{,} \\PY{l+m+mi}{1}\\PY{p}{,} \\PY{n}{device}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{device}\\PY{p}{(}\\PY{l+s+s2}{\\PYZdq{}}\\PY{l+s+s2}{cuda:0}\\PY{l+s+s2}{\\PYZdq{}}\\PY{p}{)}\\PY{p}{,} \\PY{n}{dtype}\\PY{o}{=}\\PY{n}{torch}\\PY{o}{.}\\PY{n}{float32}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t3}\n",
       "    \\PY{c+c1}{\\PYZsh{} t3 = ltorch.full((64, 64), 1, device=torch.device(\\PYZdq{}cuda:0\\PYZdq{}), dtype=torch.float32)  \\PYZsh{} t3}\n",
       "      \\PY{c+c1}{\\PYZsh{} t3 = prims.full((64, 64), 1, device=devices.Device(\\PYZdq{}cuda:0\\PYZdq{}), dtype=dtypes.float32)  \\PYZsh{} t3}\n",
       "  \\PY{n}{p4} \\PY{o}{=} \\PY{n}{torch\\PYZus{}all\\PYZus{}gather\\PYZus{}prim\\PYZus{}impl}\\PY{p}{(}\\PY{n}{t0}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}2}\\PY{p}{,} \\PY{k+kc}{True}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} p4}\n",
       "  \\PY{k}{del} \\PY{n}{t0}\n",
       "  \\PY{n}{t5} \\PY{o}{=} \\PY{n}{torch\\PYZus{}wait\\PYZus{}prim\\PYZus{}impl}\\PY{p}{(}\\PY{n}{p4}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t5}\n",
       "  \\PY{k}{del} \\PY{n}{p4}\n",
       "  \\PY{n}{p6} \\PY{o}{=} \\PY{n}{torch\\PYZus{}all\\PYZus{}gather\\PYZus{}prim\\PYZus{}impl}\\PY{p}{(}\\PY{n}{t1}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}2}\\PY{p}{,} \\PY{k+kc}{True}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} p6}\n",
       "  \\PY{k}{del} \\PY{n}{t1}\n",
       "  \\PY{n}{t7} \\PY{o}{=} \\PY{n}{torch\\PYZus{}wait\\PYZus{}prim\\PYZus{}impl}\\PY{p}{(}\\PY{n}{p6}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t7}\n",
       "  \\PY{k}{del} \\PY{n}{p6}\n",
       "  \\PY{n}{t8} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{functional}\\PY{o}{.}\\PY{n}{linear}\\PY{p}{(}\\PY{n}{t2}\\PY{p}{,} \\PY{n}{t5}\\PY{p}{,} \\PY{k+kc}{None}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t8}\n",
       "    \\PY{c+c1}{\\PYZsh{} t8 = ltorch.linear(t2, t5, None)  \\PYZsh{} t8}\n",
       "      \\PY{c+c1}{\\PYZsh{} t8 = prims.linear(t2, t5, None)  \\PYZsh{} t8}\n",
       "  \\PY{n}{t9} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{tanh}\\PY{p}{(}\\PY{n}{t8}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t9}\n",
       "    \\PY{c+c1}{\\PYZsh{} t9 = ltorch.tanh(t8)  \\PYZsh{} t9}\n",
       "      \\PY{c+c1}{\\PYZsh{} t9 = prims.tanh(t8)  \\PYZsh{} t9}\n",
       "  \\PY{k}{del} \\PY{n}{t8}\n",
       "  \\PY{n}{t10} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{nn}\\PY{o}{.}\\PY{n}{functional}\\PY{o}{.}\\PY{n}{linear}\\PY{p}{(}\\PY{n}{t9}\\PY{p}{,} \\PY{n}{t7}\\PY{p}{,} \\PY{k+kc}{None}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t10}\n",
       "    \\PY{c+c1}{\\PYZsh{} t10 = ltorch.linear(t9, t7, None)  \\PYZsh{} t10}\n",
       "      \\PY{c+c1}{\\PYZsh{} t10 = prims.linear(t9, t7, None)  \\PYZsh{} t10}\n",
       "  \\PY{n}{t11} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t3}\\PY{p}{,} \\PY{p}{(}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t11}\n",
       "    \\PY{c+c1}{\\PYZsh{} t11 = ltorch.reshape(t3, (\\PYZhy{}1, 64))  \\PYZsh{} t11}\n",
       "      \\PY{c+c1}{\\PYZsh{} t11 = prims.reshape(t3, (64, 64))  \\PYZsh{} t11}\n",
       "  \\PY{n}{t12} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n}{t11}\\PY{p}{,} \\PY{n}{t7}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t12}\n",
       "    \\PY{c+c1}{\\PYZsh{} t12 = ltorch.matmul(t11, t7)  \\PYZsh{} t12}\n",
       "      \\PY{c+c1}{\\PYZsh{} t12 = prims.matmul(t11, t7)  \\PYZsh{} t12}\n",
       "  \\PY{k}{del} \\PY{n}{t11}\\PY{p}{,} \\PY{n}{t7}\n",
       "  \\PY{n}{t13} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t3}\\PY{p}{,} \\PY{p}{(}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t13}\n",
       "    \\PY{c+c1}{\\PYZsh{} t13 = ltorch.reshape(t3, (\\PYZhy{}1, 64))  \\PYZsh{} t13}\n",
       "      \\PY{c+c1}{\\PYZsh{} t13 = prims.reshape(t3, (64, 64))  \\PYZsh{} t13}\n",
       "  \\PY{k}{del} \\PY{n}{t3}\n",
       "  \\PY{n}{t14} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{permute}\\PY{p}{(}\\PY{n}{t13}\\PY{p}{,} \\PY{p}{(}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{0}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t14}\n",
       "    \\PY{c+c1}{\\PYZsh{} t14 = ltorch.permute(t13, (1, 0))  \\PYZsh{} t14}\n",
       "      \\PY{c+c1}{\\PYZsh{} t14 = prims.transpose(t13, (1, 0))  \\PYZsh{} t14}\n",
       "  \\PY{k}{del} \\PY{n}{t13}\n",
       "  \\PY{n}{t15} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t9}\\PY{p}{,} \\PY{p}{(}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t15}\n",
       "    \\PY{c+c1}{\\PYZsh{} t15 = ltorch.reshape(t9, (\\PYZhy{}1, 64))  \\PYZsh{} t15}\n",
       "      \\PY{c+c1}{\\PYZsh{} t15 = prims.reshape(t9, (64, 64))  \\PYZsh{} t15}\n",
       "  \\PY{n}{t16} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n}{t14}\\PY{p}{,} \\PY{n}{t15}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t16}\n",
       "    \\PY{c+c1}{\\PYZsh{} t16 = ltorch.matmul(t14, t15)  \\PYZsh{} t16}\n",
       "      \\PY{c+c1}{\\PYZsh{} t16 = prims.matmul(t14, t15)  \\PYZsh{} t16}\n",
       "  \\PY{k}{del} \\PY{n}{t14}\\PY{p}{,} \\PY{n}{t15}\n",
       "  \\PY{n}{t17} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{mul}\\PY{p}{(}\\PY{n}{t9}\\PY{p}{,} \\PY{n}{t9}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t17}\n",
       "    \\PY{c+c1}{\\PYZsh{} t17 = ltorch.mul(t9, t9)  \\PYZsh{} t17}\n",
       "      \\PY{c+c1}{\\PYZsh{} t17 = prims.mul(t9, t9)  \\PYZsh{} t17}\n",
       "  \\PY{k}{del} \\PY{n}{t9}\n",
       "  \\PY{n}{t18} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{sub}\\PY{p}{(}\\PY{l+m+mf}{1.0}\\PY{p}{,} \\PY{n}{t17}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t18}\n",
       "    \\PY{c+c1}{\\PYZsh{} t18 = ltorch.sub(1.0, t17, alpha=None)  \\PYZsh{} t18}\n",
       "      \\PY{c+c1}{\\PYZsh{} t18 = prims.sub(1.0, t17)  \\PYZsh{} t18}\n",
       "  \\PY{k}{del} \\PY{n}{t17}\n",
       "  \\PY{n}{t19} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{mul}\\PY{p}{(}\\PY{n}{t12}\\PY{p}{,} \\PY{n}{t18}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t19}\n",
       "    \\PY{c+c1}{\\PYZsh{} t19 = ltorch.mul(t12, t18)  \\PYZsh{} t19}\n",
       "      \\PY{c+c1}{\\PYZsh{} t19 = prims.mul(t12, t18)  \\PYZsh{} t19}\n",
       "  \\PY{k}{del} \\PY{n}{t12}\\PY{p}{,} \\PY{n}{t18}\n",
       "  \\PY{n}{t20} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t19}\\PY{p}{,} \\PY{p}{(}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t20}\n",
       "    \\PY{c+c1}{\\PYZsh{} t20 = ltorch.reshape(t19, (\\PYZhy{}1, 64))  \\PYZsh{} t20}\n",
       "      \\PY{c+c1}{\\PYZsh{} t20 = prims.reshape(t19, (64, 64))  \\PYZsh{} t20}\n",
       "  \\PY{n}{t21} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n}{t20}\\PY{p}{,} \\PY{n}{t5}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t21}\n",
       "    \\PY{c+c1}{\\PYZsh{} t21 = ltorch.matmul(t20, t5)  \\PYZsh{} t21}\n",
       "      \\PY{c+c1}{\\PYZsh{} t21 = prims.matmul(t20, t5)  \\PYZsh{} t21}\n",
       "  \\PY{k}{del} \\PY{n}{t20}\\PY{p}{,} \\PY{n}{t5}\n",
       "  \\PY{n}{t22} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t19}\\PY{p}{,} \\PY{p}{(}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t22}\n",
       "    \\PY{c+c1}{\\PYZsh{} t22 = ltorch.reshape(t19, (\\PYZhy{}1, 64))  \\PYZsh{} t22}\n",
       "      \\PY{c+c1}{\\PYZsh{} t22 = prims.reshape(t19, (64, 64))  \\PYZsh{} t22}\n",
       "  \\PY{k}{del} \\PY{n}{t19}\n",
       "  \\PY{n}{t23} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{permute}\\PY{p}{(}\\PY{n}{t22}\\PY{p}{,} \\PY{p}{(}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{0}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t23}\n",
       "    \\PY{c+c1}{\\PYZsh{} t23 = ltorch.permute(t22, (1, 0))  \\PYZsh{} t23}\n",
       "      \\PY{c+c1}{\\PYZsh{} t23 = prims.transpose(t22, (1, 0))  \\PYZsh{} t23}\n",
       "  \\PY{k}{del} \\PY{n}{t22}\n",
       "  \\PY{n}{t24} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{reshape}\\PY{p}{(}\\PY{n}{t2}\\PY{p}{,} \\PY{p}{(}\\PY{o}{\\PYZhy{}}\\PY{l+m+mi}{1}\\PY{p}{,} \\PY{l+m+mi}{64}\\PY{p}{)}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t24}\n",
       "    \\PY{c+c1}{\\PYZsh{} t24 = ltorch.reshape(t2, (\\PYZhy{}1, 64))  \\PYZsh{} t24}\n",
       "      \\PY{c+c1}{\\PYZsh{} t24 = prims.reshape(t2, (64, 64))  \\PYZsh{} t24}\n",
       "  \\PY{k}{del} \\PY{n}{t2}\n",
       "  \\PY{n}{t25} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{matmul}\\PY{p}{(}\\PY{n}{t23}\\PY{p}{,} \\PY{n}{t24}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t25}\n",
       "    \\PY{c+c1}{\\PYZsh{} t25 = ltorch.matmul(t23, t24)  \\PYZsh{} t25}\n",
       "      \\PY{c+c1}{\\PYZsh{} t25 = prims.matmul(t23, t24)  \\PYZsh{} t25}\n",
       "  \\PY{k}{del} \\PY{n}{t23}\\PY{p}{,} \\PY{n}{t24}\n",
       "  \\PY{n}{t26} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{true\\PYZus{}divide}\\PY{p}{(}\\PY{n}{t16}\\PY{p}{,} \\PY{l+m+mi}{2}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t26}\n",
       "    \\PY{c+c1}{\\PYZsh{} t26 = ltorch.true\\PYZus{}divide(t16, 2)  \\PYZsh{} t26}\n",
       "      \\PY{c+c1}{\\PYZsh{} \\PYZus{} = prims.convert\\PYZus{}element\\PYZus{}type(2, float)}\n",
       "      \\PY{c+c1}{\\PYZsh{} t26 = prims.div(t16, 2.0)  \\PYZsh{} t26}\n",
       "  \\PY{k}{del} \\PY{n}{t16}\n",
       "  \\PY{n}{p27} \\PY{o}{=} \\PY{n}{torch\\PYZus{}reduce\\PYZus{}scatter\\PYZus{}prim\\PYZus{}impl}\\PY{p}{(}\\PY{n}{t26}\\PY{p}{,} \\PY{n}{\\PYZus{}DistributedReduceOps\\PYZus{}3}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}2}\\PY{p}{,} \\PY{k+kc}{True}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} p27}\n",
       "  \\PY{k}{del} \\PY{n}{t26}\n",
       "  \\PY{n}{t28} \\PY{o}{=} \\PY{n}{torch\\PYZus{}wait\\PYZus{}prim\\PYZus{}impl}\\PY{p}{(}\\PY{n}{p27}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t28}\n",
       "  \\PY{k}{del} \\PY{n}{p27}\n",
       "  \\PY{n}{t29} \\PY{o}{=} \\PY{n}{torch}\\PY{o}{.}\\PY{n}{true\\PYZus{}divide}\\PY{p}{(}\\PY{n}{t25}\\PY{p}{,} \\PY{l+m+mi}{2}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t29}\n",
       "    \\PY{c+c1}{\\PYZsh{} t29 = ltorch.true\\PYZus{}divide(t25, 2)  \\PYZsh{} t29}\n",
       "      \\PY{c+c1}{\\PYZsh{} \\PYZus{} = prims.convert\\PYZus{}element\\PYZus{}type(2, float)}\n",
       "      \\PY{c+c1}{\\PYZsh{} t29 = prims.div(t25, 2.0)  \\PYZsh{} t29}\n",
       "  \\PY{k}{del} \\PY{n}{t25}\n",
       "  \\PY{n}{p30} \\PY{o}{=} \\PY{n}{torch\\PYZus{}reduce\\PYZus{}scatter\\PYZus{}prim\\PYZus{}impl}\\PY{p}{(}\\PY{n}{t29}\\PY{p}{,} \\PY{n}{\\PYZus{}DistributedReduceOps\\PYZus{}3}\\PY{p}{,} \\PY{n}{\\PYZus{}torch\\PYZus{}distributed\\PYZus{}distributed\\PYZus{}c10d\\PYZus{}ProcessGroup\\PYZus{}2}\\PY{p}{,} \\PY{k+kc}{True}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} p30}\n",
       "  \\PY{k}{del} \\PY{n}{t29}\n",
       "  \\PY{n}{t31} \\PY{o}{=} \\PY{n}{torch\\PYZus{}wait\\PYZus{}prim\\PYZus{}impl}\\PY{p}{(}\\PY{n}{p30}\\PY{p}{)}  \\PY{c+c1}{\\PYZsh{} t31}\n",
       "  \\PY{k}{del} \\PY{n}{p30}\n",
       "  \\PY{k}{return} \\PY{p}{(}\\PY{n}{t10}\\PY{p}{,} \\PY{p}{(}\\PY{n}{t31}\\PY{p}{,} \\PY{n}{t28}\\PY{p}{,} \\PY{p}{\\PYZob{}}\\PY{l+s+s1}{\\PYZsq{}}\\PY{l+s+s1}{x}\\PY{l+s+s1}{\\PYZsq{}}\\PY{p}{:} \\PY{n}{t21}\\PY{p}{\\PYZcb{}}\\PY{p}{)}\\PY{p}{)}\n",
       "\\end{Verbatim}\n"
      ],
      "text/plain": [
       "# Constructed by Delete Last Used (took 0 milliseconds)\n",
       "import torch\n",
       "import torch.nn.functional\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast()\n",
       "def _value_and_grad(*args, **kwargs):\n",
       "  # args \n",
       "  # kwargs \n",
       "  t0, \\\n",
       "  t1, \\\n",
       "  = args\n",
       "  del args\n",
       "  t2 = kwargs['x']\n",
       "  del kwargs\n",
       "  t3 = torch.full((64, 64), 1, device=torch.device(\"cuda:0\"), dtype=torch.float32)  # t3\n",
       "    # t3 = ltorch.full((64, 64), 1, device=torch.device(\"cuda:0\"), dtype=torch.float32)  # t3\n",
       "      # t3 = prims.full((64, 64), 1, device=devices.Device(\"cuda:0\"), dtype=dtypes.float32)  # t3\n",
       "  p4 = torch_all_gather_prim_impl(t0, _torch_distributed_distributed_c10d_ProcessGroup_2, True)  # p4\n",
       "  del t0\n",
       "  t5 = torch_wait_prim_impl(p4)  # t5\n",
       "  del p4\n",
       "  p6 = torch_all_gather_prim_impl(t1, _torch_distributed_distributed_c10d_ProcessGroup_2, True)  # p6\n",
       "  del t1\n",
       "  t7 = torch_wait_prim_impl(p6)  # t7\n",
       "  del p6\n",
       "  t8 = torch.nn.functional.linear(t2, t5, None)  # t8\n",
       "    # t8 = ltorch.linear(t2, t5, None)  # t8\n",
       "      # t8 = prims.linear(t2, t5, None)  # t8\n",
       "  t9 = torch.tanh(t8)  # t9\n",
       "    # t9 = ltorch.tanh(t8)  # t9\n",
       "      # t9 = prims.tanh(t8)  # t9\n",
       "  del t8\n",
       "  t10 = torch.nn.functional.linear(t9, t7, None)  # t10\n",
       "    # t10 = ltorch.linear(t9, t7, None)  # t10\n",
       "      # t10 = prims.linear(t9, t7, None)  # t10\n",
       "  t11 = torch.reshape(t3, (-1, 64))  # t11\n",
       "    # t11 = ltorch.reshape(t3, (-1, 64))  # t11\n",
       "      # t11 = prims.reshape(t3, (64, 64))  # t11\n",
       "  t12 = torch.matmul(t11, t7)  # t12\n",
       "    # t12 = ltorch.matmul(t11, t7)  # t12\n",
       "      # t12 = prims.matmul(t11, t7)  # t12\n",
       "  del t11, t7\n",
       "  t13 = torch.reshape(t3, (-1, 64))  # t13\n",
       "    # t13 = ltorch.reshape(t3, (-1, 64))  # t13\n",
       "      # t13 = prims.reshape(t3, (64, 64))  # t13\n",
       "  del t3\n",
       "  t14 = torch.permute(t13, (1, 0))  # t14\n",
       "    # t14 = ltorch.permute(t13, (1, 0))  # t14\n",
       "      # t14 = prims.transpose(t13, (1, 0))  # t14\n",
       "  del t13\n",
       "  t15 = torch.reshape(t9, (-1, 64))  # t15\n",
       "    # t15 = ltorch.reshape(t9, (-1, 64))  # t15\n",
       "      # t15 = prims.reshape(t9, (64, 64))  # t15\n",
       "  t16 = torch.matmul(t14, t15)  # t16\n",
       "    # t16 = ltorch.matmul(t14, t15)  # t16\n",
       "      # t16 = prims.matmul(t14, t15)  # t16\n",
       "  del t14, t15\n",
       "  t17 = torch.mul(t9, t9)  # t17\n",
       "    # t17 = ltorch.mul(t9, t9)  # t17\n",
       "      # t17 = prims.mul(t9, t9)  # t17\n",
       "  del t9\n",
       "  t18 = torch.sub(1.0, t17)  # t18\n",
       "    # t18 = ltorch.sub(1.0, t17, alpha=None)  # t18\n",
       "      # t18 = prims.sub(1.0, t17)  # t18\n",
       "  del t17\n",
       "  t19 = torch.mul(t12, t18)  # t19\n",
       "    # t19 = ltorch.mul(t12, t18)  # t19\n",
       "      # t19 = prims.mul(t12, t18)  # t19\n",
       "  del t12, t18\n",
       "  t20 = torch.reshape(t19, (-1, 64))  # t20\n",
       "    # t20 = ltorch.reshape(t19, (-1, 64))  # t20\n",
       "      # t20 = prims.reshape(t19, (64, 64))  # t20\n",
       "  t21 = torch.matmul(t20, t5)  # t21\n",
       "    # t21 = ltorch.matmul(t20, t5)  # t21\n",
       "      # t21 = prims.matmul(t20, t5)  # t21\n",
       "  del t20, t5\n",
       "  t22 = torch.reshape(t19, (-1, 64))  # t22\n",
       "    # t22 = ltorch.reshape(t19, (-1, 64))  # t22\n",
       "      # t22 = prims.reshape(t19, (64, 64))  # t22\n",
       "  del t19\n",
       "  t23 = torch.permute(t22, (1, 0))  # t23\n",
       "    # t23 = ltorch.permute(t22, (1, 0))  # t23\n",
       "      # t23 = prims.transpose(t22, (1, 0))  # t23\n",
       "  del t22\n",
       "  t24 = torch.reshape(t2, (-1, 64))  # t24\n",
       "    # t24 = ltorch.reshape(t2, (-1, 64))  # t24\n",
       "      # t24 = prims.reshape(t2, (64, 64))  # t24\n",
       "  del t2\n",
       "  t25 = torch.matmul(t23, t24)  # t25\n",
       "    # t25 = ltorch.matmul(t23, t24)  # t25\n",
       "      # t25 = prims.matmul(t23, t24)  # t25\n",
       "  del t23, t24\n",
       "  t26 = torch.true_divide(t16, 2)  # t26\n",
       "    # t26 = ltorch.true_divide(t16, 2)  # t26\n",
       "      # _ = prims.convert_element_type(2, float)\n",
       "      # t26 = prims.div(t16, 2.0)  # t26\n",
       "  del t16\n",
       "  p27 = torch_reduce_scatter_prim_impl(t26, _DistributedReduceOps_3, _torch_distributed_distributed_c10d_ProcessGroup_2, True)  # p27\n",
       "  del t26\n",
       "  t28 = torch_wait_prim_impl(p27)  # t28\n",
       "  del p27\n",
       "  t29 = torch.true_divide(t25, 2)  # t29\n",
       "    # t29 = ltorch.true_divide(t25, 2)  # t29\n",
       "      # _ = prims.convert_element_type(2, float)\n",
       "      # t29 = prims.div(t25, 2.0)  # t29\n",
       "  del t25\n",
       "  p30 = torch_reduce_scatter_prim_impl(t29, _DistributedReduceOps_3, _torch_distributed_distributed_c10d_ProcessGroup_2, True)  # p30\n",
       "  del t29\n",
       "  t31 = torch_wait_prim_impl(p30)  # t31\n",
       "  del p30\n",
       "  return (t10, (t31, t28, {'x': t21}))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimized_trace = thunder.transform_for_execution(forward_backward_trace, executors_list=thunder.get_always_executors())\n",
    "\n",
    "# Grab the final trace\n",
    "exec_trace = optimized_trace[-1]\n",
    "wrap_as_highlighted_code(exec_trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4 : Running the actual computation\n",
    "\n",
    "Running the actual computation will require setting up 2 processes and running our above code in both those processes (which can be tricky with Jupyter Notebook). Instead, we will write a small script and run it with `torchrun` which takes care of setting up the processes and relevant state.\n",
    "\n",
    "**NOTE**: This requires device running this notebook to have at least 2-GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below, we will use `thunder.distributed.fsdp` which does the same as what we did above (with some extra checks). The code below should look familiar as it is roughly all the above pieces in a single script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting thunder_fsdp_simple_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile thunder_fsdp_simple_example.py\n",
    "\n",
    "# imports\n",
    "from thunder.tests.lit_gpt_model import GPT, Config\n",
    "import torch\n",
    "import torch.distributed\n",
    "import thunder\n",
    "import thunder.distributed\n",
    "import os\n",
    "\n",
    "# # # # # # # #\n",
    "# Create Model\n",
    "# # # # # # # #\n",
    "\n",
    "# NOTE: We create the model on CPU.\n",
    "device='cpu'\n",
    "dim = 64\n",
    "def create_model():\n",
    "    layers = []\n",
    "    layers.append(torch.nn.Linear(dim, dim))\n",
    "    layers.append(torch.nn.ReLU())\n",
    "    layers.append(torch.nn.Linear(dim, dim))\n",
    "    return torch.nn.Sequential(*layers).to(device)\n",
    "\n",
    "# Model\n",
    "model = create_model()\n",
    "# Input\n",
    "x = torch.randn(dim, dim, device=device)\n",
    "\n",
    "# # # # # # # #\n",
    "# Setup for distributed\n",
    "# # # # # # # #\n",
    "torch.distributed.init_process_group(backend='nccl')\n",
    "\n",
    "rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "\n",
    "device = f\"cuda:{rank}\"\n",
    "\n",
    "# # # # # # # #\n",
    "# Move inputs to correct device\n",
    "# # # # # # # #\n",
    "x = x.to(device)\n",
    "\n",
    "# # # # # # # #\n",
    "# Wrap the model in thunder.distributed.fsdp\n",
    "# # # # # # # #\n",
    "\n",
    "# thunder.distributed.fsdp takes care of moving the parameter\n",
    "# shard to the correct GPU for the current process.\n",
    "cmodel = thunder.jit(thunder.distributed.fsdp(model))\n",
    "\n",
    "# Run the forward pass.\n",
    "cmodel(x)\n",
    "\n",
    "# # # # # # # #\n",
    "# Check the traces\n",
    "# # # # # # # #\n",
    "fwd_traces, bwd_traces = thunder.last_traces(cmodel)\n",
    "\n",
    "# # # # # # # #\n",
    "# Print and check to see if they match ours\n",
    "# # # # # # # #\n",
    "if rank == 0:\n",
    "    print(fwd_traces[-1])\n",
    "    print(\"*******\"* 8)\n",
    "    print(bwd_traces[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run the above script and check what the trace looks like.\n",
    "\n",
    "We can observe that forward trace has `torch_all_gather_prim_impl` to gather the parameter before forward pass and the backward trace has `torch_reduce_scatter_prim_impl` to reduce and scatter the gradients back to different GPUs. This is similar to our implementation above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-06 15:59:54,829] torch.distributed.run: [WARNING] \n",
      "[2024-03-06 15:59:54,829] torch.distributed.run: [WARNING] *****************************************\n",
      "[2024-03-06 15:59:54,829] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "[2024-03-06 15:59:54,829] torch.distributed.run: [WARNING] *****************************************\n",
      "/home/kkalambarkar/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/kkalambarkar/miniconda3/envs/pytorch-dev/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "# Constructed by Delete Last Used (took 0 milliseconds)\n",
      "import torch\n",
      "import torch.nn.functional\n",
      "from thunder.executors.torchex import no_autocast\n",
      "\n",
      "@torch.no_grad()\n",
      "@no_autocast()\n",
      "def augmented_forward_fn(t_0_weight, t_0_bias, t_0, t_2_weight, t_2_bias):\n",
      "  # t_0_weight \n",
      "  p0 = torch_all_gather_prim_impl(t_0_weight, _torch_distributed_distributed_c10d_ProcessGroup_0, True)  # p0\n",
      "  # t_0_bias \n",
      "  p2 = torch_all_gather_prim_impl(t_0_bias, _torch_distributed_distributed_c10d_ProcessGroup_0, True)  # p2\n",
      "  # t_0 \n",
      "  # t_2_weight \n",
      "  p7 = torch_all_gather_prim_impl(t_2_weight, _torch_distributed_distributed_c10d_ProcessGroup_0, True)  # p7\n",
      "  # t_2_bias \n",
      "  p9 = torch_all_gather_prim_impl(t_2_bias, _torch_distributed_distributed_c10d_ProcessGroup_0, True)  # p9\n",
      "  t1 = torch_wait_prim_impl(p0)  # t1\n",
      "  del p0\n",
      "  t3 = torch_wait_prim_impl(p2)  # t3\n",
      "  del p2\n",
      "  t4 = torch.nn.functional.linear(t_0, t1, t3)  # t4\n",
      "    # t4 = ltorch.linear(t_0, t1, t3)  # t4\n",
      "      # t4 = prims.linear(t_0, t1, t3)  # t4\n",
      "  del t1, t3\n",
      "  [t5, t6] = nvFusion0(t4)\n",
      "    # t5 = prims.gt(t4, 0.0)  # t5\n",
      "    # t6 = prims.where(t5, t4, 0.0)  # t6\n",
      "  del t4\n",
      "  t8 = torch_wait_prim_impl(p7)  # t8\n",
      "  del p7\n",
      "  t10 = torch_wait_prim_impl(p9)  # t10\n",
      "  del p9\n",
      "  t11 = torch.nn.functional.linear(t6, t8, t10)  # t11\n",
      "    # t11 = ltorch.linear(t6, t8, t10)  # t11\n",
      "      # t11 = prims.linear(t6, t8, t10)  # t11\n",
      "  del t10\n",
      "  return {'output': (t11, ()), 'flat_args': [t_0_weight, t_0_bias, t_0, t_2_weight, t_2_bias], 'flat_output': (t11,)}, ((t5, t6, t8, t_0), ())\n",
      "********************************************************\n",
      "# Constructed by Delete Last Used (took 0 milliseconds)\n",
      "import torch\n",
      "from thunder.executors.torchex import no_autocast\n",
      "\n",
      "@torch.no_grad()\n",
      "@no_autocast()\n",
      "def backward_fn(saved_for_backward, cotangents):\n",
      "  # saved_for_backward \n",
      "  # cotangents \n",
      "  C0, \\\n",
      "  _, \\\n",
      "  = saved_for_backward\n",
      "  clear_collection(saved_for_backward)\n",
      "  del saved_for_backward\n",
      "  t0, \\\n",
      "  = cotangents\n",
      "  clear_collection(cotangents)\n",
      "  del cotangents\n",
      "  t5, \\\n",
      "  t6, \\\n",
      "  t8, \\\n",
      "  t_0, \\\n",
      "  = C0\n",
      "  clear_collection(C0)\n",
      "  del C0\n",
      "  t31 = torch.reshape(t0, (-1, 64))  # t31\n",
      "    # t31 = ltorch.reshape(t0, (-1, 64))  # t31\n",
      "      # t31 = prims.reshape(t0, (64, 64))  # t31\n",
      "  t32 = torch.permute(t31, (1, 0))  # t32\n",
      "    # t32 = ltorch.permute(t31, (1, 0))  # t32\n",
      "      # t32 = prims.transpose(t31, (1, 0))  # t32\n",
      "  t33 = torch.reshape(t6, (-1, 64))  # t33\n",
      "    # t33 = ltorch.reshape(t6, (-1, 64))  # t33\n",
      "      # t33 = prims.reshape(t6, (64, 64))  # t33\n",
      "  del t6\n",
      "  t48 = torch.reshape(t_0, (-1, 64))  # t48\n",
      "    # t48 = ltorch.reshape(t_0, (-1, 64))  # t48\n",
      "      # t48 = prims.reshape(t_0, (64, 64))  # t48\n",
      "  del t_0\n",
      "  [t36] = nvFusion0(t0)\n",
      "    # t35 = prims.sum(t0, (0,))  # t35\n",
      "    # t36 = prims.div(t35, 2.0)  # t36\n",
      "  del t0\n",
      "  p37 = torch_reduce_scatter_prim_impl(t36, _DistributedReduceOps_0, _torch_distributed_distributed_c10d_ProcessGroup_1, True)  # p37\n",
      "  del t36\n",
      "  t30 = torch.matmul(t31, t8)  # t30\n",
      "    # t30 = ltorch.matmul(t29, t8)  # t30\n",
      "      # t30 = prims.matmul(t29, t8)  # t30\n",
      "  del t31, t8\n",
      "  t34 = torch.matmul(t32, t33)  # t34\n",
      "    # t34 = ltorch.matmul(t32, t33)  # t34\n",
      "      # t34 = prims.matmul(t32, t33)  # t34\n",
      "  del t32, t33\n",
      "  [t39, t42, t51] = nvFusion1(t30, t34, t5)\n",
      "    # t42 = prims.where(t5, t30, 0.0)  # t42\n",
      "    # t50 = prims.sum(t42, (0,))  # t50\n",
      "    # t51 = prims.div(t50, 2.0)  # t51\n",
      "    # t39 = prims.div(t34, 2.0)  # t39\n",
      "  del t30, t34, t5\n",
      "  p40 = torch_reduce_scatter_prim_impl(t39, _DistributedReduceOps_0, _torch_distributed_distributed_c10d_ProcessGroup_1, True)  # p40\n",
      "  del t39\n",
      "  p52 = torch_reduce_scatter_prim_impl(t51, _DistributedReduceOps_0, _torch_distributed_distributed_c10d_ProcessGroup_1, True)  # p52\n",
      "  del t51\n",
      "  t46 = torch.reshape(t42, (-1, 64))  # t46\n",
      "    # t46 = ltorch.reshape(t42, (-1, 64))  # t46\n",
      "      # t46 = prims.reshape(t42, (64, 64))  # t46\n",
      "  del t42\n",
      "  t47 = torch.permute(t46, (1, 0))  # t47\n",
      "    # t47 = ltorch.permute(t46, (1, 0))  # t47\n",
      "      # t47 = prims.transpose(t46, (1, 0))  # t47\n",
      "  del t46\n",
      "  t49 = torch.matmul(t47, t48)  # t49\n",
      "    # t49 = ltorch.matmul(t47, t48)  # t49\n",
      "      # t49 = prims.matmul(t47, t48)  # t49\n",
      "  del t47, t48\n",
      "  [t54] = nvFusion2(t49)\n",
      "    # t54 = prims.div(t49, 2.0)  # t54\n",
      "  del t49\n",
      "  p55 = torch_reduce_scatter_prim_impl(t54, _DistributedReduceOps_0, _torch_distributed_distributed_c10d_ProcessGroup_1, True)  # p55\n",
      "  del t54\n",
      "  t38 = torch_wait_prim_impl(p37)  # t38\n",
      "  del p37\n",
      "  t41 = torch_wait_prim_impl(p40)  # t41\n",
      "  del p40\n",
      "  t53 = torch_wait_prim_impl(p52)  # t53\n",
      "  del p52\n",
      "  t56 = torch_wait_prim_impl(p55)  # t56\n",
      "  del p55\n",
      "  return (t56, t53, None, t41, t38)\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 thunder_fsdp_simple_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "We have created our implementation of FSDP to shard our model across multiple GPUs. In the process, we also learned that \n",
    "1. `thunder` provides us with primitives for synchronization across mutiple GPUs.\n",
    "2. `thunder` also takes care of implementing the backward support, so we don't have to explicitly do anything to get the backward working.\n",
    "3. We can just easily apply `thunder.distributed.fsdp` to our model and it will take care of sharding the parameters and also adding synchronizations to our model. Also, we can easily check the modifications by inspecting the traces."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
