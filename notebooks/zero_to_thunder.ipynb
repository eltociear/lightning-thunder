{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1638964c",
   "metadata": {},
   "source": [
    "# Zero to Thunder\n",
    "\n",
    "Here we take a very short tour of what is possible with Thunder.\n",
    "\n",
    "To get started we import it (and a bunch of things for this notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28b99b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "import inspect\n",
    "\n",
    "\n",
    "import torch, thunder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f87aba",
   "metadata": {},
   "source": [
    "## Compiling a first module with Thunder\n",
    "\n",
    "So let's get started! As a \"Hello World\", let us apply it to it to a small model, say, the MLP part found in Llama 2. We take it from LitGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6ca6328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLaMAMLP(\n",
      "  (fc_1): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (fc_2): Linear(in_features=4096, out_features=11008, bias=False)\n",
      "  (proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class LLaMAMLP(torch.nn.Module):\n",
    "    def __init__(self, n_embd, intermediate_size) -> None:\n",
    "        super().__init__()\n",
    "        self.fc_1 = torch.nn.Linear(n_embd, intermediate_size, bias=False)\n",
    "        self.fc_2 = torch.nn.Linear(n_embd, intermediate_size, bias=False)\n",
    "        self.proj = torch.nn.Linear(intermediate_size, n_embd, bias=False)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_fc_1 = self.fc_1(x)\n",
    "        x_fc_2 = self.fc_2(x)\n",
    "        x = torch.nn.functional.silu(x_fc_1) * x_fc_2\n",
    "        return self.proj(x)\n",
    "\n",
    "\n",
    "with torch.device(\"cuda\"):\n",
    "    m = LLaMAMLP(4096, 11008)\n",
    "for p in m.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702ea054",
   "metadata": {},
   "source": [
    "Now we can apply Thunder. This uses the most important function of Thunder, `thunder.jit`, which can be used to compile a `torch.nn.Module` or a function. It will wrap our MLP in a `ThunderModule`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "67ca2d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "thunder_model = thunder.jit(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "964e2689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ThunderModule(\n",
       "  (_model): LLaMAMLP(\n",
       "    (fc_1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "    (fc_2): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "    (proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thunder_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db20f6",
   "metadata": {},
   "source": [
    "Our Thunder module computes (up to numerical accuracy) the same thing as our original model and for a small model like this, it also has approximately the same performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f4de1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deviation: 1.4901161193847656e-07\n",
      "58.2 ms ± 306 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "58.7 ms ± 50.9 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(2, 2048, 4096, device=\"cuda\")\n",
    "print('deviation:', (thunder_model(x) - m(x)).abs().max().item())\n",
    "\n",
    "%timeit thunder_model(x); torch.cuda.synchronize()\n",
    "%timeit m(x); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8835543e",
   "metadata": {},
   "source": [
    "So what has changed?\n",
    "Quite a bit!\n",
    "\n",
    "When we call the Thunder module, it do the computation in a single function without control flow. And what's more, it applies optimizations, such as creating fusions for NVFuser to execute. We can see all this by showing the last computation trace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6f4b77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 0 milliseconds)\n",
       "import torch\n",
       "import torch.nn.functional\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast()\n",
       "def computation(x, t_fc_1_weight, t_fc_2_weight, t_proj_weight):\n",
       "  # x: \"cuda:0 f32[2, 2048, 4096]\" \n",
       "  # t_fc_1_weight: \"cuda:0 f32[11008, 4096]\" \n",
       "  # t_fc_2_weight: \"cuda:0 f32[11008, 4096]\" \n",
       "  # t_proj_weight: \"cuda:0 f32[4096, 11008]\" \n",
       "  x_fc_1 = torch.nn.functional.linear(x, t_fc_1_weight, None)  # x_fc_1: \"cuda:0 f32[2, 2048, 11008]\"\n",
       "    # x_fc_1 = ltorch.linear(x, t_fc_1_weight, None)  # x_fc_1: \"cuda:0 f32[2, 2048, 11008]\"\n",
       "      # x_fc_1 = prims.linear(x, t_fc_1_weight, None)  # x_fc_1: \"cuda:0 f32[2, 2048, 11008]\"\n",
       "  del t_fc_1_weight\n",
       "  x_fc_2 = torch.nn.functional.linear(x, t_fc_2_weight, None)  # x_fc_2: \"cuda:0 f32[2, 2048, 11008]\"\n",
       "    # x_fc_2 = ltorch.linear(x, t_fc_2_weight, None)  # x_fc_2: \"cuda:0 f32[2, 2048, 11008]\"\n",
       "      # x_fc_2 = prims.linear(x, t_fc_2_weight, None)  # x_fc_2: \"cuda:0 f32[2, 2048, 11008]\"\n",
       "  del x, t_fc_2_weight\n",
       "  [result] = nvFusion0(x_fc_1, x_fc_2)\n",
       "    # t9 = prims.neg(x_fc_1)  # t9: \"cuda:0 f32[2, 2048, 11008]\"\n",
       "    # t10 = prims.exp(t9)  # t10: \"cuda:0 f32[2, 2048, 11008]\"\n",
       "    # t11 = prims.add(1.0, t10)  # t11: \"cuda:0 f32[2, 2048, 11008]\"\n",
       "    # t12 = prims.reciprocal(t11)  # t12: \"cuda:0 f32[2, 2048, 11008]\"\n",
       "    # a = prims.mul(x_fc_1, t12)  # a: \"cuda:0 f32[2, 2048, 11008]\"\n",
       "    # result = prims.mul(a, x_fc_2)  # result: \"cuda:0 f32[2, 2048, 11008]\"\n",
       "  del x_fc_1, x_fc_2\n",
       "  t18 = torch.nn.functional.linear(result, t_proj_weight, None)  # t18: \"cuda:0 f32[2, 2048, 4096]\"\n",
       "    # t18 = ltorch.linear(result, t_proj_weight, None)  # t18: \"cuda:0 f32[2, 2048, 4096]\"\n",
       "      # t18 = prims.linear(result, t_proj_weight, None)  # t18: \"cuda:0 f32[2, 2048, 4096]\"\n",
       "  del result, t_proj_weight\n",
       "  return t18"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thunder.last_traces(thunder_model)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0071924",
   "metadata": {},
   "source": [
    "For more detail of what is going on in this trace:\n",
    "- Thunder has transformed the computation (more precisely, `m.__call__`) into a single function which has all the MLP parameters as arguments.\n",
    "- It has recorded the tensor metadata.\n",
    "- Operations have been mapped from the PyTorch functions to `thunder.torch`(aka `ltorch`) equivalents and decomposed into _primitive operations_.\n",
    "- The multiplication and activation (`x = torch.nn.functional.silu(x_fc_1) * x_fc_2`have been put into one NVFuser fusion. (NVFuser here is (a particularly important) one of many optimizations, and we make it easy to add your own.) \n",
    "- You can see how the parameters are obtained and the metadata is checked in the prologue - get it through `thunder.last_prologue_traces(thunder_model)[-1]`.\n",
    "\n",
    "You can actually see the series of traces, `last_traces` gives you a list of transformed traces in chronological order - for example the initial trace `thunder.last_traces(thunder_model)[0]` does not have the fusion yet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7749aed1",
   "metadata": {},
   "source": [
    "## Compiling a more complex model\n",
    "\n",
    "Obviously, we aim for larger models, so we can do the same with the entire LLama 2 (well, we have a smaller momdel here to be mild to our CI, but if you have a large GPU, just drop reducing the number of layers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d53e0c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(32000, 4096)\n",
       "    (h): ModuleList(\n",
       "      (0-3): 4 x Block(\n",
       "        (norm_1): RMSNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (attn): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): RMSNorm()\n",
       "        (mlp): LLaMAMLP(\n",
       "          (fc_1): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (fc_2): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): RMSNorm()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lit_gpt import GPT\n",
    "from thunder.tests.lit_gpt_model import Config\n",
    "cfg = Config.from_name('Llama-2-7b-hf')\n",
    "cfg.n_layer = 4 # fewer layers\n",
    "with torch.device('cuda'):\n",
    "    m = GPT(cfg)\n",
    "m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e536a4aa",
   "metadata": {},
   "source": [
    "Again we jit our model and compare the output..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36a7be96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deviation: 1.8477439880371094e-06\n"
     ]
    }
   ],
   "source": [
    "thunder_model = thunder.jit(m)\n",
    "\n",
    "inp = torch.randint(1, m.config.vocab_size, (1, 512), device=\"cuda\")\n",
    "\n",
    "actual = thunder_model(inp)\n",
    "expected = m(inp)\n",
    "\n",
    "print(\"deviation:\", (actual - expected).abs().max().item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f681093",
   "metadata": {},
   "source": [
    "Just like before, we can see the program it ran:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac7e8bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 1 milliseconds)\n",
       "import torch\n",
       "from torch import Tensor\n",
       "import torch.nn.functional\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast()\n",
       "def augmented_forward_fn(*args):\n",
       "  # args: \"Collection\" \n",
       "  t0, \\\n",
       "  t1, \\\n",
       "  t2, \\\n",
       "  t3, \\\n",
       "  t4, \\\n",
       "  t5, \\\n",
       "  t6, \\\n",
       "  t7, \\\n",
       "  t8, \\\n",
       "  t9, \\\n",
       "  t10, \\\n",
       "  t11, \\\n",
       "  t12, \\\n",
       "  t13, \\\n",
       "  t14, \\\n",
       "  t15, \\\n",
       "  t16, \\\n",
       "  t17, \\\n",
       "  t18, \\\n",
       "  t19, \\\n",
       "  t20, \\\n",
       "  t21, \\\n",
       "  t22, \\\n",
       "  t23, \\\n",
       "  t24, \\\n",
       "  t25, \\\n",
       "  t26, \\\n",
       "  t27, \\\n",
       "  t28, \\\n",
       "  t29, \\\n",
       "  t30, \\\n",
       "  t31, \\\n",
       "  t32, \\\n",
       "  t33, \\\n",
       "  = args\n",
       "  del args\n",
       "  t38 = torch.nn.functional.embedding(t0, t33, None, None, 2.0, False, False)  # t38: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t38 = ltorch.embedding(t0, t33, None, None, 2.0, False, False)  # t38: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t334 = ltorch.reshape(t0, [512])  # t334: \"cuda:0 i64[512]\"\n",
       "        # t334 = prims.reshape(t0, (512,))  # t334: \"cuda:0 i64[512]\"\n",
       "      # t335 = prims.take(t33, t334, 0)  # t335: \"cuda:0 f32[512, 4096]\"\n",
       "      # t38 = ltorch.reshape(t335, [1, 512, 4096])  # t38: \"cuda:0 f32[1, 512, 4096]\"\n",
       "        # t38 = prims.reshape(t335, (1, 512, 4096))  # t38: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  t34 = torch_slice_prim_impl(t1, [0, 0], [512, 128], [1, 1])  # t34: \"cuda:0 f32[512, 128]\"\n",
       "  t35 = torch_slice_prim_impl(t2, [0, 0], [512, 128], [1, 1])  # t35: \"cuda:0 f32[512, 128]\"\n",
       "  t374 = torch.unsqueeze(t17, 0)  # t374: \"cuda:0 f32[1, 4096]\"\n",
       "    # t374 = ltorch.unsqueeze(t17, 0)  # t374: \"cuda:0 f32[1, 4096]\"\n",
       "      # t374 = prims.broadcast_in_dim(t17, [1, 4096], [1])  # t374: \"cuda:0 f32[1, 4096]\"\n",
       "  t375 = torch.unsqueeze(t374, 1)  # t375: \"cuda:0 f32[1, 1, 4096]\"\n",
       "    # t375 = ltorch.unsqueeze(t374, 1)  # t375: \"cuda:0 f32[1, 1, 4096]\"\n",
       "      # t375 = prims.broadcast_in_dim(t374, [1, 1, 4096], [0, 2])  # t375: \"cuda:0 f32[1, 1, 4096]\"\n",
       "  del t374\n",
       "  t47 = Tensor.expand(t375, (1, 512, 4096))  # t47: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t47 = ltorch.expand(t375, (1, 512, 4096))  # t47: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t47 = prims.broadcast_in_dim(t375, (1, 512, 4096), (0, 1, 2))  # t47: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t375\n",
       "  t475 = torch.unsqueeze(t24, 0)  # t475: \"cuda:0 f32[1, 4096]\"\n",
       "    # t475 = ltorch.unsqueeze(t24, 0)  # t475: \"cuda:0 f32[1, 4096]\"\n",
       "      # t475 = prims.broadcast_in_dim(t24, [1, 4096], [1])  # t475: \"cuda:0 f32[1, 4096]\"\n",
       "  t476 = torch.unsqueeze(t475, 1)  # t476: \"cuda:0 f32[1, 1, 4096]\"\n",
       "    # t476 = ltorch.unsqueeze(t475, 1)  # t476: \"cuda:0 f32[1, 1, 4096]\"\n",
       "      # t476 = prims.broadcast_in_dim(t475, [1, 1, 4096], [0, 2])  # t476: \"cuda:0 f32[1, 1, 4096]\"\n",
       "  del t475\n",
       "  t311 = Tensor.expand(t476, (1, 512, 4096))  # t311: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t311 = ltorch.expand(t476, (1, 512, 4096))  # t311: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t311 = prims.broadcast_in_dim(t476, (1, 512, 4096), (0, 1, 2))  # t311: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t476\n",
       "  t478 = torch.unsqueeze(t16, 0)  # t478: \"cuda:0 f32[1, 4096]\"\n",
       "    # t478 = ltorch.unsqueeze(t16, 0)  # t478: \"cuda:0 f32[1, 4096]\"\n",
       "      # t478 = prims.broadcast_in_dim(t16, [1, 4096], [1])  # t478: \"cuda:0 f32[1, 4096]\"\n",
       "  t479 = torch.unsqueeze(t478, 1)  # t479: \"cuda:0 f32[1, 1, 4096]\"\n",
       "    # t479 = ltorch.unsqueeze(t478, 1)  # t479: \"cuda:0 f32[1, 1, 4096]\"\n",
       "      # t479 = prims.broadcast_in_dim(t478, [1, 1, 4096], [0, 2])  # t479: \"cuda:0 f32[1, 1, 4096]\"\n",
       "  del t478\n",
       "  t331 = Tensor.expand(t479, (1, 512, 4096))  # t331: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t331 = ltorch.expand(t479, (1, 512, 4096))  # t331: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t331 = prims.broadcast_in_dim(t479, (1, 512, 4096), (0, 1, 2))  # t331: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t479\n",
       "  t403 = torch.unsqueeze(t21, 0)  # t403: \"cuda:0 f32[1, 4096]\"\n",
       "    # t403 = ltorch.unsqueeze(t21, 0)  # t403: \"cuda:0 f32[1, 4096]\"\n",
       "      # t403 = prims.broadcast_in_dim(t21, [1, 4096], [1])  # t403: \"cuda:0 f32[1, 4096]\"\n",
       "  t404 = torch.unsqueeze(t403, 1)  # t404: \"cuda:0 f32[1, 1, 4096]\"\n",
       "    # t404 = ltorch.unsqueeze(t403, 1)  # t404: \"cuda:0 f32[1, 1, 4096]\"\n",
       "      # t404 = prims.broadcast_in_dim(t403, [1, 1, 4096], [0, 2])  # t404: \"cuda:0 f32[1, 1, 4096]\"\n",
       "  del t403\n",
       "  t98 = Tensor.expand(t404, (1, 512, 4096))  # t98: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t98 = ltorch.expand(t404, (1, 512, 4096))  # t98: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t98 = prims.broadcast_in_dim(t404, (1, 512, 4096), (0, 1, 2))  # t98: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t404\n",
       "  t406 = torch.unsqueeze(t18, 0)  # t406: \"cuda:0 f32[1, 4096]\"\n",
       "    # t406 = ltorch.unsqueeze(t18, 0)  # t406: \"cuda:0 f32[1, 4096]\"\n",
       "      # t406 = prims.broadcast_in_dim(t18, [1, 4096], [1])  # t406: \"cuda:0 f32[1, 4096]\"\n",
       "  t407 = torch.unsqueeze(t406, 1)  # t407: \"cuda:0 f32[1, 1, 4096]\"\n",
       "    # t407 = ltorch.unsqueeze(t406, 1)  # t407: \"cuda:0 f32[1, 1, 4096]\"\n",
       "      # t407 = prims.broadcast_in_dim(t406, [1, 1, 4096], [0, 2])  # t407: \"cuda:0 f32[1, 1, 4096]\"\n",
       "  del t406\n",
       "  t118 = Tensor.expand(t407, (1, 512, 4096))  # t118: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t118 = ltorch.expand(t407, (1, 512, 4096))  # t118: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t118 = prims.broadcast_in_dim(t407, (1, 512, 4096), (0, 1, 2))  # t118: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t407\n",
       "  t427 = torch.unsqueeze(t22, 0)  # t427: \"cuda:0 f32[1, 4096]\"\n",
       "    # t427 = ltorch.unsqueeze(t22, 0)  # t427: \"cuda:0 f32[1, 4096]\"\n",
       "      # t427 = prims.broadcast_in_dim(t22, [1, 4096], [1])  # t427: \"cuda:0 f32[1, 4096]\"\n",
       "  t428 = torch.unsqueeze(t427, 1)  # t428: \"cuda:0 f32[1, 1, 4096]\"\n",
       "    # t428 = ltorch.unsqueeze(t427, 1)  # t428: \"cuda:0 f32[1, 1, 4096]\"\n",
       "      # t428 = prims.broadcast_in_dim(t427, [1, 1, 4096], [0, 2])  # t428: \"cuda:0 f32[1, 1, 4096]\"\n",
       "  del t427\n",
       "  t169 = Tensor.expand(t428, (1, 512, 4096))  # t169: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t169 = ltorch.expand(t428, (1, 512, 4096))  # t169: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t169 = prims.broadcast_in_dim(t428, (1, 512, 4096), (0, 1, 2))  # t169: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t428\n",
       "  t430 = torch.unsqueeze(t19, 0)  # t430: \"cuda:0 f32[1, 4096]\"\n",
       "    # t430 = ltorch.unsqueeze(t19, 0)  # t430: \"cuda:0 f32[1, 4096]\"\n",
       "      # t430 = prims.broadcast_in_dim(t19, [1, 4096], [1])  # t430: \"cuda:0 f32[1, 4096]\"\n",
       "  t431 = torch.unsqueeze(t430, 1)  # t431: \"cuda:0 f32[1, 1, 4096]\"\n",
       "    # t431 = ltorch.unsqueeze(t430, 1)  # t431: \"cuda:0 f32[1, 1, 4096]\"\n",
       "      # t431 = prims.broadcast_in_dim(t430, [1, 1, 4096], [0, 2])  # t431: \"cuda:0 f32[1, 1, 4096]\"\n",
       "  del t430\n",
       "  t189 = Tensor.expand(t431, (1, 512, 4096))  # t189: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t189 = ltorch.expand(t431, (1, 512, 4096))  # t189: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t189 = prims.broadcast_in_dim(t431, (1, 512, 4096), (0, 1, 2))  # t189: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t431\n",
       "  t451 = torch.unsqueeze(t23, 0)  # t451: \"cuda:0 f32[1, 4096]\"\n",
       "    # t451 = ltorch.unsqueeze(t23, 0)  # t451: \"cuda:0 f32[1, 4096]\"\n",
       "      # t451 = prims.broadcast_in_dim(t23, [1, 4096], [1])  # t451: \"cuda:0 f32[1, 4096]\"\n",
       "  t452 = torch.unsqueeze(t451, 1)  # t452: \"cuda:0 f32[1, 1, 4096]\"\n",
       "    # t452 = ltorch.unsqueeze(t451, 1)  # t452: \"cuda:0 f32[1, 1, 4096]\"\n",
       "      # t452 = prims.broadcast_in_dim(t451, [1, 1, 4096], [0, 2])  # t452: \"cuda:0 f32[1, 1, 4096]\"\n",
       "  del t451\n",
       "  t240 = Tensor.expand(t452, (1, 512, 4096))  # t240: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t240 = ltorch.expand(t452, (1, 512, 4096))  # t240: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t240 = prims.broadcast_in_dim(t452, (1, 512, 4096), (0, 1, 2))  # t240: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t452\n",
       "  t454 = torch.unsqueeze(t20, 0)  # t454: \"cuda:0 f32[1, 4096]\"\n",
       "    # t454 = ltorch.unsqueeze(t20, 0)  # t454: \"cuda:0 f32[1, 4096]\"\n",
       "      # t454 = prims.broadcast_in_dim(t20, [1, 4096], [1])  # t454: \"cuda:0 f32[1, 4096]\"\n",
       "  t455 = torch.unsqueeze(t454, 1)  # t455: \"cuda:0 f32[1, 1, 4096]\"\n",
       "    # t455 = ltorch.unsqueeze(t454, 1)  # t455: \"cuda:0 f32[1, 1, 4096]\"\n",
       "      # t455 = prims.broadcast_in_dim(t454, [1, 1, 4096], [0, 2])  # t455: \"cuda:0 f32[1, 1, 4096]\"\n",
       "  del t454\n",
       "  t260 = Tensor.expand(t455, (1, 512, 4096))  # t260: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t260 = ltorch.expand(t455, (1, 512, 4096))  # t260: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t260 = prims.broadcast_in_dim(t455, (1, 512, 4096), (0, 1, 2))  # t260: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t455\n",
       "  t395 = torch.unsqueeze(t34, 0)  # t395: \"cuda:0 f32[1, 512, 128]\"\n",
       "    # t395 = ltorch.unsqueeze(t34, 0)  # t395: \"cuda:0 f32[1, 512, 128]\"\n",
       "      # t395 = prims.broadcast_in_dim(t34, [1, 512, 128], [1, 2])  # t395: \"cuda:0 f32[1, 512, 128]\"\n",
       "  del t34\n",
       "  t396 = torch.unsqueeze(t395, 1)  # t396: \"cuda:0 f32[1, 1, 512, 128]\"\n",
       "    # t396 = ltorch.unsqueeze(t395, 1)  # t396: \"cuda:0 f32[1, 1, 512, 128]\"\n",
       "      # t396 = prims.broadcast_in_dim(t395, [1, 1, 512, 128], [0, 2, 3])  # t396: \"cuda:0 f32[1, 1, 512, 128]\"\n",
       "  del t395\n",
       "  t63 = Tensor.expand(t396, (1, 32, 512, 128))  # t63: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t63 = ltorch.expand(t396, (1, 32, 512, 128))  # t63: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t63 = prims.broadcast_in_dim(t396, (1, 32, 512, 128), (0, 1, 2, 3))  # t63: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t396\n",
       "  t398 = torch.unsqueeze(t35, 0)  # t398: \"cuda:0 f32[1, 512, 128]\"\n",
       "    # t398 = ltorch.unsqueeze(t35, 0)  # t398: \"cuda:0 f32[1, 512, 128]\"\n",
       "      # t398 = prims.broadcast_in_dim(t35, [1, 512, 128], [1, 2])  # t398: \"cuda:0 f32[1, 512, 128]\"\n",
       "  del t35\n",
       "  t399 = torch.unsqueeze(t398, 1)  # t399: \"cuda:0 f32[1, 1, 512, 128]\"\n",
       "    # t399 = ltorch.unsqueeze(t398, 1)  # t399: \"cuda:0 f32[1, 1, 512, 128]\"\n",
       "      # t399 = prims.broadcast_in_dim(t398, [1, 1, 512, 128], [0, 2, 3])  # t399: \"cuda:0 f32[1, 1, 512, 128]\"\n",
       "  del t398\n",
       "  t65 = Tensor.expand(t399, (1, 32, 512, 128))  # t65: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t65 = ltorch.expand(t399, (1, 32, 512, 128))  # t65: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t65 = prims.broadcast_in_dim(t399, (1, 32, 512, 128), (0, 1, 2, 3))  # t65: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t399\n",
       "  [t44, t48] = nvFusion0(t38, t47)\n",
       "    # t39 = prims.mul(t38, t38)  # t39: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t40 = prims.sum(t39, (2,))  # t40: \"cuda:0 f32[1, 512]\"\n",
       "    # t41 = prims.broadcast_in_dim(t40, [1, 512, 1], [0, 1])  # t41: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t42 = prims.div(t41, 4096.0)  # t42: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t43 = prims.add(t42, 1e-05)  # t43: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t44 = prims.rsqrt(t43)  # t44: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t45 = prims.broadcast_in_dim(t44, (1, 512, 4096), (0, 1, 2))  # t45: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t46 = prims.mul(t38, t45)  # t46: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t48 = prims.mul(t46, t47)  # t48: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  t49 = torch.nn.functional.linear(t48, t3, None)  # t49: \"cuda:0 f32[1, 512, 12288]\"\n",
       "    # t49 = ltorch.linear(t48, t3, None)  # t49: \"cuda:0 f32[1, 512, 12288]\"\n",
       "      # t49 = prims.linear(t48, t3, None)  # t49: \"cuda:0 f32[1, 512, 12288]\"\n",
       "  t50 = torch.reshape(t49, (1, 512, 32, 3, 128))  # t50: \"cuda:0 f32[1, 512, 32, 3, 128]\"\n",
       "    # t50 = ltorch.reshape(t49, (1, 512, 32, 3, 128))  # t50: \"cuda:0 f32[1, 512, 32, 3, 128]\"\n",
       "      # t50 = prims.reshape(t49, (1, 512, 32, 3, 128))  # t50: \"cuda:0 f32[1, 512, 32, 3, 128]\"\n",
       "  del t49\n",
       "  t51 = torch.permute(t50, (0, 2, 3, 1, 4))  # t51: \"cuda:0 f32[1, 32, 3, 512, 128]\"\n",
       "    # t51 = ltorch.permute(t50, (0, 2, 3, 1, 4))  # t51: \"cuda:0 f32[1, 32, 3, 512, 128]\"\n",
       "      # t51 = prims.transpose(t50, (0, 2, 3, 1, 4))  # t51: \"cuda:0 f32[1, 32, 3, 512, 128]\"\n",
       "  del t50\n",
       "  (t52, t53, t54) = torch.split(t51, (1, 1, 1), 2)\n",
       "    # (t52, t53, t54) = ltorch.split(t51, (1, 1, 1), 2)\n",
       "      # t52 = prims.slice_prim(t51, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t52: \"cuda:0 f32[1, 32, 1, 512, 128]\"\n",
       "      # t53 = prims.slice_prim(t51, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t53: \"cuda:0 f32[1, 32, 1, 512, 128]\"\n",
       "      # t54 = prims.slice_prim(t51, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t54: \"cuda:0 f32[1, 32, 1, 512, 128]\"\n",
       "  del t51\n",
       "  t55 = torch.reshape(t52, (1, 32, 512, 128))  # t55: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t55 = ltorch.reshape(t52, (1, 32, 512, 128))  # t55: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t55 = prims.reshape(t52, (1, 32, 512, 128))  # t55: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t52\n",
       "  t56 = torch.reshape(t53, (1, 32, 512, 128))  # t56: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t56 = ltorch.reshape(t53, (1, 32, 512, 128))  # t56: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t56 = prims.reshape(t53, (1, 32, 512, 128))  # t56: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t53\n",
       "  t57 = torch.reshape(t54, (1, 32, 512, 128))  # t57: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t57 = ltorch.reshape(t54, (1, 32, 512, 128))  # t57: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t57 = prims.reshape(t54, (1, 32, 512, 128))  # t57: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t54\n",
       "  t58 = torch_slice_prim_impl(t55, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t58: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  t68 = torch_slice_prim_impl(t56, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t68: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  t78 = torch_slice_prim_impl(t55, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t78: \"cuda:0 f32[1, 32, 512, 0]\"\n",
       "  del t55\n",
       "  t80 = torch_slice_prim_impl(t56, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t80: \"cuda:0 f32[1, 32, 512, 0]\"\n",
       "  del t56\n",
       "  t60 = torch_slice_prim_impl(t58, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t60: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  t59 = torch_slice_prim_impl(t58, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t59: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  t69 = torch_slice_prim_impl(t68, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t69: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  t70 = torch_slice_prim_impl(t68, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t70: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  [t61, t71] = nvFusion1(t60, t70)\n",
       "    # t61 = prims.neg(t60)  # t61: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "    # t71 = prims.neg(t70)  # t71: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  del t60, t70\n",
       "  t62 = torch.cat((t61, t59), -1)  # t62: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t62 = ltorch.cat((t61, t59), -1)  # t62: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t62 = prims.cat((t61, t59), -1)  # t62: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t61, t59\n",
       "  t72 = torch.cat((t71, t69), -1)  # t72: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t72 = ltorch.cat((t71, t69), -1)  # t72: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t72 = prims.cat((t71, t69), -1)  # t72: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t71, t69\n",
       "  [t67, t77] = nvFusion2(t58, t62, t63, t65, t68, t72)\n",
       "    # t64 = prims.mul(t58, t63)  # t64: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t66 = prims.mul(t62, t65)  # t66: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t67 = prims.add(t64, t66)  # t67: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t74 = prims.mul(t68, t63)  # t74: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t76 = prims.mul(t72, t65)  # t76: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t77 = prims.add(t74, t76)  # t77: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t58, t62, t68, t72\n",
       "  t79 = torch.cat((t67, t78), -1)  # t79: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t79 = ltorch.cat((t67, t78), -1)  # t79: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t79 = prims.cat((t67, t78), -1)  # t79: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t67, t78\n",
       "  t81 = torch.cat((t77, t80), -1)  # t81: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t81 = ltorch.cat((t77, t80), -1)  # t81: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t81 = prims.cat((t77, t80), -1)  # t81: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t77, t80\n",
       "  (t82, t83, t84, t85) = sdpaex_grad_forward_scaled_dot_product_efficient_attention(t79, t81, t57, None, 0.0, True, 0.08838834764831843)\n",
       "  t86 = torch.permute(t82, (0, 2, 1, 3))  # t86: \"cuda:0 f32[1, 512, 32, 128]\"\n",
       "    # t86 = ltorch.permute(t82, (0, 2, 1, 3))  # t86: \"cuda:0 f32[1, 512, 32, 128]\"\n",
       "      # t86 = prims.transpose(t82, (0, 2, 1, 3))  # t86: \"cuda:0 f32[1, 512, 32, 128]\"\n",
       "  t87 = torch.reshape(t86, (1, 512, 4096))  # t87: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t87 = ltorch.reshape(t86, (1, 512, 4096))  # t87: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t87 = prims.reshape(t86, (1, 512, 4096))  # t87: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t86\n",
       "  t88 = torch.nn.functional.linear(t87, t25, None)  # t88: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t88 = ltorch.linear(t87, t25, None)  # t88: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t88 = prims.linear(t87, t25, None)  # t88: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  [t89, t95, t99] = nvFusion3(t38, t88, t98)\n",
       "    # t89 = prims.add(t88, t38)  # t89: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t90 = prims.mul(t89, t89)  # t90: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t91 = prims.sum(t90, (2,))  # t91: \"cuda:0 f32[1, 512]\"\n",
       "    # t92 = prims.broadcast_in_dim(t91, [1, 512, 1], [0, 1])  # t92: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t93 = prims.div(t92, 4096.0)  # t93: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t94 = prims.add(t93, 1e-05)  # t94: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t95 = prims.rsqrt(t94)  # t95: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t96 = prims.broadcast_in_dim(t95, (1, 512, 4096), (0, 1, 2))  # t96: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t97 = prims.mul(t89, t96)  # t97: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t99 = prims.mul(t97, t98)  # t99: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t88\n",
       "  t101 = torch.nn.functional.linear(t99, t11, None)  # t101: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t101 = ltorch.linear(t99, t11, None)  # t101: \"cuda:0 f32[1, 512, 11008]\"\n",
       "      # t101 = prims.linear(t99, t11, None)  # t101: \"cuda:0 f32[1, 512, 11008]\"\n",
       "  t100 = torch.nn.functional.linear(t99, t7, None)  # t100: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t100 = ltorch.linear(t99, t7, None)  # t100: \"cuda:0 f32[1, 512, 11008]\"\n",
       "      # t100 = prims.linear(t99, t7, None)  # t100: \"cuda:0 f32[1, 512, 11008]\"\n",
       "  [t107] = nvFusion4(t100, t101)\n",
       "    # t102 = prims.neg(t100)  # t102: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t103 = prims.exp(t102)  # t103: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t104 = prims.add(1.0, t103)  # t104: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t105 = prims.reciprocal(t104)  # t105: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t106 = prims.mul(t100, t105)  # t106: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t107 = prims.mul(t106, t101)  # t107: \"cuda:0 f32[1, 512, 11008]\"\n",
       "  t108 = torch.nn.functional.linear(t107, t26, None)  # t108: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t108 = ltorch.linear(t107, t26, None)  # t108: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t108 = prims.linear(t107, t26, None)  # t108: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  [t109, t115, t119] = nvFusion5(t108, t118, t89)\n",
       "    # t109 = prims.add(t108, t89)  # t109: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t110 = prims.mul(t109, t109)  # t110: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t111 = prims.sum(t110, (2,))  # t111: \"cuda:0 f32[1, 512]\"\n",
       "    # t112 = prims.broadcast_in_dim(t111, [1, 512, 1], [0, 1])  # t112: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t113 = prims.div(t112, 4096.0)  # t113: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t114 = prims.add(t113, 1e-05)  # t114: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t115 = prims.rsqrt(t114)  # t115: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t116 = prims.broadcast_in_dim(t115, (1, 512, 4096), (0, 1, 2))  # t116: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t117 = prims.mul(t109, t116)  # t117: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t119 = prims.mul(t117, t118)  # t119: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t108\n",
       "  t120 = torch.nn.functional.linear(t119, t4, None)  # t120: \"cuda:0 f32[1, 512, 12288]\"\n",
       "    # t120 = ltorch.linear(t119, t4, None)  # t120: \"cuda:0 f32[1, 512, 12288]\"\n",
       "      # t120 = prims.linear(t119, t4, None)  # t120: \"cuda:0 f32[1, 512, 12288]\"\n",
       "  t121 = torch.reshape(t120, (1, 512, 32, 3, 128))  # t121: \"cuda:0 f32[1, 512, 32, 3, 128]\"\n",
       "    # t121 = ltorch.reshape(t120, (1, 512, 32, 3, 128))  # t121: \"cuda:0 f32[1, 512, 32, 3, 128]\"\n",
       "      # t121 = prims.reshape(t120, (1, 512, 32, 3, 128))  # t121: \"cuda:0 f32[1, 512, 32, 3, 128]\"\n",
       "  del t120\n",
       "  t122 = torch.permute(t121, (0, 2, 3, 1, 4))  # t122: \"cuda:0 f32[1, 32, 3, 512, 128]\"\n",
       "    # t122 = ltorch.permute(t121, (0, 2, 3, 1, 4))  # t122: \"cuda:0 f32[1, 32, 3, 512, 128]\"\n",
       "      # t122 = prims.transpose(t121, (0, 2, 3, 1, 4))  # t122: \"cuda:0 f32[1, 32, 3, 512, 128]\"\n",
       "  del t121\n",
       "  (t123, t124, t125) = torch.split(t122, (1, 1, 1), 2)\n",
       "    # (t123, t124, t125) = ltorch.split(t122, (1, 1, 1), 2)\n",
       "      # t123 = prims.slice_prim(t122, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t123: \"cuda:0 f32[1, 32, 1, 512, 128]\"\n",
       "      # t124 = prims.slice_prim(t122, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t124: \"cuda:0 f32[1, 32, 1, 512, 128]\"\n",
       "      # t125 = prims.slice_prim(t122, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t125: \"cuda:0 f32[1, 32, 1, 512, 128]\"\n",
       "  del t122\n",
       "  t126 = torch.reshape(t123, (1, 32, 512, 128))  # t126: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t126 = ltorch.reshape(t123, (1, 32, 512, 128))  # t126: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t126 = prims.reshape(t123, (1, 32, 512, 128))  # t126: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t123\n",
       "  t127 = torch.reshape(t124, (1, 32, 512, 128))  # t127: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t127 = ltorch.reshape(t124, (1, 32, 512, 128))  # t127: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t127 = prims.reshape(t124, (1, 32, 512, 128))  # t127: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t124\n",
       "  t128 = torch.reshape(t125, (1, 32, 512, 128))  # t128: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t128 = ltorch.reshape(t125, (1, 32, 512, 128))  # t128: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t128 = prims.reshape(t125, (1, 32, 512, 128))  # t128: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t125\n",
       "  t149 = torch_slice_prim_impl(t126, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t149: \"cuda:0 f32[1, 32, 512, 0]\"\n",
       "  t151 = torch_slice_prim_impl(t127, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t151: \"cuda:0 f32[1, 32, 512, 0]\"\n",
       "  t129 = torch_slice_prim_impl(t126, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t129: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t126\n",
       "  t139 = torch_slice_prim_impl(t127, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t139: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t127\n",
       "  t130 = torch_slice_prim_impl(t129, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t130: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  t131 = torch_slice_prim_impl(t129, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t131: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  t141 = torch_slice_prim_impl(t139, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t141: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  t140 = torch_slice_prim_impl(t139, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t140: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  [t132, t142] = nvFusion6(t131, t141)\n",
       "    # t132 = prims.neg(t131)  # t132: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "    # t142 = prims.neg(t141)  # t142: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  del t131, t141\n",
       "  t143 = torch.cat((t142, t140), -1)  # t143: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t143 = ltorch.cat((t142, t140), -1)  # t143: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t143 = prims.cat((t142, t140), -1)  # t143: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t142, t140\n",
       "  t133 = torch.cat((t132, t130), -1)  # t133: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t133 = ltorch.cat((t132, t130), -1)  # t133: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t133 = prims.cat((t132, t130), -1)  # t133: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t132, t130\n",
       "  [t138, t148] = nvFusion7(t129, t133, t139, t143, t63, t65)\n",
       "    # t145 = prims.mul(t139, t63)  # t145: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t147 = prims.mul(t143, t65)  # t147: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t148 = prims.add(t145, t147)  # t148: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t135 = prims.mul(t129, t63)  # t135: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t137 = prims.mul(t133, t65)  # t137: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t138 = prims.add(t135, t137)  # t138: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t129, t133, t139, t143\n",
       "  t150 = torch.cat((t138, t149), -1)  # t150: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t150 = ltorch.cat((t138, t149), -1)  # t150: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t150 = prims.cat((t138, t149), -1)  # t150: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t138, t149\n",
       "  t152 = torch.cat((t148, t151), -1)  # t152: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t152 = ltorch.cat((t148, t151), -1)  # t152: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t152 = prims.cat((t148, t151), -1)  # t152: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t148, t151\n",
       "  (t153, t154, t155, t156) = sdpaex_grad_forward_scaled_dot_product_efficient_attention(t150, t152, t128, None, 0.0, True, 0.08838834764831843)\n",
       "  t157 = torch.permute(t153, (0, 2, 1, 3))  # t157: \"cuda:0 f32[1, 512, 32, 128]\"\n",
       "    # t157 = ltorch.permute(t153, (0, 2, 1, 3))  # t157: \"cuda:0 f32[1, 512, 32, 128]\"\n",
       "      # t157 = prims.transpose(t153, (0, 2, 1, 3))  # t157: \"cuda:0 f32[1, 512, 32, 128]\"\n",
       "  t158 = torch.reshape(t157, (1, 512, 4096))  # t158: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t158 = ltorch.reshape(t157, (1, 512, 4096))  # t158: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t158 = prims.reshape(t157, (1, 512, 4096))  # t158: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t157\n",
       "  t159 = torch.nn.functional.linear(t158, t27, None)  # t159: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t159 = ltorch.linear(t158, t27, None)  # t159: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t159 = prims.linear(t158, t27, None)  # t159: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  [t160, t166, t170] = nvFusion8(t109, t159, t169)\n",
       "    # t160 = prims.add(t159, t109)  # t160: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t161 = prims.mul(t160, t160)  # t161: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t162 = prims.sum(t161, (2,))  # t162: \"cuda:0 f32[1, 512]\"\n",
       "    # t163 = prims.broadcast_in_dim(t162, [1, 512, 1], [0, 1])  # t163: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t164 = prims.div(t163, 4096.0)  # t164: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t165 = prims.add(t164, 1e-05)  # t165: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t166 = prims.rsqrt(t165)  # t166: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t167 = prims.broadcast_in_dim(t166, (1, 512, 4096), (0, 1, 2))  # t167: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t168 = prims.mul(t160, t167)  # t168: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t170 = prims.mul(t168, t169)  # t170: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t159\n",
       "  t172 = torch.nn.functional.linear(t170, t12, None)  # t172: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t172 = ltorch.linear(t170, t12, None)  # t172: \"cuda:0 f32[1, 512, 11008]\"\n",
       "      # t172 = prims.linear(t170, t12, None)  # t172: \"cuda:0 f32[1, 512, 11008]\"\n",
       "  t171 = torch.nn.functional.linear(t170, t8, None)  # t171: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t171 = ltorch.linear(t170, t8, None)  # t171: \"cuda:0 f32[1, 512, 11008]\"\n",
       "      # t171 = prims.linear(t170, t8, None)  # t171: \"cuda:0 f32[1, 512, 11008]\"\n",
       "  [t178] = nvFusion9(t171, t172)\n",
       "    # t173 = prims.neg(t171)  # t173: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t174 = prims.exp(t173)  # t174: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t175 = prims.add(1.0, t174)  # t175: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t176 = prims.reciprocal(t175)  # t176: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t177 = prims.mul(t171, t176)  # t177: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t178 = prims.mul(t177, t172)  # t178: \"cuda:0 f32[1, 512, 11008]\"\n",
       "  t179 = torch.nn.functional.linear(t178, t28, None)  # t179: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t179 = ltorch.linear(t178, t28, None)  # t179: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t179 = prims.linear(t178, t28, None)  # t179: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  [t180, t186, t190] = nvFusion10(t160, t179, t189)\n",
       "    # t180 = prims.add(t179, t160)  # t180: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t181 = prims.mul(t180, t180)  # t181: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t182 = prims.sum(t181, (2,))  # t182: \"cuda:0 f32[1, 512]\"\n",
       "    # t183 = prims.broadcast_in_dim(t182, [1, 512, 1], [0, 1])  # t183: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t184 = prims.div(t183, 4096.0)  # t184: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t185 = prims.add(t184, 1e-05)  # t185: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t186 = prims.rsqrt(t185)  # t186: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t187 = prims.broadcast_in_dim(t186, (1, 512, 4096), (0, 1, 2))  # t187: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t188 = prims.mul(t180, t187)  # t188: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t190 = prims.mul(t188, t189)  # t190: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t179\n",
       "  t191 = torch.nn.functional.linear(t190, t5, None)  # t191: \"cuda:0 f32[1, 512, 12288]\"\n",
       "    # t191 = ltorch.linear(t190, t5, None)  # t191: \"cuda:0 f32[1, 512, 12288]\"\n",
       "      # t191 = prims.linear(t190, t5, None)  # t191: \"cuda:0 f32[1, 512, 12288]\"\n",
       "  t192 = torch.reshape(t191, (1, 512, 32, 3, 128))  # t192: \"cuda:0 f32[1, 512, 32, 3, 128]\"\n",
       "    # t192 = ltorch.reshape(t191, (1, 512, 32, 3, 128))  # t192: \"cuda:0 f32[1, 512, 32, 3, 128]\"\n",
       "      # t192 = prims.reshape(t191, (1, 512, 32, 3, 128))  # t192: \"cuda:0 f32[1, 512, 32, 3, 128]\"\n",
       "  del t191\n",
       "  t193 = torch.permute(t192, (0, 2, 3, 1, 4))  # t193: \"cuda:0 f32[1, 32, 3, 512, 128]\"\n",
       "    # t193 = ltorch.permute(t192, (0, 2, 3, 1, 4))  # t193: \"cuda:0 f32[1, 32, 3, 512, 128]\"\n",
       "      # t193 = prims.transpose(t192, (0, 2, 3, 1, 4))  # t193: \"cuda:0 f32[1, 32, 3, 512, 128]\"\n",
       "  del t192\n",
       "  (t194, t195, t196) = torch.split(t193, (1, 1, 1), 2)\n",
       "    # (t194, t195, t196) = ltorch.split(t193, (1, 1, 1), 2)\n",
       "      # t194 = prims.slice_prim(t193, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t194: \"cuda:0 f32[1, 32, 1, 512, 128]\"\n",
       "      # t195 = prims.slice_prim(t193, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t195: \"cuda:0 f32[1, 32, 1, 512, 128]\"\n",
       "      # t196 = prims.slice_prim(t193, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t196: \"cuda:0 f32[1, 32, 1, 512, 128]\"\n",
       "  del t193\n",
       "  t197 = torch.reshape(t194, (1, 32, 512, 128))  # t197: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t197 = ltorch.reshape(t194, (1, 32, 512, 128))  # t197: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t197 = prims.reshape(t194, (1, 32, 512, 128))  # t197: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t194\n",
       "  t198 = torch.reshape(t195, (1, 32, 512, 128))  # t198: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t198 = ltorch.reshape(t195, (1, 32, 512, 128))  # t198: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t198 = prims.reshape(t195, (1, 32, 512, 128))  # t198: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t195\n",
       "  t199 = torch.reshape(t196, (1, 32, 512, 128))  # t199: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t199 = ltorch.reshape(t196, (1, 32, 512, 128))  # t199: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t199 = prims.reshape(t196, (1, 32, 512, 128))  # t199: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t196\n",
       "  t200 = torch_slice_prim_impl(t197, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t200: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  t210 = torch_slice_prim_impl(t198, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t210: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  t220 = torch_slice_prim_impl(t197, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t220: \"cuda:0 f32[1, 32, 512, 0]\"\n",
       "  del t197\n",
       "  t222 = torch_slice_prim_impl(t198, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t222: \"cuda:0 f32[1, 32, 512, 0]\"\n",
       "  del t198\n",
       "  t201 = torch_slice_prim_impl(t200, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t201: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  t202 = torch_slice_prim_impl(t200, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t202: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  t211 = torch_slice_prim_impl(t210, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t211: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  t212 = torch_slice_prim_impl(t210, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t212: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  [t203, t213] = nvFusion11(t202, t212)\n",
       "    # t203 = prims.neg(t202)  # t203: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "    # t213 = prims.neg(t212)  # t213: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  del t202, t212\n",
       "  t214 = torch.cat((t213, t211), -1)  # t214: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t214 = ltorch.cat((t213, t211), -1)  # t214: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t214 = prims.cat((t213, t211), -1)  # t214: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t213, t211\n",
       "  t204 = torch.cat((t203, t201), -1)  # t204: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t204 = ltorch.cat((t203, t201), -1)  # t204: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t204 = prims.cat((t203, t201), -1)  # t204: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t203, t201\n",
       "  [t209, t219] = nvFusion12(t200, t204, t210, t214, t63, t65)\n",
       "    # t216 = prims.mul(t210, t63)  # t216: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t218 = prims.mul(t214, t65)  # t218: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t219 = prims.add(t216, t218)  # t219: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t206 = prims.mul(t200, t63)  # t206: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t208 = prims.mul(t204, t65)  # t208: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t209 = prims.add(t206, t208)  # t209: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t200, t204, t210, t214\n",
       "  t223 = torch.cat((t219, t222), -1)  # t223: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t223 = ltorch.cat((t219, t222), -1)  # t223: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t223 = prims.cat((t219, t222), -1)  # t223: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t219, t222\n",
       "  t221 = torch.cat((t209, t220), -1)  # t221: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t221 = ltorch.cat((t209, t220), -1)  # t221: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t221 = prims.cat((t209, t220), -1)  # t221: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t209, t220\n",
       "  (t224, t225, t226, t227) = sdpaex_grad_forward_scaled_dot_product_efficient_attention(t221, t223, t199, None, 0.0, True, 0.08838834764831843)\n",
       "  t228 = torch.permute(t224, (0, 2, 1, 3))  # t228: \"cuda:0 f32[1, 512, 32, 128]\"\n",
       "    # t228 = ltorch.permute(t224, (0, 2, 1, 3))  # t228: \"cuda:0 f32[1, 512, 32, 128]\"\n",
       "      # t228 = prims.transpose(t224, (0, 2, 1, 3))  # t228: \"cuda:0 f32[1, 512, 32, 128]\"\n",
       "  t229 = torch.reshape(t228, (1, 512, 4096))  # t229: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t229 = ltorch.reshape(t228, (1, 512, 4096))  # t229: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t229 = prims.reshape(t228, (1, 512, 4096))  # t229: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t228\n",
       "  t230 = torch.nn.functional.linear(t229, t29, None)  # t230: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t230 = ltorch.linear(t229, t29, None)  # t230: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t230 = prims.linear(t229, t29, None)  # t230: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  [t231, t237, t241] = nvFusion13(t180, t230, t240)\n",
       "    # t231 = prims.add(t230, t180)  # t231: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t232 = prims.mul(t231, t231)  # t232: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t233 = prims.sum(t232, (2,))  # t233: \"cuda:0 f32[1, 512]\"\n",
       "    # t234 = prims.broadcast_in_dim(t233, [1, 512, 1], [0, 1])  # t234: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t235 = prims.div(t234, 4096.0)  # t235: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t236 = prims.add(t235, 1e-05)  # t236: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t237 = prims.rsqrt(t236)  # t237: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t238 = prims.broadcast_in_dim(t237, (1, 512, 4096), (0, 1, 2))  # t238: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t239 = prims.mul(t231, t238)  # t239: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t241 = prims.mul(t239, t240)  # t241: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t230\n",
       "  t242 = torch.nn.functional.linear(t241, t9, None)  # t242: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t242 = ltorch.linear(t241, t9, None)  # t242: \"cuda:0 f32[1, 512, 11008]\"\n",
       "      # t242 = prims.linear(t241, t9, None)  # t242: \"cuda:0 f32[1, 512, 11008]\"\n",
       "  t243 = torch.nn.functional.linear(t241, t13, None)  # t243: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t243 = ltorch.linear(t241, t13, None)  # t243: \"cuda:0 f32[1, 512, 11008]\"\n",
       "      # t243 = prims.linear(t241, t13, None)  # t243: \"cuda:0 f32[1, 512, 11008]\"\n",
       "  [t249] = nvFusion14(t242, t243)\n",
       "    # t244 = prims.neg(t242)  # t244: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t245 = prims.exp(t244)  # t245: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t246 = prims.add(1.0, t245)  # t246: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t247 = prims.reciprocal(t246)  # t247: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t248 = prims.mul(t242, t247)  # t248: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t249 = prims.mul(t248, t243)  # t249: \"cuda:0 f32[1, 512, 11008]\"\n",
       "  t250 = torch.nn.functional.linear(t249, t30, None)  # t250: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t250 = ltorch.linear(t249, t30, None)  # t250: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t250 = prims.linear(t249, t30, None)  # t250: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  [t251, t257, t261] = nvFusion15(t231, t250, t260)\n",
       "    # t251 = prims.add(t250, t231)  # t251: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t252 = prims.mul(t251, t251)  # t252: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t253 = prims.sum(t252, (2,))  # t253: \"cuda:0 f32[1, 512]\"\n",
       "    # t254 = prims.broadcast_in_dim(t253, [1, 512, 1], [0, 1])  # t254: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t255 = prims.div(t254, 4096.0)  # t255: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t256 = prims.add(t255, 1e-05)  # t256: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t257 = prims.rsqrt(t256)  # t257: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t258 = prims.broadcast_in_dim(t257, (1, 512, 4096), (0, 1, 2))  # t258: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t259 = prims.mul(t251, t258)  # t259: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t261 = prims.mul(t259, t260)  # t261: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t250\n",
       "  t262 = torch.nn.functional.linear(t261, t6, None)  # t262: \"cuda:0 f32[1, 512, 12288]\"\n",
       "    # t262 = ltorch.linear(t261, t6, None)  # t262: \"cuda:0 f32[1, 512, 12288]\"\n",
       "      # t262 = prims.linear(t261, t6, None)  # t262: \"cuda:0 f32[1, 512, 12288]\"\n",
       "  t263 = torch.reshape(t262, (1, 512, 32, 3, 128))  # t263: \"cuda:0 f32[1, 512, 32, 3, 128]\"\n",
       "    # t263 = ltorch.reshape(t262, (1, 512, 32, 3, 128))  # t263: \"cuda:0 f32[1, 512, 32, 3, 128]\"\n",
       "      # t263 = prims.reshape(t262, (1, 512, 32, 3, 128))  # t263: \"cuda:0 f32[1, 512, 32, 3, 128]\"\n",
       "  del t262\n",
       "  t264 = torch.permute(t263, (0, 2, 3, 1, 4))  # t264: \"cuda:0 f32[1, 32, 3, 512, 128]\"\n",
       "    # t264 = ltorch.permute(t263, (0, 2, 3, 1, 4))  # t264: \"cuda:0 f32[1, 32, 3, 512, 128]\"\n",
       "      # t264 = prims.transpose(t263, (0, 2, 3, 1, 4))  # t264: \"cuda:0 f32[1, 32, 3, 512, 128]\"\n",
       "  del t263\n",
       "  (t265, t266, t267) = torch.split(t264, (1, 1, 1), 2)\n",
       "    # (t265, t266, t267) = ltorch.split(t264, (1, 1, 1), 2)\n",
       "      # t265 = prims.slice_prim(t264, [0, 0, 0, 0, 0], [1, 32, 1, 512, 128], [1, 1, 1, 1, 1])  # t265: \"cuda:0 f32[1, 32, 1, 512, 128]\"\n",
       "      # t266 = prims.slice_prim(t264, [0, 0, 1, 0, 0], [1, 32, 2, 512, 128], [1, 1, 1, 1, 1])  # t266: \"cuda:0 f32[1, 32, 1, 512, 128]\"\n",
       "      # t267 = prims.slice_prim(t264, [0, 0, 2, 0, 0], [1, 32, 3, 512, 128], [1, 1, 1, 1, 1])  # t267: \"cuda:0 f32[1, 32, 1, 512, 128]\"\n",
       "  del t264\n",
       "  t268 = torch.reshape(t265, (1, 32, 512, 128))  # t268: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t268 = ltorch.reshape(t265, (1, 32, 512, 128))  # t268: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t268 = prims.reshape(t265, (1, 32, 512, 128))  # t268: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t265\n",
       "  t269 = torch.reshape(t266, (1, 32, 512, 128))  # t269: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t269 = ltorch.reshape(t266, (1, 32, 512, 128))  # t269: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t269 = prims.reshape(t266, (1, 32, 512, 128))  # t269: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t266\n",
       "  t270 = torch.reshape(t267, (1, 32, 512, 128))  # t270: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t270 = ltorch.reshape(t267, (1, 32, 512, 128))  # t270: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t270 = prims.reshape(t267, (1, 32, 512, 128))  # t270: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t267\n",
       "  t271 = torch_slice_prim_impl(t268, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t271: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  t281 = torch_slice_prim_impl(t269, [0, 0, 0, 0], [1, 32, 512, 128], [1, 1, 1, 1])  # t281: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  t291 = torch_slice_prim_impl(t268, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t291: \"cuda:0 f32[1, 32, 512, 0]\"\n",
       "  del t268\n",
       "  t293 = torch_slice_prim_impl(t269, [0, 0, 0, 0], [1, 32, 512, 0], [1, 1, 1, 1])  # t293: \"cuda:0 f32[1, 32, 512, 0]\"\n",
       "  del t269\n",
       "  t272 = torch_slice_prim_impl(t271, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t272: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  t273 = torch_slice_prim_impl(t271, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t273: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  t282 = torch_slice_prim_impl(t281, [0, 0, 0, 0], [1, 32, 512, 64], [1, 1, 1, 1])  # t282: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  t283 = torch_slice_prim_impl(t281, [0, 0, 0, 64], [1, 32, 512, 128], [1, 1, 1, 1])  # t283: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  [t274, t284] = nvFusion16(t273, t283)\n",
       "    # t274 = prims.neg(t273)  # t274: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "    # t284 = prims.neg(t283)  # t284: \"cuda:0 f32[1, 32, 512, 64]\"\n",
       "  del t273, t283\n",
       "  t275 = torch.cat((t274, t272), -1)  # t275: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t275 = ltorch.cat((t274, t272), -1)  # t275: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t275 = prims.cat((t274, t272), -1)  # t275: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t274, t272\n",
       "  t285 = torch.cat((t284, t282), -1)  # t285: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t285 = ltorch.cat((t284, t282), -1)  # t285: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t285 = prims.cat((t284, t282), -1)  # t285: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t284, t282\n",
       "  [t280, t290] = nvFusion17(t271, t275, t281, t285, t63, t65)\n",
       "    # t277 = prims.mul(t271, t63)  # t277: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t279 = prims.mul(t275, t65)  # t279: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t280 = prims.add(t277, t279)  # t280: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t287 = prims.mul(t281, t63)  # t287: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t289 = prims.mul(t285, t65)  # t289: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t290 = prims.add(t287, t289)  # t290: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t271, t275, t281, t285\n",
       "  t292 = torch.cat((t280, t291), -1)  # t292: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t292 = ltorch.cat((t280, t291), -1)  # t292: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t292 = prims.cat((t280, t291), -1)  # t292: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t280, t291\n",
       "  t294 = torch.cat((t290, t293), -1)  # t294: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "    # t294 = ltorch.cat((t290, t293), -1)  # t294: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "      # t294 = prims.cat((t290, t293), -1)  # t294: \"cuda:0 f32[1, 32, 512, 128]\"\n",
       "  del t290, t293\n",
       "  (t295, t296, t297, t298) = sdpaex_grad_forward_scaled_dot_product_efficient_attention(t292, t294, t270, None, 0.0, True, 0.08838834764831843)\n",
       "  t299 = torch.permute(t295, (0, 2, 1, 3))  # t299: \"cuda:0 f32[1, 512, 32, 128]\"\n",
       "    # t299 = ltorch.permute(t295, (0, 2, 1, 3))  # t299: \"cuda:0 f32[1, 512, 32, 128]\"\n",
       "      # t299 = prims.transpose(t295, (0, 2, 1, 3))  # t299: \"cuda:0 f32[1, 512, 32, 128]\"\n",
       "  t300 = torch.reshape(t299, (1, 512, 4096))  # t300: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t300 = ltorch.reshape(t299, (1, 512, 4096))  # t300: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t300 = prims.reshape(t299, (1, 512, 4096))  # t300: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t299\n",
       "  t301 = torch.nn.functional.linear(t300, t31, None)  # t301: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t301 = ltorch.linear(t300, t31, None)  # t301: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t301 = prims.linear(t300, t31, None)  # t301: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  [t302, t308, t312] = nvFusion18(t251, t301, t311)\n",
       "    # t302 = prims.add(t301, t251)  # t302: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t303 = prims.mul(t302, t302)  # t303: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t304 = prims.sum(t303, (2,))  # t304: \"cuda:0 f32[1, 512]\"\n",
       "    # t305 = prims.broadcast_in_dim(t304, [1, 512, 1], [0, 1])  # t305: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t306 = prims.div(t305, 4096.0)  # t306: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t307 = prims.add(t306, 1e-05)  # t307: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t308 = prims.rsqrt(t307)  # t308: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t309 = prims.broadcast_in_dim(t308, (1, 512, 4096), (0, 1, 2))  # t309: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t310 = prims.mul(t302, t309)  # t310: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t312 = prims.mul(t310, t311)  # t312: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t301\n",
       "  t314 = torch.nn.functional.linear(t312, t14, None)  # t314: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t314 = ltorch.linear(t312, t14, None)  # t314: \"cuda:0 f32[1, 512, 11008]\"\n",
       "      # t314 = prims.linear(t312, t14, None)  # t314: \"cuda:0 f32[1, 512, 11008]\"\n",
       "  t313 = torch.nn.functional.linear(t312, t10, None)  # t313: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t313 = ltorch.linear(t312, t10, None)  # t313: \"cuda:0 f32[1, 512, 11008]\"\n",
       "      # t313 = prims.linear(t312, t10, None)  # t313: \"cuda:0 f32[1, 512, 11008]\"\n",
       "  [t320] = nvFusion19(t313, t314)\n",
       "    # t315 = prims.neg(t313)  # t315: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t316 = prims.exp(t315)  # t316: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t317 = prims.add(1.0, t316)  # t317: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t318 = prims.reciprocal(t317)  # t318: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t319 = prims.mul(t313, t318)  # t319: \"cuda:0 f32[1, 512, 11008]\"\n",
       "    # t320 = prims.mul(t319, t314)  # t320: \"cuda:0 f32[1, 512, 11008]\"\n",
       "  t321 = torch.nn.functional.linear(t320, t32, None)  # t321: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t321 = ltorch.linear(t320, t32, None)  # t321: \"cuda:0 f32[1, 512, 4096]\"\n",
       "      # t321 = prims.linear(t320, t32, None)  # t321: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  [t322, t328, t332] = nvFusion20(t302, t321, t331)\n",
       "    # t322 = prims.add(t321, t302)  # t322: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t323 = prims.mul(t322, t322)  # t323: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t324 = prims.sum(t323, (2,))  # t324: \"cuda:0 f32[1, 512]\"\n",
       "    # t325 = prims.broadcast_in_dim(t324, [1, 512, 1], [0, 1])  # t325: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t326 = prims.div(t325, 4096.0)  # t326: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t327 = prims.add(t326, 1e-05)  # t327: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t328 = prims.rsqrt(t327)  # t328: \"cuda:0 f32[1, 512, 1]\"\n",
       "    # t329 = prims.broadcast_in_dim(t328, (1, 512, 4096), (0, 1, 2))  # t329: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t330 = prims.mul(t322, t329)  # t330: \"cuda:0 f32[1, 512, 4096]\"\n",
       "    # t332 = prims.mul(t330, t331)  # t332: \"cuda:0 f32[1, 512, 4096]\"\n",
       "  del t321\n",
       "  t333 = torch.nn.functional.linear(t332, t15, None)  # t333: \"cuda:0 f32[1, 512, 32000]\"\n",
       "    # t333 = ltorch.linear(t332, t15, None)  # t333: \"cuda:0 f32[1, 512, 32000]\"\n",
       "      # t333 = prims.linear(t332, t15, None)  # t333: \"cuda:0 f32[1, 512, 32000]\"\n",
       "  return {'output': t333, 'flat_args': [t0, t1, t2, t3, t4, t5, t6, t7, t8, t9, t10, t11, t12, t13, t14, t15, t16, t17, t18, t19, t20, t21, t22, t23, t24, t25, t26, t27, t28, t29, t30, t31, t32, t33], 'flat_output': (t333,)}, ((t0, t10, t100, t101, t107, t109, t11, t115, t118, t119, t12, t128, t13, t14, t15, t150, t152, t153, t154, t155, t156, t158, t160, t166, t169, t170, t171, t172, t178, t180, t186, t189, t190, t199, t221, t223, t224, t225, t226, t227, t229, t231, t237, t240, t241, t242, t243, t249, t25, t251, t257, t26, t260, t261, t27, t270, t28, t29, t292, t294, t295, t296, t297, t298, t3, t30, t300, t302, t308, t31, t311, t312, t313, t314, t32, t320, t322, t328, t331, t332, t38, t4, t44, t47, t48, t5, t57, t6, t63, t65, t7, t79, t8, t81, t82, t83, t84, t85, t87, t89, t9, t95, t98, t99), (False, True, True, False, True, True, 4096.0, 4096.0, 0.0, 0.08838834764831843, 4096.0, 4096.0, 0.0, 0.08838834764831843, 4096.0, 4096.0, 0.0, 0.08838834764831843, 4096.0, 4096.0, 4096.0, 0.0, 0.08838834764831843, 32000, 2, 2, 2, 2))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thunder.last_traces(thunder_model)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4944f352",
   "metadata": {},
   "source": [
    "Well, that is quite a bit to look through.\n",
    "But here is a key thing: The function now returns a buch of things. This is because Thunder applies the same treatment to the backward and to this end saves information from the forward. You can see a hint of this because the output has a `ThunderFunctionBackward` on as its `grad_fn`. (You can see the backward trace with \n",
    "`thunder.last_backward_traces(thunder_model)[-1]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d90df65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.9922,  0.5946, -0.2173,  ..., -0.0981, -0.5058,  0.2747],\n",
       "         [-1.1552,  0.5770, -0.7432,  ...,  0.0688,  0.1238,  0.6786],\n",
       "         [-0.7813,  0.6960,  0.1235,  ..., -0.4840,  0.1373,  0.6490],\n",
       "         ...,\n",
       "         [ 0.3711,  0.1656,  0.3350,  ..., -0.0294,  0.3670,  0.5099],\n",
       "         [-0.2544, -0.8470,  0.2063,  ..., -0.1341,  0.1877,  0.2612],\n",
       "         [ 0.3420, -1.1421,  0.9222,  ...,  0.5636,  0.1666,  0.6947]]],\n",
       "       device='cuda:0', grad_fn=<ThunderFunctionBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcec40f",
   "metadata": {},
   "source": [
    "One thing to keep in mind here is that for bf16, the numerical accuracy impact of rearranging operations can be quite pronounced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ba7f715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "maximum deviation grads: 0.00042724609375\n"
     ]
    }
   ],
   "source": [
    "actual_grads = torch.autograd.grad(actual.sum(), m.parameters())\n",
    "expected_grads = torch.autograd.grad(expected.sum(), m.parameters())\n",
    "print(\"maximum deviation grads:\", max((a-e).abs().max().item() for a, e in zip(actual_grads, expected_grads)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0261eb11",
   "metadata": {},
   "source": [
    "But is it faster? Yes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "854f29a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154 ms ± 281 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "150 ms ± 342 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "%timeit r = m(inp); torch.autograd.grad(r.sum(), m.parameters()); torch.cuda.synchronize()\n",
    "%timeit r = thunder_model(inp); torch.autograd.grad(r.sum(), m.parameters()); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb177aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "del m, thunder_model\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d31e7f8",
   "metadata": {},
   "source": [
    "So far, so good! Thunder should work with LitGPT today and we busy are adding the support required to run other models as well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23ebbf5",
   "metadata": {},
   "source": [
    "## Distributed with Thunder\n",
    "\n",
    "Those Large Language Models are called Large for a reason, and memory in a single GPU is invariably small. So we need multiple.\n",
    "\n",
    "Happily Thunder sports an FSDP interface to use multiple cards in our box.\n",
    "\n",
    "You still need to setup the process group, but as far as the model is concerned,\n",
    "\n",
    "```python\n",
    "model = thunder.jit(thunder.distributed.fsdp(model))\n",
    "```\n",
    "\n",
    "is all you need. Because it is tricky to run multiprocessing from Notebooks, we write a small example into a file and run it though `torch-run`.\n",
    "\n",
    "Check out our LitGPT Thunder examples for complete distributed training and finetuning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18dd3379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting zero_to_thunder_fsdp_simple_example.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile zero_to_thunder_fsdp_simple_example.py\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "from thunder.tests.lit_gpt_model import GPT, Config\n",
    "\n",
    "import torch\n",
    "import torch.distributed\n",
    "import thunder\n",
    "import thunder.distributed\n",
    "import os\n",
    "\n",
    "# Create Model\n",
    "# NOTE: We create the model on CPU.\n",
    "device='cpu'\n",
    "torch.set_default_dtype(torch.bfloat16)\n",
    "model = GPT.from_name('llama2-like')\n",
    "# Setup for distributed\n",
    "torch.distributed.init_process_group(backend='nccl')\n",
    "rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "\n",
    "device = f\"cuda:{rank}\"\n",
    "x = torch.randint(1, model.config.vocab_size, (1, 1024), device=device)\n",
    "\n",
    "# thunder.distributed.fsdp takes care of moving the parameter\n",
    "# shard to the correct GPU for the current process.\n",
    "model = thunder.jit(thunder.distributed.fsdp(model)) #  <---------------------------------------\n",
    "\n",
    "# Run the forward pass.\n",
    "res = model(x)\n",
    "res.sum().backward()\n",
    "\n",
    "res = model(x)\n",
    "res.sum().backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bad9b64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W0316 11:53:02.156000 140513675427904 torch/distributed/run.py:757] \r\n",
      "W0316 11:53:02.156000 140513675427904 torch/distributed/run.py:757] *****************************************\r\n",
      "W0316 11:53:02.156000 140513675427904 torch/distributed/run.py:757] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \r\n",
      "W0316 11:53:02.156000 140513675427904 torch/distributed/run.py:757] *****************************************\r\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc_per_node=2 zero_to_thunder_fsdp_simple_example.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c65e75d",
   "metadata": {},
   "source": [
    "So there. FSDP with just wrapping the model in `fsdp`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6d7a20",
   "metadata": {},
   "source": [
    "## Extending Thunder\n",
    "\n",
    "But we promised that thunder is extensible. Let's find out what's up with that.\n",
    "\n",
    "Specifically, we will incorporate the RMSNorm kernel from the great [Unsloth project](https://github.com/unslothai/unsloth/) into our model (note that NVFuser also creates a fused kernel for this).\n",
    "\n",
    "In Thunder, extensions (as well as most builtin optimizations which use the exact same mechanism) work with _executors_ handling operations. Let us define one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7639065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "my_ex"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_ex = thunder.extend.OperatorExecutor('my_ex', version='0.0.1')\n",
    "thunder.extend.register_executor(my_ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63595ab",
   "metadata": {},
   "source": [
    "For our base implementation, we take the ccode from [LitGPT's RMSNorm implementation](https://github.com/Lightning-AI/litgpt/blob/7c1574925f973e64c0a53e056b77229bedee1619/lit_gpt/rmsnorm.py)\n",
    "\n",
    "In thunder, we define a *meta* function that only defines the metadata (like shapes) of outputs and the actual implementation for each operator and then register the pair with our executor using the `register_operator` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "247074b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thunder import TensorProxy\n",
    "\n",
    "# Taken from LitGPT, who in turn credit:\n",
    "# Derived from https://github.com/bzhangGo/rmsnorm/blob/master/rmsnorm_torch.py. BSD 3-Clause License:\n",
    "#    https://github.com/bzhangGo/rmsnorm/blob/master/LICENSE.\n",
    "\n",
    "def rms_norm_impl(x: torch.Tensor, weight, dim: int, eps: float, add_unit_offset: bool) -> torch.Tensor:\n",
    "    dtype = x.dtype\n",
    "    x = x.float()\n",
    "    # NOTE: the original RMSNorm paper implementation is not equivalent\n",
    "    norm_x = torch.mean(x * x, dim=dim, keepdim=True)\n",
    "    x_normed = x * torch.rsqrt(norm_x + eps)\n",
    "    x_normed = x_normed.to(dtype=dtype)\n",
    "    if add_unit_offset:\n",
    "        # Gemma model requires a unit offset\n",
    "        # https://github.com/google/gemma_pytorch/blob/main/gemma/model.py#L176\n",
    "        return x_normed * (1 + weight)\n",
    "    return x_normed * weight\n",
    "\n",
    "def rms_norm_meta(x: TensorProxy, weight, dim: int, eps: float, add_unit_offset: bool) -> TensorProxy:\n",
    "    return TensorProxy(like=x)\n",
    "\n",
    "rms_norm = my_ex.register_operator('rms_norm', meta=rms_norm_meta, fn=rms_norm_impl)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ad1dbf",
   "metadata": {},
   "source": [
    "Because evil monkey-patching is a thing for short demos is a thing, let's replace LitGPT's own implementation. For your own model, you might start out with a that in your code directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0bdecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lit_gpt.rmsnorm\n",
    "if not hasattr(lit_gpt.rmsnorm, 'ThunderOrigRMSNorm'):\n",
    "    lit_gpt.rmsnorm.ThunderOrigRMSNorm = lit_gpt.rmsnorm.RMSNorm\n",
    "\n",
    "class ThunderizedRMSNorm(lit_gpt.rmsnorm.ThunderOrigRMSNorm):\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # This isn't the best paradigm. :/\n",
    "        if thunder.core.interpreter.is_jitting():\n",
    "            return rms_norm(x, self.weight, self.dim, self.eps, self.add_unit_offset)\n",
    "        else:\n",
    "            return super().forward(x)\n",
    "\n",
    "lit_gpt.rmsnorm.RMSNorm = ThunderizedRMSNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b7d056",
   "metadata": {},
   "source": [
    "We can try our new RMSNorm: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ebd5dd1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deviation: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 0 milliseconds)\n",
       "import torch\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast()\n",
       "def computation(x, t_weight):\n",
       "  # x: \"cuda:0 f32[256, 4096]\" \n",
       "  # t_weight: \"cuda:0 f32[4096]\" \n",
       "  t7 = rms_norm(x, t_weight, -1, 1e-06, False)  # t7: \"cuda:0 f32[256, 4096]\"\n",
       "  del x, t_weight\n",
       "  return t7"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.device('cuda'):\n",
    "    norm_module = ThunderizedRMSNorm(4096)\n",
    "    x = torch.randn(256, 4096)\n",
    "\n",
    "# we're not quite there to handle forward and backward yet, we'll re-enable them below\n",
    "for p in norm_module.parameters():  \n",
    "    p.requires_grad_(False)\n",
    "\n",
    "thunder_norm_module = thunder.jit(norm_module, executors=(my_ex,) + thunder.get_default_executors())    \n",
    "\n",
    "expected = norm_module(x)\n",
    "actual = thunder_norm_module(x)\n",
    "\n",
    "print(\"deviation:\", (expected - actual).abs().max().item())\n",
    "\n",
    "thunder.last_traces(thunder_norm_module)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c620a38",
   "metadata": {},
   "source": [
    "But why did we do this? Well, we can now layer a faster implementation on top.\n",
    "For this we take the [unsloth RMSNorm](https://github.com/unslothai/unsloth/blob/42076f6580e71522ed1c122043edfba595be64e4/unsloth/kernels/rms_layernorm.py) kernels. We the bits that were in the forward and backward of the `autograd.Function` into our implementation functions and define the corresponding metas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7a26f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023-present Daniel Han-Chen & the Unsloth team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "\n",
    "MAX_FUSED_SIZE = 65536\n",
    "next_power_of_2 = triton.next_power_of_2\n",
    "\n",
    "def calculate_settings(n):\n",
    "    BLOCK_SIZE = next_power_of_2(n)\n",
    "    if BLOCK_SIZE > MAX_FUSED_SIZE:\n",
    "        raise RuntimeError(f\"Cannot launch Triton kernel since n = {n} exceeds \"\\\n",
    "                           f\"the maximum CUDA blocksize = {MAX_FUSED_SIZE}.\")\n",
    "    num_warps = 4\n",
    "    if   BLOCK_SIZE >= 32768: num_warps = 32\n",
    "    elif BLOCK_SIZE >=  8192: num_warps = 16\n",
    "    elif BLOCK_SIZE >=  2048: num_warps = 8\n",
    "    return BLOCK_SIZE, num_warps\n",
    "\n",
    "@triton.jit\n",
    "def _rms_layernorm_forward(\n",
    "    Y, Y_row_stride,\n",
    "    X, X_row_stride,\n",
    "    W, W_row_stride,\n",
    "    r, r_row_stride,\n",
    "    n_cols, eps,\n",
    "    BLOCK_SIZE : tl.constexpr\n",
    "):\n",
    "    \"\"\"\n",
    "        Fast RMS Layernorm kernel\n",
    "        Inspiration from a Triton tutorial:\n",
    "        https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n",
    "    \"\"\"\n",
    "    row_idx = tl.program_id(0)\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    mask = col_offsets < n_cols\n",
    "\n",
    "    Y += row_idx * Y_row_stride\n",
    "    X += row_idx * X_row_stride\n",
    "    r += row_idx * r_row_stride\n",
    "\n",
    "    X_row = tl.load(X + col_offsets, mask = mask, other = 0).to(tl.float32)\n",
    "    W_row = tl.load(W + col_offsets, mask = mask, other = 0)#.to(tl.float32)\n",
    "\n",
    "    row_var = tl.sum(X_row * X_row, axis = 0) / n_cols\n",
    "    inv_var = tl.math.rsqrt(row_var + eps)\n",
    "    tl.store(r, inv_var)\n",
    "    normed = X_row * inv_var\n",
    "    normed = normed.to(W_row.dtype) # Exact copy from HF\n",
    "    output = normed * W_row\n",
    "    tl.store(Y + col_offsets, output, mask = mask)\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def _rms_layernorm_backward(\n",
    "    dY, dY_row_stride,\n",
    "    X,   X_row_stride,\n",
    "    W,   W_row_stride,\n",
    "    r,   r_row_stride,\n",
    "    dW, dW_row_stride,\n",
    "    n_cols, eps,\n",
    "    BLOCK_SIZE : tl.constexpr,\n",
    "):\n",
    "    \"\"\"\n",
    "        Fast RMS Layernorm kernel for the backward pass\n",
    "        Inspiration from a Triton tutorial:\n",
    "        https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html\n",
    "    \"\"\"\n",
    "    row_idx = tl.program_id(0)\n",
    "    col_offsets = tl.arange(0, BLOCK_SIZE)\n",
    "    mask = col_offsets < n_cols\n",
    "\n",
    "    dY += row_idx * dY_row_stride\n",
    "    X  += row_idx *  X_row_stride\n",
    "    r  += row_idx *  r_row_stride\n",
    "\n",
    "    dY_row = tl.load(dY + col_offsets, mask = mask, other = 0).to(tl.float32)\n",
    "    X_row  = tl.load(X  + col_offsets, mask = mask, other = 0).to(tl.float32)\n",
    "    W_row  = tl.load(W  + col_offsets, mask = mask, other = 0).to(tl.float32)\n",
    "\n",
    "    # Get saved row variance\n",
    "    inv_var = tl.load(r).to(tl.float32)\n",
    "    normed = X_row * inv_var\n",
    "\n",
    "    dY_W = dY_row * W_row\n",
    "\n",
    "    rowsum_dY_normed = tl.sum(dY_W * normed, axis = 0)\n",
    "    output = inv_var/n_cols * (n_cols*dY_W - normed*rowsum_dY_normed)\n",
    "    tl.store(dY + col_offsets, output, mask = mask)\n",
    "    \n",
    "def rms_layernorm_forward_impl(X, W, eps):\n",
    "    shape = X.shape\n",
    "    dim = shape[-1]\n",
    "    X = X.view(-1, dim)\n",
    "    n_rows, n_cols = X.shape\n",
    "    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n",
    "\n",
    "    Y = torch.empty((n_rows, n_cols), dtype = X.dtype, device = \"cuda\")\n",
    "    r = torch.empty(n_rows, dtype = torch.float32, device = \"cuda\")\n",
    "\n",
    "    _rms_layernorm_forward[(n_rows,)](\n",
    "        Y, Y.stride(0),\n",
    "        X, X.stride(0),\n",
    "        W, W.stride(0),\n",
    "        r, r.stride(0),\n",
    "        n_cols, eps,\n",
    "        BLOCK_SIZE = BLOCK_SIZE,\n",
    "        num_warps  = num_warps,\n",
    "    )\n",
    "    return Y.view(*shape), (r, BLOCK_SIZE, num_warps)\n",
    "\n",
    "def rms_layernorm_forward_meta(X, W, eps):\n",
    "    n_cols = X.shape[-1]\n",
    "    n_rows = 1\n",
    "    for i in X.shape[:-1]:\n",
    "        n_rows *= i\n",
    "    BLOCK_SIZE, num_warps = calculate_settings(n_cols)\n",
    "    Y = TensorProxy(like=X, requires_grad=True)\n",
    "    return (Y,\n",
    "            (TensorProxy(shape=(n_rows,), device=X.device, dtype=thunder.dtypes.float32, requires_grad=False),\n",
    "             BLOCK_SIZE, \n",
    "             num_warps,\n",
    "            )\n",
    "           )\n",
    "\n",
    "def rms_layernorm_backward_impl(X, W, r, eps, BLOCK_SIZE, num_warps, dY):\n",
    "    shape = dY.shape\n",
    "    dim = shape[-1]\n",
    "    dY = dY.view(-1, dim)\n",
    "    n_rows, n_cols = dY.shape\n",
    "    dW = X\n",
    "    dX = dY.clone()\n",
    "    _rms_layernorm_backward[(n_rows,)](\n",
    "        dX, dX.stride(0),\n",
    "        X,  X .stride(0),\n",
    "        W,  W .stride(0),\n",
    "        r,  r .stride(0),\n",
    "        dW, dW.stride(0),\n",
    "        n_cols, eps,\n",
    "        BLOCK_SIZE = BLOCK_SIZE,\n",
    "        num_warps  = num_warps,\n",
    "    )\n",
    "    dX = dX.view(*shape)\n",
    "    return dX\n",
    "\n",
    "def rms_layernorm_backward_meta(X, W, r, eps, BLOCK_SIZE, num_warps, dY):\n",
    "    return TensorProxy(like=dY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70eba5f",
   "metadata": {},
   "source": [
    "With this, we can just register the additional operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8f1e77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unsloth_rms_norm_forward = my_ex.register_operator('unsloth_rms_norm_forward', meta=rms_layernorm_forward_meta, fn=rms_layernorm_forward_impl)\n",
    "unsloth_rms_norm_backward = my_ex.register_operator('unsloth_rms_norm_backward', meta=rms_layernorm_backward_meta, fn=rms_layernorm_backward_impl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426263d",
   "metadata": {},
   "source": [
    "But instead of monkey-patching more, we can now register the kernel as an _implementation_ of the base `rms_norm` primitive defined above. For this we need an _execution transform_ - which is a fancy word for a function that implements the original operator (`rms_norm`) in terms of our new operator - so it has the call signature of the `rms_norm`. Because - like many fast implementations - the unsloth RMS norm does not implement the operator in full generality (to do them justice, they have a variant adding the unit offset, we just didn't copy it over), we implement a checker function, too: It takes the arguments of the operator we want specialize and returns a bool whether our implementation handles the given inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6b5c8320",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms_norm_to_unsloth(x: TensorProxy, weight: TensorProxy, dim: int, eps: float, add_unit_offset: bool):\n",
    "    assert dim == -1 and not add_unit_offset\n",
    "    res, _ = unsloth_rms_norm_forward(x, weight, eps)\n",
    "    return res\n",
    "\n",
    "def rms_norm_to_unsloth_checker(x: TensorProxy, weight: TensorProxy, dim: int, eps: float, add_unit_offset: bool):\n",
    "    if dim != -1 or add_unit_offset:\n",
    "        return False\n",
    "    if weight.requires_grad:\n",
    "        return False  # the unsloth rms norm backwward only gives the grad w.r.t. x\n",
    "    return x.device.devicetype == thunder.devices.DeviceType.CUDA and weight.device.devicetype == thunder.devices.DeviceType.CUDA\n",
    "\n",
    "my_ex.register_implementation(rms_norm, checker=rms_norm_to_unsloth_checker, execution_transform=rms_norm_to_unsloth)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec7c95a",
   "metadata": {},
   "source": [
    "So let us give that a try! Works great..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "965ba1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deviation: 9.5367431640625e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 0 milliseconds)\n",
       "import torch\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast()\n",
       "def computation(x, t_weight):\n",
       "  # x: \"cuda:0 f32[2048, 4096]\" \n",
       "  # t_weight: \"cuda:0 f32[4096]\" \n",
       "  (t7, (_, _, _)) = unsloth_rms_norm_forward(x, t_weight, 1e-06)\n",
       "  del x, t_weight\n",
       "  return t7"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.device('cuda'):\n",
    "    norm_module = ThunderizedRMSNorm(4096)\n",
    "\n",
    "# unfortunately, we meet dragons if we don't do this at this stage\n",
    "for p in norm_module.parameters():  \n",
    "    p.requires_grad_(False)\n",
    "\n",
    "thunder_norm_module = thunder.jit(norm_module, executors=[my_ex,])    \n",
    "x = torch.randn(2048, 4096, device=\"cuda\")\n",
    "\n",
    "expected = norm_module(x)\n",
    "actual = thunder_norm_module(x)\n",
    "\n",
    "print(\"deviation:\", (expected - actual).abs().max().item())\n",
    "\n",
    "thunder.last_traces(thunder_norm_module)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3e4d85",
   "metadata": {},
   "source": [
    "And this is also automatic when we instantiate a larger llama2-like model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fff2522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deviation: 4.76837158203125e-07\n"
     ]
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float32)\n",
    "with torch.device('cuda'):\n",
    "    m = GPT(Config.from_name('llama2-like'))\n",
    "\n",
    "for p in m.parameters():\n",
    "    p.requires_grad_(False)\n",
    "\n",
    "thunder_model = thunder.jit(m, executors=(my_ex,) + thunder.get_default_executors())\n",
    "\n",
    "inp = torch.randint(1, m.config.vocab_size, (1, 128), device=\"cuda\")\n",
    "actual = thunder_model(inp)\n",
    "expected = m(inp)\n",
    "\n",
    "print(\"deviation:\", (actual - expected).abs().max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b538cb40",
   "metadata": {},
   "source": [
    "By peeking into the trace, we can see that it actually used the unsloth RMS kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c260cb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  (n_1, (_, _, _)) = unsloth_rms_norm_forward(x, t_transformer_h_0_norm_1_weight, 1e-05)',\n",
       " '  (t110, (_, _, _)) = unsloth_rms_norm_forward(t102, t_transformer_h_0_norm_2_weight, 1e-05)',\n",
       " '  (t139, (_, _, _)) = unsloth_rms_norm_forward(t130, t_transformer_h_1_norm_1_weight, 1e-05)',\n",
       " '  (t215, (_, _, _)) = unsloth_rms_norm_forward(t207, t_transformer_h_1_norm_2_weight, 1e-05)',\n",
       " '  (t243, (_, _, _)) = unsloth_rms_norm_forward(t235, t_transformer_ln_f_weight, 1e-05)']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in str(thunder.last_traces(thunder_model)[-1]).split('\\n') if 'rms' in s]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6c0780",
   "metadata": {},
   "source": [
    "But what about the backward?\n",
    "\n",
    "Well, we have to connect forward and backward with a grad transformation. With our specialized ops, this is very simple, we compute the forward, call `get_grad` for the output, compute the backward, and put it on the input with `put_grads`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7670a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from thunder.core.transforms import get_grad, put_grads\n",
    "\n",
    "def unsloth_rms_norm_grad(x: TensorProxy, weight, dim: int, eps: float, add_unit_offset: bool):\n",
    "    res, (r, BLOCK_SIZE, num_warps) = unsloth_rms_norm_forward(x, weight, eps)\n",
    "    grad_res = get_grad(res)\n",
    "    grad_x = unsloth_rms_norm_backward(x, weight, r, eps, BLOCK_SIZE, num_warps, grad_res)\n",
    "    put_grads((x,), (grad_x,))\n",
    "    return res\n",
    "\n",
    "my_ex.register_implementation(rms_norm, checker=rms_norm_to_unsloth_checker,\n",
    "                              execution_transform=rms_norm_to_unsloth,\n",
    "                              grad_transform=unsloth_rms_norm_grad \n",
    "                              )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d31aced0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 4096]) torch.Size([256, 4096]) torch.Size([4096]) torch.Size([256]) torch.Size([256, 4096])\n",
      "(4096, 1) (4096, 1) (1,) (1,) (4096, 1)\n",
      "maximum deviation grads: 3.5762786865234375e-07\n"
     ]
    }
   ],
   "source": [
    "with torch.device('cuda'):\n",
    "    norm_module = ThunderizedRMSNorm(4096)\n",
    "    norm_module.weight.requires_grad_(False)\n",
    "    x = torch.randn(256, 4096, requires_grad=True)\n",
    "\n",
    "thunder_norm_module = thunder.jit(norm_module, executors=(my_ex,) + thunder.get_default_executors())    \n",
    "\n",
    "actual = thunder_norm_module(x)\n",
    "expected = norm_module(x)\n",
    "actual_grads = torch.autograd.grad(actual.sum(), x)\n",
    "expected_grads = torch.autograd.grad(expected.sum(),  x)\n",
    "\n",
    "print(\"maximum deviation grads:\", max((a-e).abs().max().item() for a, e in zip(actual_grads, expected_grads)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be218e9d",
   "metadata": {},
   "source": [
    "And here is our module having the unsloth backward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ac00153b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "# Constructed by Delete Last Used (took 0 milliseconds)\n",
       "import torch\n",
       "from thunder.executors.torchex import no_autocast\n",
       "\n",
       "@torch.no_grad()\n",
       "@no_autocast()\n",
       "def backward_fn(saved_for_backward, cotangents):\n",
       "  # saved_for_backward: \"Collection\" \n",
       "  # cotangents: \"Collection\" \n",
       "  C0, \\\n",
       "  C1, \\\n",
       "  = saved_for_backward\n",
       "  clear_collection(saved_for_backward)\n",
       "  del saved_for_backward\n",
       "  t4, \\\n",
       "  = cotangents\n",
       "  clear_collection(cotangents)\n",
       "  del cotangents\n",
       "  t0, \\\n",
       "  t1, \\\n",
       "  t3, \\\n",
       "  = C0\n",
       "  clear_collection(C0)\n",
       "  del C0\n",
       "  f0, \\\n",
       "  = C1\n",
       "  clear_collection(C1)\n",
       "  del C1\n",
       "  t2 = unsloth_rms_norm_backward(t0, t1, t3, f0, 4096, 8, t4)  # t2: \"cuda:0 f32[256, 4096]\"\n",
       "  del t0, t1, t3, f0, t4\n",
       "  return (t2, None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thunder.last_backward_traces(thunder_norm_module)[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ac79f0",
   "metadata": {},
   "source": [
    "That's it! Do check out our LitGPT studios and the other tutorial notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586cdd30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
